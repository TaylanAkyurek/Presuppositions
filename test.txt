In short, Neuroplasticity. Pain and pleasure are both literally "just in your mind". By that, I mean that what our minds interpret as pain or pleasure is dictated by the firing of certain neurons in our brains. The pain and pleasure areas lie fairly closely together in the brain. Neurons from one "area" can grow across to the other, or get crossed over, resulting in a type of mixed signal. 

Norman Doidge in his book "the Brain that Changes Itself" address this in more detail.
Deuterium fusion is too easy. There's only a small percentage of deuterium to start with, and it (along with lithium isotopes) are fused early on, during the transition from protostar to main-sequence star. Hydrogen fusion is dramatically slower, which is why stars live so long, they *can't* burn their fuel any faster.
No. There are no closed orbits around a cosmic string.

You can do it very easily with pen and paper. Take a sheet of paper and cut a triangular wedge. Glue the two edges of the wedge to form a cone. That's the space around a cosmic string (minus the longitudinal dimension, not important). There is no attraction from the string as the spacetime around it is actually flat. So your planet will move on geodesic (straight) lines on the paper. Take a ruler and try to draw a line that is straight and closed in a loop.

You'll see you can't. All you can draw are trajectories arriving to the string from infinity and being scattered off to infinity with an angle.
A calorie is a unit of work. The work required to perform a task should technically be the same for two humans (controlling for anatomic variation). That being said, the reason two people of different body types have a different reaction to the exact same exertion is due to efficiency. The body is a amazingly capable of adapting to the various stresses placed upon it. In the case you mentioned, someone who is fit should have a lower heart rate than the unfit person. This will occur for a few reasons, but they all come down to efficiency. (1) the heart muscle of a trained person becomes better at utilizing oxygen for energy than an untrained person. Also, depending on the type of training, a trained person's heart actually grows larger as well. This means that the volume of fluid ejected with each pump, something known as stroke volume, will increase. Thus a trained heart will utilize less energy for the same task. Also, studies have shown that changes in heart cell surface receptors will also make trained hearts more sensitive to adrenaline and noradrenaline. (2) the type of muscle fiber determines how energy is used. With enough training, some studies have shown individuals to have an increase in total mitochondrial mass. Mitochondria are essential for energy production during aerobic exercise. Therefore, a trained person will have more mitochondria, which also means they can utilize oxygen more efficiently (this is also part of the reason why people with more muscle mass have higher resting metabolic rates). 

So, all that being said, someone who is in better shape will probably burn more calories just because they can perform the task longer, or with more intensity. The task itself should be the same. I approached this question from a physiological perspective, I'd like to hear what a physicist has to say about this.
To produce a different isotope of the same element, you add or remove neutrons. We can add neutrons using neutron capture (for example (n,γ)) reactions, and remove neutrons using neutron removal (for example (n,2n)) reactions.
The teen pregnancy stats are generally higher in the "bible-belt" states which preach abstinence.  Map:  _URL_1_


Study on religiosity affecting teen pregnancy:  _URL_0_

"Increased religiosity in residents of states in the U.S. strongly predicted a higher teen birth rate"
For any planet or moon, there are always ±90° of latitude and ±180° of longitude. Latitude is naturally defined by the equator and the poles, so the only decision that needs to be made when setting up a planetary coordinte system is where to put the prime meridian. 

-

On tidally locked moons the meridian that constantly faces the planet is usually chosen as zero longitude. [[1](_URL_0_)] For bodies that are not tidally locked, just like on Earth, the choice is arbitrary, and is decided by the [International Astronomical Union](_URL_2_). [[2](_URL_1_)]
There are at least a couple contributing factors that we can discuss speculatively, and likely many more that we can't speak on for certain (like the aforementioned selection argument) which matter less, are dependent themselves on other variables, etc. So let's stick to the two we know about to get the discussion started about what does influence height. I'm sure there are more but others can add. 

One certainty is that trends in height can be linked to geographical locations. Genetic lineages with ancestry in colder climates typically express as shorter humans-- lower surface-area to exchange heat being the primary driving-force behind this correlation. The contrast is true of hotter locations and taller people. One could potentially analyze the frequency of hot-climate lineages in some population as compared to some time in the past (assuming it's possible) and make an argument based on this if the occurence of warm-climate lineage is higher now than in the past.

Another is nutrition. As previously mentioned: genetics matter. You can think of everyone as having a "genetic-limit" with respect to physical size -- so within the reference frame of a single individual, you can make a case for nutrition (moreso health in general) having an impact on height. It's thought that the availability of food and modern medicine practices, coupled with knowledge about healthy adolescent development, has allowed more people to approach their size-potential. Prolonged physiological stressors such as illness, cold, starvation, etc definitely hinder development.
Most of the everyday, practical aspects of forecasting have improved quite a bit.  Models are more sophisticated and the observation network has grown by leaps and bounds in the last 30 years.  Day-10 forecasts haven't been made for the whole 30 year period, but day-5 forecasts have improved substantially over that time frame, using the 500 hPa height anomaly correlation as a metric [(see here)](_URL_0_).  
  
It's hard to say how the next 30 years will go.  Focus shifts away from large-scale phenomena that are currently pretty well forecast to small-scale, convective systems that are still a huge problem (e.g. tornadic supercell thunderstorms, hurricane intensity, etc.).  You run up against problems that can't be solved by just throwing more computer cores at them.  If pressed for speculation, I think the big gains will be made in expansion of the observation network through on-demand, targeted satellite observations, the implementation of non-standard data, greater time-resolution, and ensemble forecasting techniques.  The question is if the forecasting of small-scale phenomena we're interested in have basically already reached saturation.  The other option would be to offer on-demand, short-range forecasts.
F=G * m_1 * m_2 / r^2


With G as a constant and m_1 as your mass and m_2 as the mass of the moon or the earth and r as the distance between the two objects.

So F_e=F_m?

The masses are to see as dots.

m_e= 5,974 · 10^24 kg

m_m= 7,349 · 10^22 kg

So with F_e=F_m we get:

m_m/r^2 = m_m/r_m^2

(G and your mass are canceled on both sides)

So since you want to know the height of your chair, we solve for r, and put r_m for the radius of the moon (1738 km)

So we get

sqr(m_e / m_m *r_m^2) = r (hope I didn't fuck up the math there)

So we should get something like 15670 km.

take away the 6378km of the earth and you get 9292km.

That should be your chairheight. :)

If I am wrong I am happy for any corrections.
The probability of a nuclear reaction or decay depends on many things, one of them being the Q-value.

Every reaction and decay is different, but you can come up with general rules of thumb. For example, the [Geiger-Nuttall law](_URL_0_) relates alpha decay lifetimes to the Q-value. You can see that the lifetime depends very strongly on the Q-value, and higher Q-value decays tend to have shorter lifetimes.

Similarly for gamma decay, you can show that the different types of gamma decays all have energy dependence. Gamma decays with higher multipolarity have stronger energy dependence. And higher-energy decays tend to have shorter lifetimes.
Monoculture of trees is a type of desertification
An autoclave is very much like a pressure cooker, with the difference that its temperature and pressure are more well controlled, so that you can be sure whatever was inside got exposed to a certain pressure at a certain temperature for at least a certain amount of time (the exact values depend on standard sterilization procedures).

Now regarding you question about using it instead of a dishwasher, I doubt it would be practical, since you would have to wash the dishes very well so that there are no food left and no particles left that could interfere in the sterilization and there is no reason why kitchenware needs to be sterile and it would be contaminated anyway as soon as you took it out of the pressure cooker, unless you individually packed each utensil in a special package as it is done with surgical instruments.
Yes, we have a lot of imagery of this happening.

[Collage of galactic collisions](_URL_0_)

[The Mice Galaxies](_URL_2_)

[The Antennae Galaxies](_URL_1_)
Dry dirt is a moderately good insulator, damp dirt is a somewhat less good insulator. Both will carry charges away, albeit some more efficiently than others.  I don't know of any practical limit to the charge you could remove from the planet. There must be one, but I imagine it would be extremely large. And where would you put it?   
   
Here's a table you might find interesting (all values are ohm-meters): 
 
Ashes of Cinders 6-70  

Clay (Damp) 14-30  

Clay (Dry/Compacted) 100-200  

Granite 2000-3000  

Limestone 1000-5000  

Loam (Humus) 200-400  

Loam (with Sand and Gravel) 30-50  

Loam (with some stones) 100-300  

Loam (with stones and poor vegetation) 200-350  

Marshy Soils 300-400  

Mountain Rocks (with little or not soil) 5-40  

Mountain Soil (Damp Peat over Rock Base) 1000-5000  

Mountain Soil (Over Rock Base) 150-300  

Salt Pans 300-1000  

Sand (Below Water Table) 6-70  

Sand (Damp) 60-130  

Sand (Dry) 1000-5000  
  

Sand (Leached) 1000-5000  

Sandstone 120-7000  

Shales 100-160  
===========================
From *_URL_0_
I'll let Feyman answer this one:

_URL_0_

I think one way of describing it, is they are wave functions with exact opposite patterns, and when they interact, they destructively interfere perfectly, converting into energy.
I got to this one a bit late, but people are throwing out guesses so I'll post a response anyway.

Opium is derived from the [milky latex](_URL_0_) of the opium poppy. The entire Papaveraceae family is united, in part, by production of this thick milky or watery fluid. The purpose of milky latex is, as some users have pointed out, primarily as a defensive mechanism against herbivory. Opium itself is just one of a variety of tertiary metabolites that are dumped into the milky latex to serve this end. 

So, to make the distinction clear, **opium did not evolve as a defensive mechanism**. Milky latex developed as a defensive mechanism, it contains a large number of compounds and has a broad negative effect on herbivores. The texture of the substance destroys the mouth parts of insects and has serious toxic effects on most animals. 

A few users are suggesting that if creatures at the opium poppy they would just get high - this is not the case. Ingestion of sufficient amounts of milky latex from any of the members of the Papaveraceae has deleterious effects on the lining of organs and is generally toxic. Also, the milky latex is produced by vessels on all parts of the plan, with the exception of the seeds, and is not just produced in the maturing capsule. 

So, why modern day opium? That is a process of artificial selection. All members of the poppy family produce alkaloids in the milky latex, the exact combination of this defense cocktail is variable. The milky latex, once harvested, is refined in order to produce a useable product. People have selected for opium poppys just as they have with other agricultural cultivars. 

One user has linked a NY times article about the mechanism of production suggesting that production of the desired compounds is limited to a short time period. This has been largely shown to be inaccurate based on new information gathered in Afghanistan regarding opium production. A full [characterization of the milky latex](_URL_1_) is now available. Selection for ideal harvest time is chosen based on maximum production of desired compounds, this is why it seemed that the desired alkaloids were only produced in a short window - they are just produced in higher quantities during that window in modern cultivars.
Because neurologically, being asleep and being unconscious are two different things.

When you're sleeping, you're not unconscious, you're in a *different state of consciousness*. You're still responsive and somewhat aware of your surroundings. It's why a loud noise or being shaken can wake you up, or how sounds around you can make their way into your dreams. While you may not be aware of it, while you're asleep you're still processing sensory input.

When you're anesthetized for surgery, you're not in an altered state of consciousness, you're fully unconscious. Your brain isn't processing sensory input or recording memories.

Essentially, when you're asleep, you just remember the time you spent asleep differently. When you're unconscious, you don't remember the time at all.
I'm not sure why memristers would accomplish what you claim, but I'll assume by this you mean that somehow you could build a system cost effectively that would have main memory with the same latency as the L1 cache.  The relevant factor is here the ratio between latency of main memory and the L1 cache.   On modern systems, this ratio is about 1:200.

The next question is how this would affect your actual programs.  Depending on the workload, the difference could be anywhere from literally 0 (if cache misses never occur now) up to the full ratio of the latency differences (If you use a workload that forces every access to miss cache).

Now, to translate this into something in practice is a much more complex question, since now you need to take into account the whole cache hierarchy.  But if we ignore that and just say you've got, for example, an Ivy Bridge chip running a SPEC CPU2000 benchmark, where you increase the L1 cache from the current size of 64kB per core to infinity, then according to [this chart](_URL_1_) the cache miss rate would drop pretty substantially.

You can get a pretty good idea of the sort of effect on different workloads you might see in [this benchmark](_URL_0_) comparing different L2 cache sizes which would have a pretty similar effect on performance as increasing L1 size.  You can see that it many applications the performance increases substantially, but in one of the benchmarks there is no difference at all.
I think the phrase generally refers to our perception of time.  When you are having a good time then it feels like time passes quickly, but if you are bored then it seems to drag on. Time can be thought of as this constant abstract thing separate from ourselves, or it can be thought of as something that is connected to our experience.

[There is also the concept of absolute time vs. relative time in science.](_URL_0_)
 > Newton's view on time kept it separate from space. When Albert Einstein introduced his Theory of Relativity in the early 20th century, however, he suggested that time wasn't separate from space but connected to it. Time and space combined to form space-time, and everyone measures his or her own experience in it differently because the speed of light (300,000 km per second) is the same for all observers. In other words, if all observers have to agree on the speed of light being 300,000 km per second, then they can't agree on the time it takes for other objects to travel relative to them.

 > Einstein also suggested that space-time wasn't flat, but curved or "warped" by the existence of matter and energy. Large bodies in space-time, like the Earth, aren't just floating in orbit. Instead, imagine an apple resting on a stretched out blanket -- the weight of the apple warps the sheet. If the Earth is an apple, then we can imagine the Earth's blanket as space-time.

 > This means that someone moving through space-time will experience it differently at various points. Time will actually appear to move slower near massive objects, because space-time is warped by the weight. These predictions have actually been proven. In 1962, scientists placed two atomic clocks at the bottom and top of a water tower. The clock at the bottom, the one closer to the massive center of the Earth, was running slower than the clock at the top. Einstein called this phenomenon time dilation.
There is research being done at the moment, it's still in the very early stage, to use computer games to correct amblyopia (lazy eye) as an alternative to patching. Early findings are promising but it's too early to say conclusively whether it will prove effective in the long term.

Source: Vision scientist and one of my old colleagues is involved in the research.
Dyslexia is not a just a visual impairment, there are multiple different types of Dyslexia. Meaning some people see things differently like words in a wave. While others have to deal with memorization of every word (like me). Basically almost every dyslexic person has a unique case, but they can get categorized into different types. Lastly, one main thing to know is that most heat maps are different for people with vs without dyslexia. Most dyslexic people deal with reading and words in a different section in the brain, compared to a person without dyslexia. 

TL;DR: yes, blind people can be dyslexic!
Your reaction to alcohol is in part dependent on your expected reaction, essentially a form of placebo effect. Thus it's quite possible that the expectation of different behavior when consuming different drinks could lead to different behavior.

_URL_0_

I don't know if any research has been done on this specific question, however.

edit: u/call_me_sandwich posted this article, really really interesting: _URL_1_

_URL_2_
I think this article answers your questions nicely: _URL_0_

tl;dr-the heat in the core of the earth is mostly the result of:

1. Residual heat from the formation of the planet.

2. Friction from different portions of the core rotating against each other.

3. Radioactive decay. (This one is much less certain--the elements in the Earth's core, and the degree that they contribute to heating, is debated.)
Are you saying the individuals that are recessive [aa] **never breed** with the individuals that have the dominant trait [AA] or [Aa]?

If that is the case, then the only way for [aa] to take over the population is for [aa] to fundamentally out-compete [AA] and [Aa] and push them out of the ecosystem or push them to extinction. Which *can* happen. Absolutely that can happen. 

Once the allele [A] goes extinct, then there is no point in referring to [a] as recessive if there is no variant allele that is dominant over it. But if a mutation arises that creates a new dominant [A^new ] then [a] could reasonably be referred to as a recessive allele again.
To address the first paragraph of the claim, this describes ionizing radiation. Microwaves do not have enough energy to ionize substances. 

If any nutrients are destroyed, it is due to heat and cooking time.  The interesting thing is that microwaves  use less heat than an oven, and have shorter cooking times, so things cooked in microwaves will retain most of their nutrients unlike things fried or sautéed, which will have a good bit of their nutrients destroyed.

Watering plants with microwaved water (which I assume is allowed time to cool) should be no different than non microwaved water. Water is H2O no matter how many times you microwave. And there are no nutrients in water to be denatured even if microwaves could do that.

Another thing. They say radiolytic compounds are formed by radiation. This radiation does not refer to electromagnetic radiation, which is what microwaves emit, but radioactive decay. Microwaves will not produce radiolytic compounds.
Amber starts out as sticky, liquid tree sap.  Small animals that accidentally touch it will get stuck, die, and can get surrounded by the sap.  The tree sap then hardens and, under the right conditions, it can fossilize, preserving the animals that are inside it.
>  then aren't we proposing that not only is it not constant, but that light can actually have multiple speeds at the same time to different observers?

Nope! You just explained the situation, that everyone agreed the speed of light was the same according to them. Thus, the speed of light is the same to all observers. Of course, there is something weird going on, because you intuitively expect that there is a constant background reference from which to measure the "true" speed of things. For example, if you're driving on the highway and someone passes you, they might be travelling 10km/hr faster than you, so it looks like they're traveling 10km/hr in your reference frame, but you know that since you're traveling 100km/hr, that their speed with respect to the road must be 110km/hr. Furthermore, you expect everyone else to be able to measure that cars speed with respect to the road to be 110km/hr, regardless of their motion. Unfortunately, none of these expectations are true in relativity. There is no fixed background with which to measure objects velocity, only relative velocities, and different observers don't always agree on the relative velocity of objects.

In the example you gave in your post, both spaceships will measure the speed of light to be the same. This requires that many of our intuitions about space and time must be wrong. You can learn about the weird things that have to happen to make things hang together in a mathematically consistent way. Time dilation and length contraction are effects in which different observers observe clocks run at different rates and rulers change their length based on the relative motion of an observer. The velocity addition formula tells you how to relative velocities of objects change depending on the observers motion. You can google these things to find out more; at the end of the day the whole thing hangs together to create a self-consistent, but weird, framework.
Water in liquid form is both cohesive and adhesive, meaning it will stick to surfaces (provided they aren't hydrophobic) and will also stick to other water molecules readily. When you shake the bottle, the water molecules are being thrown into the sides and top of the bottle and some of the molecules are adhering to the surface.

While the main body of water falls back down into the bottom of the bottle, the molecules that stuck to the surface continue to do so, and will trickle down the sides.
Not a physicist, and didn't watch the video, but i know what you're talking about and I'm an avid diver....and it's kind of lung-related....

The air in your lungs displaces water volume and this gives you buoyancy. As you descend, water pressure increases and the volume of air in your lungs decreases. The effect is most dramatic in the first 10m; the volume of air in your lungs will drop by half in that distance. This decreases your water displacement, decreasing buoyancy, and makes you sink faster.

It varies from person to person and what you're wearing, but for me, typically, once I pass ~6-7m I'm sinking even on a lungfull of air. I can typically get to that depth with a good pike dive and two good kicks with fins on. After that, there's no point in kicking unless you're in a hurry to get somewhere....it's more breath-efficient to just fall.
Yes, it would rust.  Its chemical and optical properties would be indiscernible from those of ordinary iron.  Though there are some small differences between matter and antimatter, in quantum electrodynamics, there is a symmetry in the laws between matter and antimatter.  Since chemistry and optics can be described by quantum electrodynamics, this block would look the same and rust the same.  While, for example, the weak force introduces some violations of matter-antimatter symmetry, unless you are probing weak force phenomena like beta-decay, you won't notice this.
Basically, we have these units in our muscle cells called sarcomeres. These things run down the whole length of our muscle. They basically look like interlocking fingers. The "fingers", known as the thin and thick filaments", are composed of proteins. Whenever our nervous system sends a message to contract a certain muscle (therefore the cells that comprise it), there's a sudden surge in calcium within the cells. This calcium essentially causes the "fingers" to interlock more through thick and thin filament interactions (the thick filament has proteins that actually "walk" across the thin filament). The combined effort of thousands of sarcomeres shortening down the length of the muscle causes what we know as muscle contraction.

If you're more interested, check out the [sliding filament model](_URL_0_)
All of your cells are floating around in an extracellular matrix (ECM). The cells are actually bound to the ECM. Cell detachment occurs when a cell unbinds from the ECM "skeleton" and this activates a specific process of apoptosis known as anoikis. I'm not sure how in depth of an explanation you want on anoikis, so if you just google/wiki it you can go as in depth as you want.
As far as I know, poison ivy produces an allergen called [urishiol](_URL_0_).  Urishiol is a skin irritant, which means that it will cause a minor reaction in almost everyone.  However, many people, on top of that minor reaction, are actually allergic to urishiol.  That means that, after a first exposure, if you get urishiol on your skin, your immune systems starts to hard-core attack it.  That's how the severe poison ivy reactions start.  

The urishiol is absorbed by the skin, and recognized as an allergen (again, this can only happen if you've been exposed once already).  Antigen-presenting cells called dendritic cells will pick up some of the chemical and take it back to your lymph nodes, where your T cells hang out.  Your T cells recognize the urishiol as "non-self" and basically proceed freak out.  All they want to do is kill the ever-loving fuck out of whatever this stuff is, because they've seen it before, and they don't like it.  So they start releasing cytokines and cytotoxic factors, which cause inflammation and start trying to kill things.  And that's probably what you're dealing with.
When a cell does not produce enough ATP their Na/K-ATPas will stop working. This will cause an influx of Na and cause the cell to become hypertonic and swell due to osmosis, causing damage. The switch from aerobic to anaerobic glycolysis will cause an accumulation of lactic acid, lowering pH which in turn decreases activity of many intracellular enzymes. An influx of Ca will also occur due to failure of Ca-pumps, activating intracellular enzymes (e.g. protease and phospholipase) and causing membrane and mitochondrial damage amongst other things. Defect mitochondria will produce reacive oxygen species to a larger degree (and less ATP) and might leak pro-apoptotic proteins like cytochrome c and anti-IAPs (IAP = Inhibitor of Apoptosis). Reactive Oxygen Species will cause membrane and DNA damage (later causing apoptosis).

If the adequate blood flow is restored you can get a reperfusion injury due to the sudden delivery of oxygen (causing further production of reactive oxygen species, e.g. through xanthin oxidase) and calcium (influx due to previous membrane damage) and leukocytes (causing an inflammatory reaction and release of free radicals, e.g. due to damage-associated molecular patterns).

Summary: Lack of oxygen **can** cause **irreversible** damage to cells through several mechanisms, but the core is failure of ion pumps and eventually mitochondrial damage. I.e. The fact that they stop working sometimes causes irreversible damage. The dose makes the poison.

Source: *Robbins and Cotran Pathologic Basis of Disease*, 9th ed.
It would require a single rock (assuming you have really good aim). There is no friction in space, so as long as you are willing to wait a while, throwing a single rock will give you a non-zero velocity that will get you there eventually. Let's say the rock is 1 kg, and that you weigh 70 kg, and that you can throw the rock at 20 m/s. Conservation of momentum tells us that you will recoil with about 0.3 m/s which will get you to the space station in just 18 and a half hours.
The tesla house battery is a basically a lithium Ion battery,same as yor phone just in Large. Here's a pretty good [link](_URL_0_) to how they work.

Barriers are :the low engie density in these types of. I think the Tesla battery weighs 100KG an can store up to 13.5 Kw/H. That equates to ~4KG of diesel.

Also the charge degrades over large time spans due to leak currents and even by high temperature. 


With our current technologies storing electrical energie is quite inefficient and expensive. And likely to not change all to fast, after all li batterys were around ~1915.

Hower a a lot of money and manhours are Invested into research so we might see completely new technologies or a battery operating on simmylar principles but with different materials.
Yes, this is done by calculating the energy levels of the electrons in the molecule using quantum mechanics. The light that is absorbed and re-emitted (reflection on a microscopic scale) depends on the difference between these levels.
So a few points on this...

First, be aware that there's recently been a lot of debate challenging the century-old wisdom that it's actually the Gulf Stream responsible for the warmer-than-expected temperatures for Europe. Other leading theories include: 

- The simple fact that oceans store a lot more heat than land does. This higher thermal inertia means that while the Atlantic is slow to heat up in the summer, it releases that heat during the winter, and the prevailing jet stream carries that heat eastwards from the Atlantic over the European continent. [This PDF](_URL_0_) has a more technical read on that.

- The prevailing jet stream moving from west to east is also interrupted while passing over the Rocky Mountains. This sets up north-south waves in the jet stream downwind of the Rockies, on average causing it to swoop downwards to bring colder air south over Eastern North America. As the jet stream continues around the globe, the upswing of that wave then brings warm air northwards over Europe. More on that in [this PDF](_URL_1_).

In truth, it's probably some combination of all three, though how much is still under debate.

With all that said, there is definitely a very noticeable difference in water temperatures off Eastern North America because of the Gulf Stream. Consider that Virginia Beach, VA is actually located farther north than Monterey, CA; however, Virginia Beach ocean temperatures can reach 30 C (83 F) in summer, while Monterey barely reaches 16 C (60 F) in summer.

To answer your original question, then: both of the above alternate theories about Europe's warmth also come into play here. Both the prevailing winds and the Gulf Stream itself tend to propagate eastward, so the inland regions of Eastern North America don't really experience much of the Gulf Stream's heat. Additionally, that atmospheric wave set up by the Rockies tends to pull colder air southwards over the region, cooling Eastern North America further.
[Sort of](_URL_0_). This study shows small gains from higher temperatures.

The duration and vigorousness of the hand washing is more important than the water temperature.
That often quoted claim is a bit misleading.

First, you have to realize we are talking of volume, not surface. The surface of the ocean we have had relatively easy access to most of for millenia - for the harder to get at parts [the polar oceans and seas] a bit more than a century. In the last decades or so, we've added satellite-based observational capacity. So we know the surface waters quite well.

The waters in the first few tens of meters, where light can reach [the photic zone] are also relatively well studied. But given that light is plentiful here, so is life - because photosynthesis. This is where plancton abounds, and the food chain which depends on plancton. So the part of the ocean richest in life is also the one we have easiest access to.

Below the photic zone are a few km of dark water where life is relatively spread thin. Some food rains down from the photic zone as plancton and larger critters croak and sink, but otherwise this immense volume of water is comparative desert next to the photic zone.

Then there is the bottom, the benthic environment. Things get fun here, as there are really 2 trophic chains which co-exist. The first is fed by whatever dead critters survive the fall to the bottom and rain down from the surface [think of this as "canned sunlight"]. The second, **very local** [as in "few and far between"], is fed by chemautotrophic reactions around black smokers on the bottom. The source of energy here *is not* the sun. It is energy released the transformation of specific chemical compounds [H2S, SO4, CH4, etc.] by bacteria, which are often symbiotic.

Now back to your question. Dinosaurs [and giant reptiles - e.g Plesiosaurs and Ichtyosaurs] are obligate air breathers. That we know. So any hypothetical surviving such critter would have to come at the surface to breathe at the surface now and then. And the surface is where we have our best observations. So far we have no serious leads suggesting the existence of any such critters. Now and then, you'll hear stories of new whale species being ID-ed, but most of these new species rely on genetic confirmation and observation of full specimens - casual observers of live specimens would just see a few whale-backs cavorting next to their boat and move on with their lives, so these new species may have been unknowingly observed for years prior to us realizing they are new. A live giant reptile would be immediately obvious, and we haven't seen any of those yet, so that makes their survival unlikely in the extreme.

But there might, however, be surviving critters we believe extinct still hanging on in a niche somewhere at the bottom, where our observations are scant. Coelacants are one example - When Miss Latimer found her first specimen in an African fish market, she was blown away. Her very words were to the effect that she couldn't have been more surprised if a live dinosaur had come lumbering down the street. And it is worth noting that there are various ways a "prehistoric survivor" may be found - one such is taxonomic revision. For instance, Stromatoporoïds were believed to be extinct since the Paleozoic. Recent studies have however found them to be closely related to a surviving, albeit rare, type of sponge. The same happened with Chaetetids, initially believed to be a group of extinct corals but rediscovered as a unique type of sponge.

What might such hypothetical survivors be? Well ... the fossil zoo is full of possibilities ... Personally I would love to see a surviving population of trilobites. Rudists are another one which would blow my mind. But the evidence so far is zilch...
Dysprosium (66) isn't used for much, but it still has a few very specific applications. Sometimes its used to absorb neutrons in Nuclear reactors. Theodore Gray, who made the visual elements books sums it up pretty well- "Look up dysprosium, and you have to go to the fourth page of results before finding anything that isn't a periodic table website's entry for dysprosium, usually an obligatory 'It's an element, so we have to have a page about it' sort of page."
**Left to Right**

[HAAS Delay](_URL_0_) - Sound hits the closer ear first

Intensity differences - Slightly louder at the closer ear

**Vertical Localization**

[HRTF](_URL_1_) - Head Related transfer function. A dramatic oversimplification is that your shoulders will either muffle or reflect sound depending on the source height.

**Forwards and backwards**

Honestly, I have no idea. My hypothesis is this is because the ears are not symmetrical in this plane, as they face "forwards". Therefore there is probably a difference in the time delay and amplitude of certain frequencies, as they are either reflected or 'muffled' by the outer ear. More authoritative data would certainly be welcomed. 


Also, /r/audioengineering may have more detailed info for you
Essentially, the reactions that release the energy of the food you eat are exactly the same reactions of lighting said food on fire, but slowed down and controlled so that the energy is useful to you, rather than just coming out as heat.

Plants use energy from sunlight to split apart water (H2**O**) and carbon dioxide (C**O2**), combining what's left to make sugars (C6H12O6)n and free oxygen (O2) as a waste product.

This process requires that energy from sunlight because water and carbon dioxide are very stable molecules, with very strong bonds between the atoms in them, and it takes a lot of energy to break those bonds. The separated sugar and oxygen molecules have weaker bonds, and thus aren't as stable, and are stuck in a high-energy state.

In your body, that oxygen is (carefully) re-combined with the sugar, in stages. By allowing the oxygen to attach back on to the hydrogen and carbon to make the strong, stable bonds of H2O and CO2, a lot of potential energy is released. However, your body doesn't do this reaction in one step, but instead forces it to proceed through a lot of intermediate cellular machinery which siphons off a large fraction of that energy, which is then used to preform all the essential functions of life.

You can think of it like a river. Water up high wants to flow back down, to where its energy is lower, and it is more stable. If you just let it flow, it will do so on its own, but you don't get anything from it. However, if you build a dam and stop the water from flowing, you can release a little bit at a time, and force it to fall through a turbine, which powers a generator, and you can steal some of the energy from the water flowing downhill.
It's an ice halo, similar to [the one asked about yesterday here](_URL_1_). Ice crystals at different orientations and of shapes form different kinds of halos when light reflects from the inside surface of the crystal. Sometimes they can be circular, sometimes curved or upright pillars like this. More information [here](_URL_0_). Click on the different types of pillars on the left hand side for pictures of how the light reflects inside the crystals.
>  So, my understanding is that you take matter and transfer its "information" (quantum state, etc) to a medium--thereby destroying the original (doesn't violate Heisenberg), and recreate the same "information" in matter at some distant area... hence, teleportation.

I'm afraid that understanding is incorrect, except as a description of a sci-fi concept which we have no reason to believe could work in reality.

Perhaps start with the wikipedia article on [Quantum Teleportation](_URL_0_):

 >  "Quantum teleportation, or entanglement-assisted teleportation, is a process by which a qubit (the basic unit of quantum information) can be transmitted exactly (in principle) from one location to another, without the qubit being transmitted through the intervening space. It is useful for quantum information processing, however it does not immediately transmit classical information, and therefore cannot be used for communication at superluminal (faster than light) speed. Quantum teleportation is unrelated to the common term teleportation - it does not transport the system itself, and does not concern rearranging particles to copy the form of an object."

A key point here is that the entanglement approach being used in quantum teleportation is not capable of directly transferring classical information, let alone transferring matter or energy.  To perform the trick, you first have to entangle some particles and then move those particles, by normal means, to your distant location.  Only then can you perform "teleportation", and what you're actually teleporting are quantum states.  That's why the Chinese experiments use photons: they're quick and relatively easy to move around (at the speed of light) without becoming entangled with something else along the way, ruining the experiment.

If you did such an experiment with matter, after entanglement you'd literally have to drive or fly the entangled matter to the distant location, which rather puts a damper on the whole "beam me up" thing.

If you google keywords like "china teleport" you'll find plenty of articles about the Chinese experiments, and the ones in reputable sources tend to mention these limitations.
The specs you have listed at the secondary assumes 100% efficiency. The secondary voltage will be 11,500V based on your turns ratio of the transformer for current less than 0.2A. However, current will depend on your load, it won't just be a steady 0.2A no matter the load. So in your example a 500kOhm resistor would draw your 0.023A as you say (but that resistor better have a power rating of 264.5W or higher otherwise it'll smoke real quick). However if you decrease the load resistance thus increasing current you will eventually reach the point at which the transformer will struggle to maintain its voltage because power must be conserved. Therefore the voltage on the secondary will droop to accommodate the increased current. This also results on lots of heat and eventually the transformer or circuit will destroy itself unless added protections are added (voltage monitors, fuses, etc.)

Edit: can't word good
Well, sharks are known to bite-and-release humans rather frequently.  I suspect that at least part of the difference may be down to the fact that killer whales can get a very accurate picture of their prey through sonar, while sharks bite and get the flavor and mouth-feel.  So it's probably a lot easier for the killer whale to "see" from a distance that a human is not ordinary food, while a shark wouldn't necessarily even know until it had already taken a bite.

I suspect that humans are also more likely to encounter resident pods of killer whales, which subsist more on fish than mammals anyway.  But it is still rather striking that so few instances of killer whale attacks on humans are known.
Some people would deny that this is even a scientific question, because there isn't a good definition of 'consciousness'. Here's my replies from a previous similar question:

I don't have a label, but I'm a research masters student in Artificial Intelligence who has read a lot about the 'consciousness' debate.
After a lot of reading and thinking, my opinion is that there is no mystery about subjective phenomena or 'consciousness'. I think a lot of people who believe in the mystery of consciousness are closet dualists who want to believe in the soul.

The main sources for convincing me of this are my own reading of cutting-edge research in machine learning, and philosophical works by Dennett and Hoftstadter in the last 30 years. I would specifically recommend 'Consciousness Explained" by Dennett, and this wikipedia page:
_URL_1_

I think consciousness is simply the subjective experiencing of a bunch of neurological processes that we already know quite a bit about, like vision and attention, proprioception, motor activity... and some processes we know less about like language and memory.

Any object that receives external information through senses, runs it through some information processing apparatus, and takes motor actions based on this has some level of conscious experience. This includes flight computers, normal humans, ants, severely mentally disabled humans, monkeys, slugs, Cyborg Stephen Hawking, amoebae... they all experience information and take action, and in between process the information in some way. There's no level of processing at which we suddenly say "this thing has the special property of experiencing consciousness".

If you accept this point, and you're just asking "how do neurons implement thought", then there are two answers. For processes that we sort of understand quite well, like the early stages of vision, we have a good understanding of exactly how neurons compute and transform images:

_URL_0_

For the general experience of being alive and conscious and all of the things our brain does, some people will say our thought "emerges" from or is "an emergent property of" the low-level functioning of all of our neurons. 

While this is true, it's not really much of an answer, IMO. It's like saying an ant colony is an emergent property of individual ants. It just means that we don't quite know how the lower level computations build together into very high-level thought - it might be so complex and dynamic as to be analogous to the weather - we understand low-level physics very well, but we still can't say if it'll rain in Boston in 6 days time - the processes are just too complex.
To expand on Search_Bot with some other differences between black holes and our universe:

Black holes contain a finite amount of "energy". Our universe looks very much like it is infinitely big, which would indicate it contains an infinite amount of energy. In our universe, everything isn't expanding from some central point - everything is expanding from everything else (that is, inflation is happening *everywhere*). Regardless of whether any of the black hole theories are correct (and there is no evidence suggesting they are), we definitely are not "literally" inside of a black hole.
In fact, this was asked [just today](_URL_0_).

Virtual particles are a useful mathematical tool, but you shouldn't think that distant particles are shooting photons or gravitons at each other to exert a force. I'd recommend reading [this article](_URL_1_) which clarifies things quite nicely.
Non-Newtonian Fluids
_URL_0_

You can make some at home with some corn starch and water in a bag.  You can shatter it with a hammer and then pour it through a funnel.
Wow, I'm surprised to see a question that is right up my alley!

So, "pilus" exclusively refers to those projections that allow for DNA transfer between cells.  "pseudopili" are those structures that bear a remarkable resemblance to pili in terms of their composition, but are used for excreting other compounds such as the proteins you have listed.  

This is evolution's way of diversifying the role of an established structure, leading to speciation!
An electron will not annihilate with an antiproton, but they will repel each other because they are both negatively charged. 

A positron can be in an orbital around an antiproton and form antihydrogen, it has been done before.
Herpes, syphilis, gonorrhea, and chlamydia can all cause hematuria. Technically HIV can cause hematuria if one has renal damage due to the disease progression.

This being said, hematuria would be a weak marker for an STD. That is to say, when someone presents with hematuria we don't immediately jump to STDs as the cause.

The most common cause of hematuria in women is a simple UTI.
Well, that question is highly dependent on the wing-loading capability of the aircraft. The reason why fighter pilots have to wear G-suits is because the planes they are flying are very high performance machines that can fly into extreme maneuvers at high speeds, and it is pretty easy to enter a climb/bank which makes the plane want to enter travel in a different direction than you are very quickly.

If the wingsuit was constructed to the same degree of that a fighter jet, then yes it would be possible. However, you need to have a pretty solid airframe in order to withstand this degree of loading. Otherwise, the suit would simply rip apart well before it could apply any significant force to the pilot in order to make them black out.
The two units measure the same thing, but they differ by 1000.  It's the same as meters and millimeters.

By the way, mAh is "milliamp hours," so a battery with 1 mAh can put out one milliamp of electrical current for 1 hour, 1 amp for 1 millihour, etc.  Ah is "amp hours".

If you're curious, 1 amp corresponds to 6.24 x 10^18 electrons flowing past any point in the circuit every second.
Both types of interactions you describe contribute to the cohesion of bulk graphite. Frankly the paper you referenced is not exactly of the highest quality, but similar results were found by others as well, e.g. see [this paper](_URL_1_). In the paper I cited, researchers used [density functional theory](_URL_0_) to study the relative contributions of dispersive (van der Waals) and "metallic" interactions. They also find that a simple van der Waals model is not sufficient for an accurate description of interplanar bonding in graphite, the contribution from van der Waals interactions only accounting for ca. 8meV/C of the cohesive energy compared to the total energy of  20meV/C. The rest of the cohesive energy is provided by a variety of "metallic" interactions that account for the interaction of electrons in one plane interacting with the ions and electrons in neighboring planes of graphite.
This is commonly referred to as a "Death Spiral"

Essentially, they're all following a chemtrail that goes in a circle instead of going back to the nest.
Richard Feynman's popular works are a great way to 'click' on a lot of physics. Books like [6 Easy Steps](_URL_0_). I'm not sure what level they expect you to be at already, but Feynman was an outstanding teacher, one of the world's best in my opinion.
You could give yourself water intoxication if you drink lots of water in a hot climate.
You've been sweating out all of your electrolytes and obviously you will drink water to rehydrate but it's that chemical imbalance that's dangerous
Francis Crick - Molecular Structure of Nucleic acids(DNA)

_URL_0_


Great paper, and very short!
Basically the first accurate measurement involved bouncing a focused light off a mirror several kilometers a way and seeing if it was reflected through a gap in a rotating cog.

You can read about the early history of light speed measurements here: _URL_0_
You don't need pi for the Fourier Transform. The appearance of pi in it is because of the geometry of the underlying space that we work with.  There is a [very general notion of Fourier Transform](_URL_0_) that works in very weird spaces.

It also wouldn't be an "different interpretation of pi", it would just be some other ratio in some other space. We can make these ourselves, and so if we ever met up with the aliens we would just interpret "their" pi as some other quantity that we already know about. If you don't use flat geometry, then you just don't use flat geometry, there are ways around this and we already do it all the time, so it isn't really that big of a deal. 

The importance or "deepness" of pi is greatly exaggerated. It's just everywhere because it is so basic. Kind of like how nails are everywhere in construction. Not because they're a sophisticated piece of construction technology, their just super basic and easy to use.
You're correct that there are lots of sources of time interference on modern computers, but believe it or not it's possible to get extremely deterministic timings if you configure the system correctly. I work in a field called "real-time computing", where "real-time" means that systems are highly predictable rather than fast, and we target safety-critical applications like avionics and industrial control. I have about six years of experience in running time-sensitive workloads, so this is going to be rather long:

The first source of time interference happens when your code stops executing and other code runs instead. There are three major sources:

1) Modern machines implement "multi-programming" where the OS rapidly switches between executing different programs roughly every 1ms. Even if the only thing running on the machine is your program you're trying to measure, the system is undoubtedly running a lot of other services that consume processor time: things like a graphical user interface, or the Read-Copy-Update (RCU) garbage collector on Linux. On Linux it's actually fairly easy to eliminate this interference- using "real-time" priorities you're able to prioritize your program above all other programs running on the system- even the operating system itself. This guarantees that no other program runs when your program is trying to run.

2) The machine hardware is also a source of interference. When hardware devices need attention from the system they will send an interrupt to the processor, which involuntarily preempts your code so that the OS can run the appropriate hardware drivers. These interrupts are external to the system and are necessary to its function, so they can't be eliminated, but you can manage them pretty effectively on multi-core machines by sacrificing one core. You can see how many interrupts are serviced by what processors by looking at /proc/interrupts on Linux.

Most interrupts from normal hardware devices (network devices, hard drives) are called maskable. On Linux you can "mask" such interrupts so that they are never delivered to certain processors, so you can dedicate a single processor core (core 0 usually) to handle all maskable interrupts. Then you can tell the OS to never use that core for your program.

Additionally, there are non-maskable interrupts (NMIs). As far as I know there is unfortunately nothing you can do about these except track them in /proc/interrupts and ensure that they're not occurring frequently enough to seriously disrupt system performance. The main thing these are used for are for the scheduler timer interrupt, but some types of computer hardware will generate them (and with some frequency as well). Fortunately the system designers are aware of these issues, so NMIs tend to be extremely consistent and are exceedingly fast, so they show up in timing measurements as a small inflation of execution time. 

Lastly, there has been a lot of recent work in making the OS better able to provide real-time performance by modifying the way it handles interrupts. In Linux you can look up and apply the "real-time patchset" managed by Ingo Molnar which allows full preemption of many interrupts serviced by the processors, which means in practice that as much interrupt handling work as possible is deferred until later and run as regular-priority, non-real-time so as not to interfere with a currently running task.

3) The third way your "code can stop executing" is not through any action of the OS or the computer hardware, but your own program may be doing something you didn't intend or expect. It's very common for programmers to call a lot of functions they didn't write, but they must be very careful about doing so or else they can get bitten by performance bugs. For example, a major source of timing variability is allocators and garbage collectors. 

Suppose you create a vector in C++... by default the system actually over-allocates too much space so that you can later push() additional elements to the end of your vector very quickly and efficiently. However, there's a limit. You might push 2047 elements onto a vector and all of those operations are fast and consistent, but when you push the 2048th element that operation takes a huge amount of time. Well, unbeknownst to your C++ vector suddenly ran out of allocated space and suddenly had to call malloc() to get more space on the heap, and malloc() had to call sbrk() to get more memory from the OS, and the OS had to evict some pages from memory to make room, and so forth. 

The problem with garbage-collected languages is that the garbage collector will run at random times, which is obviously bad when you're trying to measure execution time of parts of your code.

Fortunately (or perhaps unfortunately) the simple solution is to not use garbage-collected languages and never use someone else's data structures or functions unless you're certain how they work. Instead, at the start of your program (before you start measuring time) you pre-allocate all of the memory you'll ever need (under worst-case assumptions) and then you lock that into memory using mlockall() (on Linux) so your memory can't be paged out. 


SO, all of that said, these are the ways that your code might be prevented from running and other code can be added to or interfere with your time measurements. There are other sources of timing error in a computer system.

1) As iwantashinyunicorn points out, modern machines throttle CPU performance. They actually do it in two ways- voltage scaling and ferquency scaling, and the whole technology is usually referred to in the same breath as dynamic voltage and frequency scaling (DVFS).

If you're running time-critical workloads the best thing to do in this case is to disable DVFS at a OS level. This means that your processor is running at full speed all of the time and when there is nothing to do it is simply executing the OS idle loop, usually just checking to see if there's anything new to do.

On Linux you can look this up by Googling for CPU governors. 

In the same vein (depending on how long you're running your program) there are processor sleep and idle states provided at a BIOS level by the Advanced Configuration and Power Interface (ACPI) that should be disabled in the BIOS.

2) There is lastly an issue of fundamental timer accuracy and resolution. There area a lot of different ways a computer can know what time it is, but not all methods are created equal. An accurate clock will always return the same value after the same time has elapsed, and most computer clocks are pretty accurate. You're more likely to run into an issue with resolution- for the last 5-10 years or so both processors and OSes have been able to provide nanosecond-level resolution, but there are still a lot of scenarios out there where resolution is limited. For example, a classic time source is the Programmable Interrupt Timer (PIT) which has a basic frequency of 1193182Hz, meaning a fundamental resolution of 0.84 microseconds. Older systems (Linux particularly) may have a fundamental resolution of 1 millisecond due to the way that time and timer events were managed.

On Linux you can see the the clock_getres() man page for instructions on determining your timer resolution:

_URL_1_

Note that a timer resolution of 1ns does not mean that you can time events accurate to 1ns, it only means that when you ask for what time it is you get a value that is significant to 1ns. For example, on my machine I'm sitting at it takes about 50,000ns just to call the code to ask what time it is. This causes a whole separate issue called jitter which may cause problems in different types of applications.

3) Beyond this there are definitely sources of weird machine behavior (such as cache alignment and non-uniform memory-access (NUMA) times, but the key here is that the weirdness can be relatively deterministic. You might be testing an algorithm for problem sizes 100, 150, and 200, and it just so happens that size=150 happens to take longer than both 100 and 200 due to a weird cache conflict. That's an undesirable result you might want to investigate and eliminate, but the behavior at this point should be consistent that size=150 always takes longer than the other two sizes. 

4) If you're measuring relatively small sections of code and you set up your environment well, you absolutely can get cycle-accurate timings on modern machines. Intel produced a pretty good white paper on the topic, but this is really only for when you care about cycles rather than nanoseconds or higher:

_URL_0_

Anyway, that's all I can think of at the moment (it took me the better part of an hour to write all that out). Let me know if you have any further questions. Due to my research I'm one of the few people who look at computer performance all the way from the macro level down to the microsecond and nanosecond level.
Absolutely. There are [stories](_URL_0_) of people lost at sea and they have described turtle blood as tasting like "the elixir of life" and getting cravings for fish eyes for the liquid and vitamins.
Radioactivity doesn't depend on density.  It depends on stability of the nucleus (which does depend on density a bit but there are other factors).  Basically can the neutrons and protons stay bonded together.  If they can't, then some form of decay will occur to make it more stable.  Tritium is not as stable as He-3, so the neutron decays into a proton in order for the system to reach a stable arrangement.
HAZMAT tech here.

The Flaming O oxidizing agent symbol is there because the substance is not a fuel, but an Oxidizer. The presence of Oxidizing agents can greatly accelerate the combustion of other materials, or even cause things that are not normally flammable to burn. It also broadens the flammability limits of most substances by lowering the LFL and raising the UFL. In other words, it becomes much easier for materials to burn at lower temperatures and at a wider range of air/fuel mixtures. A small oxygen leak, for example, can turn what would otherwise be a simple structure fire into a very intense, and possibly explosive inferno. Even a slight increase in the oxygen content of air above its usual 21 vol % will greatly increase the rate of oxidation or combustion of many substances. 

And oxygen isn't the only oxidizing agent that can do this.

For example. Nitrous oxide is an oxidizing agent. By itself, it will not burn. But it will gladly support combustion in the place of atmospheric oxygen.

Firefighting techniques for Oxidizing agents are different from flammable agents, since if you extinguish the fire, the oxidizing agent is still there, ready to restart the fire at a moments notice to possibly explosive effect. Certian oxidizing agents can even catch water and concrete on fire.
Cyanoacrylate (the active component of most super glues) reacts with some fabrics to produce a lot of heat and a white smoke that is very irritating to eyes/mouth/lungs. I don't recall exactly what it is, but if it was a small amount of glue (it sounds like it was), you should be fine.

For preventing nylon fraying, I often just melt the end with a lighter, so it becomes one solid bead of fused material along the end.
Ctrl-C won't work for all applications in Linux.  Ctrl-C most often sends a SIGINT to the current foreground process (in most any shell), which can be easily ignored:

    #include  < signal.h > 
    #include  < unistd.h > 
    #include  < stdio.h > 
    
    void lol( int sig ){
    	printf( "\ntry harder\n" );
    }
    
    int main( int argc, char *argv[] ){
    	signal( SIGINT,  & lol );
    
    	while( 1 ){
    		sleep( 1 );
    	}
    }

Note that this will still respond well to a kill/killall.  SIGINT can be almost any signal.  This can be used for great evil:

    #include  < signal.h > 
    #include  < unistd.h > 
    #include  < stdio.h > 
    
    void lol( int sig ){
    	printf( "\ntry harder\n" );
    }
    
    int main( int argc, char *argv[] ){
    	signal( SIGSEGV,  & lol );
    
    	sleep( * (int *) 0 );
    }

SIGKILL and SIGSTOP can not be ignored by a running application.  You may come across zombies.  They are already dead, so you can not kill them.  You must kill the parent process at this point.

Windows is fairly similar as far as Ctrl-C is concerned.  Windows has some different signals than you might see in Linux, but most of the same functionality is there.  If you try to kill applications via the task manager, there are multiple signals which could be sent to the application (the same is true with various managers in Linux).  The application may be ignoring some of these signals, or it may just take some time to handle them.

IIRC, if you use the Applications list, you'll be sending a WM_CLOSE.  This can be intercepted by the application in a similar fashion to the examples.  It's of a similar "severity" to SIGTERM.

If you use the Processes list, the application can not intercept the signal.
Not usually.  We use liquid nitrogen to rapidly freeze cell cultures for later defrosting and re-culture so proteins don't denature just because of the cold.  Ice crystals will bust membranes, but that's a different story.

You can do lots of other really cool things with liquid N2 and food, though.  It makes really great ice cream if you mix liquid N2 right into the ice cream base and stir like crazy.
I don't have a source right now but if I remember correctly adding a massive amount of water to the sun would actually make it burn hotter. The reason is that at these scales we can't think of water as something that extinguishes fire. What's going on in a star is a nuclear fusion reaction triggered by the massive amount of energy coming from the combined gravitational pull of all the components of the star. So by adding a massive amount of water to the star you're adding a lot of weight which means more gravitational energy meaning more fusion. You're also adding fuel that can be fused in the form of water molecules. 

Soeone please correct me if I'm wrong, I'm by no means an authority on the subject but I remember seeing something like this on a PBS spacetime video a while ago.
The orientation of the moon relative to the Earth and Sun doesn't change, but your orientation does! The surface of the Earth is curved, so that when you're in England, your body (from your feet to your head) points much closer to the north pole, while when you're on the equator, your body is sticking out to the side pointing at the equator. You can see this reflected in how high the north star is above the horizon, as well - it's clearly visible fairly high in the sky in England, while it's on the horizon at the equator. 

So to point your head in the same direction that it is in England while you're on the equator, tilt your head over towards the north, *then* look at the moon. Now it's not a "U" anymore, is it? The top of the moon is closer to your new "up".

If you keep traveling south, the shadow of the moon looks backwards when it's waning/waxing compared to in England - because it looks flipped upside down! Orion and the other constellations are also conspicuously upside down. 

TL;DR: It's because your "up" isn't the same direction on the equator as it was in England.
As a rule, you don't absorb all of anything. At least a little bit of each nutrient will pass through the digestive system and become food for your gut bacteria.

For some nutrients, such as calcium, the absorption rate isn't very good even if the body is deficient in it: the most calcium you'll ever absorb from food is about 70%. However absorption of vitamin C is quite good according to [this source](_URL_0_) which pegs it at 70-90% for moderate intake, which they define as 30-185mg a day. For reference, the daily recommended intake for Vitamin C is 60-90mg. According to that source, even doses of 500mg a day or more of vitamin C may get absorbed with 50% efficiency. Of course, most of that will probably be quickly excreted via the urine.

Generally speaking, the smaller the dose of a nutrient, the higher percentage of it can be absorbed, but for vitamin C, getting all of it from one beverage doesn't seem to be a problem. 

Finally, I know that Vitamin C can increase iron absorption if consumed at the same time, but I'm not aware of any factors that could affect Vitamin C absorption itself.
Not a medical professional or anything, but I'm a mechanical engineer that works in fluid mechanics. Reynolds Number is ρuL/μ , where L is the characteristic length. Approximating flow through a blood vessel as pipe flow, the characteristic length would be the diameter of the "pipe". As the aorta is a bigger blood vessel than ones in the brain, this would make sense that the aorta would have a higher Re.
>  If I was on a train that was going 20 mph, and I began walking toward the front of the train at 5 mph, am I now going 25 mph?

Yes, you are now going 25mph with respect to the ground.

 >  his high school science teacher said that you cannot move faster than the vehicle you are in.

I do not know the context of the teacher's statement, but the meaning that is implied here is incorrect. If you're in a moving vehicle and you start to move around, you can absolutely move faster with respect to the ground than the vehicle itself.
Red cabbage contains excess amounts of a class of plant pigments called Anthocyanins. 

This is a common "Accessory Pigment" in most flowering plants, and is responsible for the pink and blue colors in many flowers. Hydrangeas or cornflowers for example. 

Red cabbage has been selectively bred for a gene mutation that causes this pigment to be overproduced.

Anthocyanin is a natural *Acid-base Indicator.* 

In acid conditions, it reacts with excessive H+ ions. This modifies the electron structure in the molecule, making it tend to absorb green light.

In basic conditions, the excess H+ is neutralized by substances like bicarbonate ions.  This shifts the energy of the pigment so it tends to absorb red light, producing a bluish- green.

Indeed, red cabbage that is grown in acidic soils will tend to have a more pink tint, while that grown in basic soils will tend to look more purple or purple-green.
The best time is after big solar explosions, at night, preferably close to the poles, looking north, outsides of cities and light pollution.
You can subscribe to _URL_0_ for e-mails about solar explosions and stuff.
Usually not very far.  These "cracks" are fissures created by the surface ground moving apart.  They rarely extend very far down and do not close up, as fictional accounts would have you believe.

_URL_0_
Retired soil ecologist with a specialty in earthworms here.

The earthworm body is essentially a fluid-filled tube with sphincters at the juncture of each segment that enable the worm to control the movement and pressure of that fluid, thus enabling movement and protecting internal organs. I don't know the first thing about physics, but I can tell you that the coelomic fluid and the lack of hard body structures mean that the worm can be tossed a great distance without sustaining damage.

As to the question of drying during the flight, earthworm skin is extremely porous, meaning that it will sustain some loss of fluid, but not enough to cause damage.  Further,the porous nature of the skin also means the worm can quickly replace lost fluids in the humid, moist environment of the soil or plants into which it is tossed.

Walking on the soil causes insufficient sudden compaction to impact earthworms to any significant degree.  Further, the worm's ability to narrow and lengthen it's body in response to sudden change in space would further limit the potential for danger.

If you need citations for any of this I will do my best to provide them.
There is some suggestions that the interior of a neutron star may produce an environment where a quark/gluon plasma is created. Whether this is considered a breaking of protons, I could not say for certain, but would certainly be interested to find out: 

_URL_0_
It's most important function is as a placeholder. Like many things, not having gone through life without a zero, it is difficult to put into layman perspective.

If you take a look at the [Babylonian Number System](_URL_0_), you will see the origin of the placeholder numeral system we now use.  Their places were just stacks of symbols in a particular place, as opposed to different symbols to represent 1-9. I'm going to substitute other symbols to represent Babylonian numbers, and hopefully it will make sense.

The number 121 would be written as two 60s followed by seven ones (XXY). The number 1812 would be written as 30 wedges and 28 wedges (XXX...XXXYYY...YYY). Pretty straightforward... but when you get to a number like 3601, what do you do? You need to write out for 1 60^2 + 1. Babylonians would write that as: X Y (Note the space). Eventually, They invented a placeholder, so it would now be written XWY.

Ok... so what?

Well, astronomers started using the placeholders and adding them to the start and end of numbers. This allowed for greater accuracy in measurements because it effectively allowed fractions. To write 1/60, they would use WY.  To explain a little more, if #### was a number set, the first # would be 60's, second # would be 1's, third # would be 1/60s and the last # would be 1/3600's. Funnily enough, the Babylonians were obviously aware of "nothing" as a concept enough to create a placeholder, but not make the leap to it being a number, itself. There are records that refer to running out of grain as "We have 20 - 20 grain."

China had the placeholder concept, but was eventually shown the zero by way of India. And in China, they used counting boards where zero was represented as an empty space. With the new feature of a zero, they could now write on paper and forego the boards.

The Arabs eventually came up with the numerals we use today and passed it on to Europe once trade started... and again, counting boards vanished.

If anything, I'd say that starts pointing toward easier calculation and what is life, science, technology but some form of calculation?

Now, earlier, I said something about Babylonians understanding the concept of nothing as a symbol, but not a number. I'll hopefully illustrate it easily. Numerals are symbols for numbers. Using zero as a symbol is easy. 10+1= ?, 10-1=?, 2x10=?, 10/2=?

Whereas zero as a number is not necessarily as easy. 1+0=?, 1-0=?, 1x0=?, 0/1=?. 

So, everyone using placeholders understood the first set of problems, but not the second.

**TL;DR** It made calculation easier and more accurate. And life is full of calculations.
Yes, there are many 90+ without cancer.  I have a 94 year-old grandmother, and as far as we know she's as healthy as ever.

Unfortunately, no, sequencing the genomes of healthy 90+ year-olds wouldn't give us much meaningful data in regards to why people *don't* develop cancer.  It's just not well enough controlled.  That means that all those 90 year-olds would have far too much in common (and in contrast) with people who *do* get cancer to really tell what [polymorphisms](_URL_0_ are contributing to preventing cancer development.  Perhaps when we have thousands and thousands of genomes, we can run some highly complex statistical studies to point us in the right direction.  

A related topic, I saw a talk in 2008 by Tim Ley covering the [sequencing of acute myeloid leukemia genomes](_URL_1_).  This was a much better experiment.  Why?  Because the researchers also sequenced the same person's normal genome.  So the differences between the normal and cancer genomes gives us important and useful information (mainly which genes are mutated).  It's far more complex than just that, but I can talk more about it if there's interest.
Humans use huge amounts of fresh water.  When it leaves our houses it is grey water, which tends to get cleaned in a processing plant, sent into a river, and lost to the sea, where it gets mixed with saline water.

Our fresh water supply, therefore depends on two things; catching new fresh water from rainfall in reservoirs and young aquifers, and utilising ancient fresh water trapped in isolated underground aquifers.  

The first is fine, although dependant on having a good amount of rainfall. The second is completely unsustainable, as these aquifers do not get recharged.  The big problem is that huge amounts of the fresh water we use is taken from these ancient aquifers, and once they run out, there's no more.

This is happening particularly across much of Northern Africa, but to some degree in most parts of the world. But that's not all, even young, connected aquifers can be tapped faster than they recharge; simple demand exceeding supply. For example, the [Ogallala aquifer which feeds so much of US demand](_URL_0_).

Now, we can use desalination to recover fresh water from sea water, but a) it's hugely energy intensive and hence expensive, and b) you have to then transport that water.  Consider that a cubic meter of water weighs 1 tonne, and the US uses [128,000 million gallons, or 581,820,000 cubic meters per day *just for crop irrigation*](_URL_1_).  How are you planning to transport nearly 600 million tonnes of water every day from the coast to the inland farming areas? And this ignores civic demand in towns and cities. So, assuming a 20 tonne truck load (which is about what you can fit on a standard haulage vehicle), you're looking at 29,100,000 freight trucks a day just transporting water, or 2,424,000 of the very biggest rail-freight tanks. And this is just for the USA.
The mechanics inside a nucleus are complex, but here's a basic picture. In a nucleus, there is a tug-of-war between the nuclear force (the residual strong force) and the electromagnetic force. Protons are positively charged. As they get closer, the electromagnetic force tries to push them away while the nuclear force tries to pull them together. The nuclear force is stronger than the electromagnetic force, but it has a much shorter range, so it only wins the tug-of-war if protons get really close to each other. When two nuclei get close enough that the attractive nuclear force overcomes the repulsive electromagnetic force, they stick together and make a new nucleus (this is nuclear fusion). 

Energy is released whenever a system reaches a more stable/lower energy configuration. A system in a less stable configuration has extra energy stored in the form of system mass. When it reaches a more stable configuration, this system mass is converted to energy which is emitted. The resulting system has less mass. For the lighter elements which have very small nuclei, two nuclei fused together is a lower energy/more stable configuration than the two nuclei separate because the nuclear force is so strong. Even though the nuclear force is short range, it doesn't matter much because the nuclei are so small.

For the elements with very large nuclei (e.g. uranium), the protons on one side of the nucleus are now far enough away from the protons on the other side of the nucleus that the short range of the nuclear force starts to matter. In other words, the electromagnetic force causes protons on opposite sides of a large nucleus to be repelled almost as strongly as the nuclear force is holding them together because they are so far apart. As a result, the separated configuration of two large nuclei is more stable/lower energy than the united configuration. Thus energy is released and overall mass is lost when a large nucleus splits apart because it is transitioning to a more stable configuration. Many nuclei of heavy elements don't just fall apart on their own even though they are in a higher energy configuration because there is an energy barrier. (At the same time, many isotopes of heavy elements do just fall apart on their own, i.e. undergo radioactive decay.) A kick from a neutron can get a heavy nucleus over the energy barrier and get it to fission into smaller pieces. 

In summary, for very light elements like hydrogen and helium, the attractive nuclear force wins the tug-of-war decidedly and you get energy out when nuclei combine. For very heavy elements like uranium and plutonium, the repulsive electromagnetic force wins the tug-of-war (because it is longer range) and you get energy out when nuclei split apart. For elements right in the middle like iron, neither force wins the tug-of-war much and you don't get much energy out or in (relatively speaking). [This graph summarizes these concepts](_URL_0_).
No, there are multiple, different types of filters responsible for removing components of the air.

Carbon Dioxide, being the main culprit, is absorbed on a zeolite and dumped overboard (by reheating).

Other minor impurities (such as methane and thiols from your butt) are captured in Carbon filters, which are regularly replaced and disposed of.
Modern magnetometers can go much deeper than one meter, but it depends on a variety of factors--primarily the size and shape of what you're wanting to observe. Though there are a number of ways to estimate how deep a magnetometer can see, in general, it ranges anywhere from 0-55m. 5-10 is realistic in practice as an upper limit, though, because at most places in the near-surface you're going to have quite a bit of iron that will obscure the deeper target.
If I understand dams correctly, electrical energy is gained from gravitational potential energy. In other words, you're trading height for energy, and there are obvious limits there.
Glass is a subcategory of materials known as [amorphous solids](_URL_2_), which are solid but have a non-repeating molecular structure.  Glass also happens to have what is known as a [glass transition](_URL_1_).

There are well more than the three [states of matter](_URL_3_) you listed, though those plus [plasma](_URL_0_) make up the four classical ones.
Why a diamond? That's expensive, you don't want to burn it up like that.

If a satellite gently releases an object, then it will share its orbit with the satellite as they both have the same speed. If you want it to reenter the atmosphere then it has to slow down enough to lower the perigee of its orbit.

For a satellite in a low orbit like the ISS, just decreasing the speed by 100 m/s is enough. Atmospheric drag will do the rest of the job. This number is little for a rocket burn if your object has thrusters, but if you're just "throwing" it's too much. It's not like you can throw it with your arm at 100 m/s. Otherwise that speed loss could be achieved by atmospheric drag (space is a vacuum, ok, but in low orbits there is a very thin atmosphere that can cause your orbit to decay) if the object has a low enough ballistic coefficient, i.e. large surface area and small mass.

From higher orbits, like geosynchronous satellites, it's just impractical.

Actually, unmanned cargo spacecraft that take supplies to the ISS are filled with waste on return, then they deorbit them intentionally to fall on a remote area of the pacific ocean. They look like shooting stars when they burn up.
This is a phenomenon arising out of Fresnel equations, which happen to describe how the amount of light reflected off of a surface changes with viewing angle. This phenomenon is not limited to aluminum laptops, but also glass, water, polished wood, and cars: _URL_0_
_URL_1_
Yup, see all the different virtual machine programs out there, the current CPUs actually have special instructions to accelerate it (though emulate is a bit of a stretch with modern VMs, they usually run most instruction nativity and emulate the hardware access instructions). There is software CPU virtual machines (bochs), and it does exist, though as you said, it's absurdly slow. It's mostly used for developing things like drivers and BIOS software which requires an accurate emulation more than speed.
There's a process called [Muller's Ratchet](_URL_0_). It describes how a parthenogenic lineage can only evolve less fit individuals, so they have a finite species lifespan (around 100K generations). But it doesn't take much mixis to stop the ratchet. So as long as they were mixing it up every now and then they should be ok.       
   
(Edit: Typo.)
A boat that can carry 100 tonnes of cargo can carry at most 100 tonnes of birds (whether they are flying or not is irrelevant).  If you are confused about this answer look into the difference between force and mass as well as Newton's 3rd law of motion.
This doesn't directly answer your question, but you may be interested in learning more about the [China brain](_URL_0_).
The human lens can be stretched by muscles to change the focal length. When looking at something far away, the focal length should be the depth of the eyeball. When something is close up the focal length is shortened.

The focal length is changed by stretching or relaxing the flexible lens. Peoples ability to do this varies.

The eyeball also changes shape over time, leading to changes in focus.

Some people are [near-sighted](_URL_0_) and some far sighted, so it is clearly not the same for every healthy human.
It's not a problem anymore for string theory. String theory is a consistent UV completion of gravity. It doesn't have the incurable divergences of General Relativity, and is indeed healthy as a quantum theory. Moreover, it reduces to GR at low energies.

And it is extremely constrained, i.e. there's only one free parameter and a discrete small set of possible superstring theories. Even the spacetime dimension is constrained.

The downside is that string theory has a large number of vacua and virtually infinite ways of getting out the standard model, and no dynamical process to select one. It is therefore pretty difficult to guess all that happens inbetween and so to make any kind of low-energy prediction. One could say this is a problem of *any* quantum gravity proposal.

Other quantum gravity approaches such as LQG instead have yet to be proven to reduce to GR in the classical limit. There's considerable difficulty building a classical limit, and there is no reasonable argument for its existence. Maybe if there's someone working on the LQG side and correct this he can chime in.
Ooh, this is very clever. Assuming the pedal is shorter than the radius of the wheel, the contraption doesn't move.

Let the radius of the wheel be 1 meter. Let P the length of the pedal shaft, anything less than 1. Let θ be some angle in radians that the wheel rotates around its axle, less than π/4 radians-- that is, some acute angle.

Rotate the wheel without the rope attached. The whole contraption will move θ meters to the right. (This is the definition of radians!)

The rotation will also move the pedal leftward, by P\*sin(θ) meters. Note that because sin(θ) < θ for all θ > 0, and we defined P < =1, that means P\*sin(θ) is strictly less than θ. 

Observe that the pedal's net rightward movement is the axle's right movement minus the pedal's own left movment: that is, θ-(P\*sin(θ)). We just showed that P\*sin(θ) is less than θ. Therefore, when the wheel rotates through any acute angle, the value is positive-- meaning the pedal has a net rightward movement.

In other words, with no skidding allowed, there's no way to pull the pedal left and have the contraption move right. Your scenario is static and the pull of a nonelastic rope has no result.

[Edited to add this part]  
~~Although... hmm. The pedal also moves upward, and therefore closer to the endpoint of the rope. Working on the math for that.~~ Never mind, I'm sticking with my original answer.
The average outdoor atmospheric concentration of CO2 is about 400 parts per million (ppm). (This varies by location and season. It is also increasing by about 5ppm per year, and accelerating.)

For a healthy person, the lowest level to cause detectable effects is around 600-1000 ppm. This will cause some people to complain of vague malaise, or things like "stale air." 

Above 2000 ppm, it starts to cause shortness of breath with exertion. You might not notice a problem sitting still, but if you got up and moved around you would feel winded more easily.

Above 3000 ppm it starts to cause an increase in respiratory depth and rate at rest. It also tends to cause headaches.

Above 5000 ppm, breathing is labored and heart rate is elevated. Severe headaches, sweating, and palpitations may be noted.

About the 6000-7500 ppm range you start to see serious problems like severe shortness of breath, headache, vertigo, drowsiness, confusion, vomiting, and muscle weakness.

10,000 ppm (10%) causes unconsciousness within ten to fifteen minutes. A person who falls unconscious in this atmosphere, and is not removed, could suffer permanent harm or death.

15,000 ppm (15%) is considered the lethal concentration. It brings unconsciousness within a few moments. It cannot be survived except by leaving the high-CO2 atmosphere.

20,000 ppm (20%) causes almost instant convulsions and unconsciousness.

**tl;dr** - Respiratory drive starts to be affected at about 5 to 6 times the normal atmospheric concentration.
This would be a better question for a physiologist. 

Smells are just your brain's interpretation of olfactory and taste input. Often times degrees of importance are placed on certain smells based on their immediate importance to the organism. The more immediately important, the stronger the smell will tend to be even at lower concentrations.
First, the shuttle did not go to the Moon.  The shuttle did not launch till 1981.  The Apollo moon landings took place between 1969 and 1972.

Second, rockets don't use atmospheric oxygen. The lunar module, for example, in its ascent stage (the part that lifted off the Moon) had aerozine and nitrogen tetroxide (the latter provided the oxygen needed for the reaction).

Third, the whole landing/return process had several stages.  The Saturn V lifted the whole thing into Earth orbit. Then the Apollo Command Module and Service Module and Lunar Module left Earth orbit and went to the Moon.  The Lunar Module separated and landed on the Moon.  The Lunar Module had two pieces; the upper piece was the ascent stage, using the lower piece as its launch pad.  Only the ascent stage lifted off the Moon, and it only needed to get back to lunar orbit, not back to the Earth.  The ascent piece docked with the Apollo Command Module (which was still also attached to the Service Module) in lunar orbit, the astronauts who had been on the Moon transferred back, and then the Lunar Module ascent stage was jettisoned.

The Service Module then provided the power to leave lunar orbit and head back to Earth.  Before entering the Earth's atmosphere, the Command and Service Modules separated, and only the Command Module returned to Earth (unpowered).
[(handy periodic table for reference)](_URL_0_)

This isn't a comprehensive answer and I really only have a prerequisite knowledge of chemistry, but this is what I can tell you:

Metals have very low electronegativity, which means they don't have a strong hold on their valence electrons, nor do they have a strong affinity for capturing electrons. There are a couple common reasons for this to happen in an atom/ion/molecule, which is why most elements are metals. Chemistry is practically all about what happens with the outer (valence) electrons, so let's think about what would cause a nucleus to have a loose grip on its valence electrons. There are two major causes:

1. The valence electrons are far from the nucleus, which by the inverse square relationship of electric force and distance ([Coulomb's law](_URL_1_)) suggests that these electrons are relatively weakly bound.

2. The valence electrons are shielded by filled orbitals interior to their orbital.

Looking over the periodic table, it can become apparent why so many elements are metals. Groups 1 and 2 each only have an electron or two, which is distant from the nucleus and shielded by filled interior orbitals. Hydrogen is an exception, since it is so small that it is actually quite electronegative.

Groups 3 through 12, including the lanthanides and actinides, are all filling the d and f orbitals, respectively. These are pretty high energy orbitals, meaning electrons in these orbitals are not bound with a lot of energy. In general, the larger elements get kind of "fluffy" and just kind of lose electrons because they're so big and their electrons are so far from the nucleus.

Basically the only elements that are going to have a large electronegativity are those that are close to filling a valence shell. These are found on the upper right side of the periodic table, since moving down on the periodic table implies larger size and more distant valence electrons.

**TL;DR** Electronegativity increases as you move up and right along the periodic table. The line where the elements are not electronegative enough to hold onto their valence electrons strongly happens to be near the edge of the table, meaning the majority of elements to the left of that line will be metals.
You can engineer the release mechanism to meet different goals.  One example that I'm familiar with is from[ Rohm  &  Hass, a big chemical company, that  formulates these extended release mechanisms](_URL_1_).  They put the drug inside an polymer and it takes time to diffuse out. 

This [graph](_URL_0_) shows the concentration of Diclofenac vs time in a extended release formulation.
Fans require a certain amount of torque to overcome friction, and then need far less to keep going.  The motors aren't that strong in a fan, and overtime as the fan ages, dust gets in there, the motor loses it's strength, etc, the lower settings might not provide the torque required to get going.  So, they just put the highest setting first, to make sure that it starts up.
Gastroenterologist here: Short answer - there are multiple mechanisms:


1) **Mu Receptor agonists** These bad boys are drugs like immodium (loperamide) and lomotil (diphynoxylate and atropine). They work by binding to Mu receptors on the smooth muslcles of the colon. By binding to these receptors the muscles of the colon wall (which normally contract like a worm) are slowed down. This is also the mechanism by which morphine, dilaudid, percocet, heroin, etc cause severe constipation - and people who undergo withdrawl from these drugs often have severe diarrhea.


2) **Fiber Supplements** Most commonly Psyllium (Metamucil), Methylcellulose (synthetic fiber) and Guar Gum. These supplements are not absorbed by the colon (the cellulose is nonabsorbable). Like a sponge they absorb the water/liquid in the colon/SB. They also bulk the stool and can be used in constipation (dual use). The colonic bacteria are able to breakdown some of the fiber (via fermentation) and as a result produce hydrogen and methane which can cause gas/bloating.


3)**Bile Acid Binding Resins** You may know these drugs as cholysteramine. They actually work when there is diarrhea due to Bile Acid Malabsorption (BAM). Bile acid is usually absorbed in the ileum (terminal small bowel). Patients that have had a surgical resection of the small bowel, a choleycstectomy, or disease of the small bowel (like Crohns) can result in failure of bile from being absorbed in the small bowel. Excess bile then enters the colon - which is actually an irritant to the colon wall - and cause diarrhea from the irritation (inflammatory diarrhea)


4) **Somatostain analogues** Now we are getting into some specialized drugs - Octreotide (most common somatostatin analogue in Canada). Works be inhibiting secretion of fluids from the cells of the small bowel (these cells are responsible mainly for absorption of liquid, but they can also secrete liquid - like in cholera). The mechanism of Somatostatin analogues are complex and involve activating signalling proteins that downregulate the production of channels (like chloride and sodium channels) that are ultimately responsible for secreting water INTO the colon/small bowel.


5) **Targeted treatment** most gastroenterologist will actually work to figure out the cause of the diarrhea and will prescribe medications that will treat the disease. These drugs are often very different from the drugs I describe above. Example Diseases **Inflammatory Bowel Disease** (5-amisosalycilic acid, azathiprine, 6-mp, infliximab), **Cholera** (oral rehydration solution), **Microscopic Colitis** (discontinue offending drug, 5-asa), **Infectious Colitis** (Antibiotics), **Irritible Bowel Syndrome** (antidpressants), etc**.

Hope that helps
Actually, modern seltzer water is just plain water which has CO2 dissolved in it under slight pressure.

It can ideally have no minerals, sodium, or dissolved solids if the source water is free of those dissolved solids to begin with.
The CO2 content is a dissolved gas in this case, which is to a certain extent present as carbonic acid, an ionic compound of CO2  &  H2O.
Seltzer water is sparkling carbonated water, but it is not "soda" or "soda water" since the CO2 content is not the result of added sodium compounds.

Club Soda is much different than seltzer.  Club soda is the sparkling water which contains Sodium Bicarbonate (baking soda) as the source of the CO2.  In this case there are considerable dissolved solids, and as the name implies, added sodium compared to the plain water it is made from.

When true seltzer water first becomes slightly flat, it has lost most of its CO2 to evaporation, there is still some residual carbonic acid which can affect the mouth pH detectably, but not an actual flavor.
If all the CO2 were allowed to escape over a longer period of time, there would be no residual flavor when drank unless the water or the CO2 it was made from had an off-flavor to begin with.
However if you do leave an open container of seltzer water to go completely flat over a long period of time, the likelihood of it picking up a taste from the immediate surroundings is increased, such as from smoke or perfume in the air nearby.
If the seltzer was from made from pure distilled or mineral-free water, then when it goes completely flat it will still be mineral-free pure water, just without carbonation.

If what you consider "normal" is your local tap water, then very pure or distilled water may not taste "normal" to you even if it is absolutely tasteless.

If the seltzer was made from tap water in some other locality, then if it was to go completely flat it would taste more like their tap water than your local water supply, and that might not be regarded as normal either.

Club soda OTOH if left to go completely flat, will still have the full amount of sodium that was added at the factory.
Generally club soda whether fresh or flat has a disagreeable taste except to sodium lovers.
Very, very generally speaking, anything that can cause genetic damage to cells is a carcinogen. This includes physically disrupting the structure of DNA directly, or interfering with transcription or other cellular machinery.

We can categorize carcinogens into groups which share more traits with each other, but this is the only solid top-level definition that's totally universal. It also explains why, given enough time, pretty much anything can be a (circumstantial) carcinogen.

In terms of how cancer is initiated- the genetic damage incurred usually involves disabling certain genes that control cell division and programmed death; most cancers are essentially caused by the failure of one or more cell types to self-regulate their own population. The genes involved are usually referred to as "tumor suppression genes", because when you turn them off, you get tumors, but there's a lot we don't understand about how cells communicate. They could do other things.

It's also worth noting that the body is not constantly teetering on the edge of developing cancer: there are a lot of safeguards in place, and in most cases you need to accumulate a lot of genetic damage before malignant tumors start showing up. This can occur quickly (a carcinogenic viral infection, a large radiation dose) or not (smoking cigarettes, exposure to asbestos).
Yes, you can get shocked.  This is basically how electrofishing works (which I do for my research).  Contrary to what is stated elsewhere, flesh does _not_ necessarily have a far higher resistance than fresh water (unless humans are quite a bit more resistant than fish).  At any rate, you can certainly be electrified even without being hit directly by lightning above the water.  When electricity enters the water, it produces a voltage gradient in the water.  If your body stretches across that voltage gradient, then your feet (eg) will be at a higher voltage than your head.  This induces a current in your body...essentially the voltage is "short circuiting" through you.  

Note that you can't do electrofishing in salt water because the electricity will just go through the water instead of the fish (salt water _is_ much more conductive than flesh).  And you can't do it in distilled water because the current can't get to the fish in the first place.  But given the massive amount of juice produced by a lightning bolt, I wouldn't count on safety even in either of those circumstances.

Here's a link with some useful infomation about electrofishing, including conductivity measurements of fish and water
_URL_0_
Not an expert on circadian rhythms but I know that modern humans didn't usually sleep 8 straight hours a night even a couple hundred years ago. It was very common to wake up for a couple of hours in the middle of the night to do things. 

Prior to the industrial revolution both biphasic (sleeping twice a day) and polyphasic (sleeping several times a day) were the norm. It's been argued that the industrial revolution, with its longer working hours led to the 8 straight hours that are commonly suggested. This is also attributed to the availability of electricity and artificial light, which extends our waking hours longer into the night. 

My understanding is that sleep is not very understood and seems to have a high degree of variability in what works best for different people. All we know for sure is that it is extremely important to get that "best".
It doesn't. v^2 is a quadratic term, not exponential. Kinetic energy and momentum are simply distinct concepts, I'm not sure if there is a satisfying answer to "why aren't they the same?" You can derive K=1/2mv^2 from a few simple principles, but I'm not sure if that will be enlightening either.
Well, it depends on the optics you're using. If you're using a human eye, the resolution is about half an arc-minute, and if you want to be able to distinguish things a thousand kilometers apart, you'd have to be about 5 million kilometers away (rough estimate).
Yes.  Heritability of IQ is generally calculated to be somewhere around [50% to 85%](_URL_0_).
Do remember that our insides are full of water with lots of dissolved ions, which readily pass through our skin in the form of sweat. These ions allow the conduction of electricity.
Its a condition called "supercooled". In order for ice to form, the water must b 1)at or below freezing, and 2) contain something for the ice to form on. This can be impurities, bubbles, a scratch in the wall of the container, or an ice crystal. If the water in the bottle is pure and undisturbed, the temperature can drop to well below freezing without ice forming. Agitating the water in the morning initiates the formation of ice crystals, which cascades very rapidly until all the water is frozen.
Breaking through the surface of a water bubble requires energy in order to overcome the surface tension. Thus it is energetically preferable for a submerged object to stay submerged.
It depends on the type of screen your phone has.  

tl;dr version: for LCD/LED/IPS screens, yes.  For OLED/AMOLED screens and e-ink screens, no.

Long version:

For LCD screens (including LED and IPS screens), then yes.  These have a backlight that is always running when the screen is on.  For these screens a black screen is a mostly opaque screen, it is black because it blocks most (but not all) of the light from the backlight.  Some light will always leak through, though.

For OLED screens (including AMOLED screens), then no.  For these screens, the light comes directly from the pixels, not from a backlight.  Black pixels are black because they emit no light.  So for OLED screens, a fully black screen emits no light, and thus uses essentially no power.

This is why some phones with OLED screens have modes where they can show, for example, text notifications or the current time on a black background with very little power consumption.  Only the pixels emitting light are using power, so by keeping the screen mostly black you get only a little more power consumption than if the screen is off.  LCD screens can't do this, because the backlight is running at the same power level either way.

For e-ink screens (which are rare on phones, but a few models have them), then not only is a fully black screen not using power, neither is a fully white screen or a screen showing some combination of white or black.  On an e-ink screen, once a pixel is set to a color, it doesn't need any additional power to stay at that color.  So the screen only takes power when it is changing, a static screen (no matter what it shows) is using no power at all.  That is, of course, assuming it isn't backlit (if it is then you get the same problem as with LCD screens), but e-ink screens don't need or use backlights unless it is very dark.

This is why e-ink screens have such enormous battery life.  For other screens types, the screen is by far the largest consumer of power on the device when it is on.  For e-ink screens, however, power consumption of the screen is tiny for things like text and static graphics.  The catch is that e-ink screens have much slower refresh rates, unsuitable for video, games, or other highly dynamic content.
Yes. The goal is living and the arrangement is called "mutualism".

For example, the Pacific Gaper claim plays host to a species of Pea crab. 

The crabs eat food that escapes the clam and keep the clam clean. They live and breed inside the clam's mantle and shell.

The clam gets cleaned.

It is a nice arrangement for everyone who is a clam or crab.
For an ideal refrigerator (carnot cycle), the coefficient of performance (a type of efficiency) goes to inifinity as the outside and inside temperatures approach each other. Also, heat "leaking" into the house from the outside also decrease as the outside and inside temperatures approach each other. So, efficiency is best when the outside and inside temperatures are similar

[wiki](_URL_0_)
Energy for the flash is stored in a high voltage capacitor. That's a much higher voltage than the battery, and a transformer is used to convert battery voltage to that higher voltage. Transformers need to be driven via oscillating current and not steady DC, and a simple circuit provides that oscillation. As the capacitor charges, load on the transformer decreases, and the simple circuit changes frequency.

A transformer basically converts oscillating electric current in one coil of wire to an oscillating magnetic field which induces an oscillating current in another coil. This oscillating magnetic field causes physical oscillation of materials in the transformer.

Modern cameras still use transformers, as long as they have a flashtube and not an LED flash. The difference is that the frequency is higher so you cannot hear it. A higher frequency allows the transformer to be smaller.
I don't know about the satellites themselves having proximity sensors, but NASA and NORAD keep very close track of anything orbiting Earth. Like down to sizes around 1cm. There's an amazing amount of junk flying around up there. There's a very real possibility of mankind trapping ourselves on the planet if we don't do something to mitigate the amount of objects in orbit. Something the size of this 'o' can be traveling at several thousand miles per hour. If/when it hits something, the kinetic force is like a 50 cal bullet impact.
It's not a static boundary - varies among individuals, and varies throughout an individuals lifetime. As I recall from class older adults often show more memory of youth and childhood. It's usually just measured through self report and average ages
If you had a constant molar concentration of gas particles at a constant temperature then the pressure would be inversely proportional to the volume of the gas. Increase the volume and the pressure decreases. Or increase the pressure and the volume must decreases (for a defined number of moles of gas) .

This is all expressed in the ideal gas equation

 > P = nRT/V
I believe what you're describing is called [Troxler's faiding](_URL_0_).
Summary: Bright blue light can powerfully alter the timing of your daily, circadian, rhythms. If applied early in the day, they will advance your rhythm to make you wake earlier, and go to bed earlier. If applied late in the day, they will delay your rhythm. Most people's rhythms delay a little each day, and light counteracts this natural trend. Insomniacs and people with depression slow their rhythms more than others.

Your circadian, or daily, rhythms are the things that make you alert in the daytime and sleepy at night. The best biological indicator of the circadian rhythm for scientists are your melatonin levels. Melatonin or metabolites are found in salive, urine, or blood. These levels are low for most of the day, and then sharply rise at night about 2 hours before you go to bed.  Then, they stay high for several hours before slowly declining near zero around the time you wake up. Most scientific studies look at circadian rhythms by analyzing the dim light melatonin onset, often referred to as DLMO. This rapid rise is easy to identify on daily plots of melatonin levels. So circadian rhythm studies in humans
are mostly studies of changes in timing of the DLMO. 

Blind people have poor circadian rhythms. They sort of drift from being synchronized to the solar cycles to being anti-synchronized.  Melatonin, if given to blind people around the timing of their desired DLMO, seems to keep them in sync a little better. Of all the cues that our bodies could use to keep track of daily rhythms, it seems light is by far the most important.
  
_URL_2_
  
How does all this work, though? I came across this
study:
  
_URL_0_
  
In it, volunteers are exposed to bright light at different times of the day. If the light occurs early in the "daytime", the DLMO phase advances, or occurs earlier. If the light occurs near the end of the day, the DLMO phase retards, or occurs later. Light is actually shifting the clock. However, instead of looking like a feedback system in which the light has a center, neutral, phase and either advances or retards the DLMO depending on its timing, it looks like light is supposed to advance timing. Every day, your body's natural rhythms tell your body to get tired, as measured by the DLMO, a little later than the day before. And then, when light hits your retina during the day, the DLMO phase advances. You need light every day to stay in sync.
  
But what kind of light? A very neat study found that the receptors in the eye responsible for the effects of light on circadian rhythms are blue sensitive. These receptors are not properly part of the visual perceptual system, and you cannot use them to see. They exist, as far as we currently understand, only to help keep you in sync. 
  
_URL_1_
  
Studies of depressed people and insomniacs find that their circadian clocks are generally later than those of normals. Could it be that some of the people with depression and insomnia have circadian clocks that advance each day more than normals? And that this is correctable by light exposure earlier in the day? There is now some evidence that this is the case. Light exposure early in the day has an antidepressive effect, and helps insomniacs and some depressed people sleep better. 
  
_URL_3_
  
The use of light to phase advance circadian rhythms is now
published and reasonably standard therapy for age-related
insomnia and major depression.
What like an inner monologue? How would you even tell?
Touching does not exist in the sense that you mean. Nucleons stay contained in the nucleus and at the same time separated from each other via the interplay of [strong force](_URL_2_), [electromagnetic force](_URL_0_), and the [Pauli exclusion principle](_URL_1_). The latter two are also responsible for the intuitive fact of the macroscopic world that solid objects occupy volume and cannot overlap.
To a first approximation, it's 1 - (1/2)^4 = 0.9375.

That's because for each base pair in the genome, you can ask "What is the probability that one individual child did not inherit this piece of my DNA?" Because you always pass on exactly one allele or the other (with the exception of rare things like trisomies, eg. Downs Syndrome), this probability is just 1/2. 

Because each child inherits DNA from you independently, we can just scale up and ask the question "What's the probability that none of my 4 children inherited this piece of DNA from me?", and the answer is (1/2)^4. Thus, the probability that they *did* inherit a given piece of DNA from you is just 1 - P(they didn't), which is 1 - (1/2)^4

I should note, however, that this is just an "expected" value. This means essentially that if you genotyped a large number of parent + four children groups, the average amount would be 93.75%. However, each locus in the genome isn't inherited independently, but rather in huge chunks. Thus, the number could be larger or smaller than that, depending on which actual pieces your kids have inherited. One could in principle do some math to work out what the expected distribution of possible values is, but that seems hard and I'm lazy.

Also, I've obviously ignored the sex chromosomes, and treated them as if they were autosomes in my calculation above. In fact, we know that both of your sex chromosomes are present among your kids, because you have both boys and girls. This means that my 93.75% figure is a tiny bit low, but some back of the envelope scratching I just did suggests that it's probably 94.xx% once we account for the sex chromosomes, so this approximation doesn't put us off by much.
It does. Stand two feet away from a light bulb and look straight at it. The light hurts your eyes. Now stand a mile away from the same light bulb and look straight at it. You can barely see it now. All isotropic localized sources of light create effectively spherical wavefronts for the light that spread out in all directions. Since the area of the spherical wavefront is increasing with distance as r^2 and since the total energy in the light wave must be conserved, the energy density of the light dissipates according to 1/r^2 . It's not that the light is destroyed or absorbed when traveling through free space, but that the light is stretched over a larger and larger area, and therefore must be locally weaker. 

Even a laser beam, which seems to be perfectly collimated and go on for ever, experiences divergence because of diffraction. If you look over a large enough distance of propagation, you can see the laser beam light spreads out as it travels and gets dimmer at each point.

UPDATE: Above I am describing how light dissipates in free space (no material around). Of course, if material is around, the material can absorb/destroy/scatter the light and increase its dissipation. Light traveling through a thick fog dissipates much more quickly than light traveling through deep space.
None of Maxwell's equations can be derived from the others. Although as a cool side note, the continuity equation *can* be derived from Maxwell's equations.

If any one of Maxwell's equations could be derived from the others, it would be redundant. In the interest of "elegance", we wouldn't list it as one of Maxwell's equations. 

In other words, Maxwell's equations are already boiled down into the minimum number of equations you need to describe electric and magnetic fields.

Depending on the level of math you're comfortable with, the content of Maxwell's equations can be written as [four equations](_URL_2_) (really eight if you consider that the latter two are *vector* equations), or [even fewer](_URL_0_). Indirectly, you can express the properties of the electromagnetic field [as a single scalar equation](_URL_1_), but you have to do some work to actually get the equations of motion out of it (plug it into the Euler-Lagrange equation, and surprise surprise, you get Maxwell's equations).
Well, have you ever tried to run, or walk briskly, without swaying your arms, or indeed letting them swing naturally? It's really awkward. The reason is that your arms act as counterbalances compensating for the torque that results from swinging your legs and rotating your pelvis.
The energy of a wave is not determined by the wavelength. 

The energy of the photons that make up the wave does depend on the wavelength.

If the photon flux is greater, the power of the wave is increased, and at the same time if you consider the wave from the classical perspective, the amplitude of the wave is also increased. 

In the classical view, it is exactly the amplitude of the wave that determines the energy.
The argument is subtle (the original derivation is in Griffiths "Introduction to Electrodynamics" if anyone want to consult the original source for mathematical detail), and you have to be very careful with the argument. Let's have another look at things:

1) When the charges in the wire are at rest the negative and positive charges balance (another way to say this is that they have equal charge densities in space). So if a positive test charge is at rest nearby it feels no net force.

2) Now both charges in the wire start moving simultaneously, and you have to be VERY careful when you define the way that they move. The positive charges in the wire move to the left at the velocity v while the negative charges in the wire move to the right with the *same* velocity v. 

If a charge is at rest nearby it will see that both the moving positive charges and the moving negative charges ARE Lorentz contracted by the exact same amount. The effect will cancel because the positive and negative charge densities increase by the same amount for each charge so there is no net charge seen by the resting test charge. So the test charge at rest feels no force like before.

3) Let us do 2) again, but now let's have the test charge move to the right at velocity v. The negative charges in the wire will now appear to be at rest relative to our test charge and there will be no Lorentz contraction for them. However, the positive charges are still moving to the left and WILL be Lorentz contracted even more so than before. The net result in this case is that the Lorentz contraction causes the positive charges to contract only, this increases the positive charge density over the negative charge density in the frame of the test charge, and the test charge is repelled.

Edit: I looked it up in Griffith's and his argument isn't exactly the same as above. The interested reader should consult section 10.3 in "Introduction to Electrodynamics 2nd Edition". I do agree that there is something not quite right with the video's description.
[Anosmia](_URL_0_) or the inability to smell is actually pretty common. Somewhere around 2 million Americans suffer from a dysfunction in their ability to smell ([source](_URL_1_)).  

There are many causes of anosmia. One interesting cause involves people in motor vehicle accidents. If a person hits there head in a high speed collision, they can actually shear off the nerves responsible for smell. Other causes range from nasal polyps to tumors and nutrition deficiencies to smoking.  

The reason you don't hear about it as often is that people with anosmia can live very normal lives. They don't have problems getting around like a blind person might, and they don't have problems communicating life someone who was deaf or mute might.  

Though the condition might not be as noticeable, it can have a definite effect on quality of life because smell plays a big role in flavor. Some patients don't eat enough to stay healthy. Other people with partial loss of smell have to oversalt or spice their foods to get a little taste out of it.
This is really a matter of [Mendelian](_URL_1_) genetics. 

Essentially, your child's phentoype (the physical manifestation of their genes) will be dependent on the genes received from you and your wife. Now, when it comes to phenotype things can get rather complicated, some traits are determined with a single gene, while others may be expressed through multiple ones. In genetics you have the concept of recessive and dominant alleles (genetic traits). What this means is that if one form of the gene is present, it can sometimes override the other form. Sometimes both genes can be activated, and the physical results will be a blending of the two.

For example, skin color is expressed from a number of genes, and that is how you get such a wide variance. As far as your nose question is concerned, I'm not entirely sure, it could be dictated by numerous genes and thus makes it hard to predict. 

As a side note, before Gregor Mendel many people did believe in blending inheritance, which is significantly closer to reality then other hypotheses at the time, like [Homoculi](_URL_0_).
Motivation is a really interesting subject, and one with a ton of nuance between a number of different factors that all interact with one another. Largely, reinforcement models are used for things like behavior modification, for aspects that tend to be very deeply seated within the psyche (like hoarding), or physically addictive (like cocaine).

For the behaviorists that would typically use positive/negative reinforcement the subject is an animal (human or otherwise) that does things (On a side note, negative reinforcement is the least effective, at least with children in educational settings). The things that the animal does can be predicted, controlled, and changed. Whether a subject was aware of the reinforcement or not doesn’t really matter if, as a strict behaviorist, only the observable actions were measurable. So, if I can put words into BF Skinner’s mouth: no, awareness doesn’t change anything. This apparently plays out in practice, if you were to look at casinos, which rely heavily on reward schedules, you would find that gamblers keep on gambling even when acknowledging that “the house always wins.”

BUT! If you are interested in the awareness of the system by the subject, then you can look into a number of different motivational theories that would suggest that YES! there is an change in motivation and behavior if the system is understood. Mostly, though, the theories are a bit incongruous with the idea of reinforcement (very different philosophical underpinnings between reward/punishment, and socio-cultural context). That said, goal achievement theories place a lot of weight on the autonomy of the individual within that social context. In that case, you could make the argument that knowing that someone else is setting the goal and rewards would be in conflict with the individual making a choice about their achievement. 

This gets all sorts of messy with attribution theory, expectancy-value theory, and any number of other models. Within those, a strong reinforcement (like grades, or money) will still persist when the student feels like they are being used, if the balance between tolerating being used, and receiving the reward is in the favor of the reinforcement.
Flipping a coin isn't random. Theoretically you can calculate how it will fall based on on the angle you toss it and how hard you toss it and wind resistance. It's a Maxwell's Daemon kind of thing. Quantum mechanics is the only thing that has true randomness.
Most spiders are perfectly harmless to humans, even if they were to bite you. The vast majority of spiders are not going to bite you randomly, if you are poking it, or shaking it in your hand then sure.

I would look of poisonous spiders that live in your region, and get familiar with how they look and where they are usually found. Unless it's a dangerous spider, there's no harm in leaving them in your house. If they aren't in the way, they should help keep the fly population in your house down.
Humans evolved in the wild. We were not always so completely safe from other predators. People who developed a good response to the possible threat of a predator typically survived long enough to reproduce and pass on those traits. What you consider as irrational now may have been a perfectly justified response when our ancestors roamed and foraged. Having an instinctual response to a possible risk of predation is commonly seen in other animals ranging from fairly basic animals right up to primates. It's a fairly basic response, except in humans it has developed as a complex state of alarm and corresponding changes to things like adrenaline release, feelings of anxiety and paranoia, goosebumps which (were we to have hair like apes) would make us look bigger and more threatening, also blood in your body is redirected from your gut and viscera to your limbs and brain to allow fight or flight. This is your sympathetic nervous system engaging.
Yes, that is essentially what a microphone does. If you have a magnet on a membrane surrounded by a loop of wire, as the membrane vibrates (because of sound), the magnet moves back and forth through the loop, which induces a current.
The problems which arise from inbreeding are usually due to recessive deleterious (bad) genes being expressed. Recessive means that these problems will only show up if both your mom and your dad give you the gene for it (in most cases). This is more likely with inbreeding as mom's genes are from the same pool as dad's (thus the likelyhood of them both getting the bad gene increase). Inbreeding also lowers the genetic diversity of a species. This is a measure of how different each offspring is from the other offspring in the group. Without a lot of generic diversity, a major problem that affects one individual is much more likely to affect all individuals. (Google the fall of honeybees for an example of this) 

To answer the last part, breeding outside a closed group will eventually negate the effects of inbreeding, but how long that will take is incredibly variable depending on the organism, the traits in question, and the extent and length of the inbreeding.

Also, there is no such thing as a 100% guarantee you'll produce a defect free individual. Inbreeding just makes defects more likely.
It's because the wood popsicle stick has a rough surface, which means there are many nucleation points available for crystal formation. The speed and uniformity of cooling is also important, which is largely determined by the thermal conductivity of the popsicle mold.
It's almost certainly just a large concentration of galaxies - there's no reason to believe it's anything particularly exotic. Our view of it happens to be blocked by the Milky Way, so we can't see it directly. However, the universe is not really "smooth" on this scale, and you expect to see some regions that have more galaxies than others.
Both of these are highly complex variables dependent on genetics and environmental influence such as nutrition. Specific species adopt specific patterns because of all kinds of selective pressures, such as predation, reproductive rate, resource availability, social factors and all sorts of other things which shape the DNA over many generations.

As with most things that are partly controlled by DNA, it is possible to change them by genetic manipulation. Our understanding of the systems involved are so poor that the results we've achieved in things like mice are conservative at best. I believe we've increased the lifespan of mice several fold in some instances, whether by drug interaction with the genome or genome modification itself.

Theoretically though, a thorough enough understanding of the genome and a means by which to modify it should allow all kinds of genetic modifications, and growth speed and life span are not exempt from this!
You don't need variability to detect a radar bounce. Most systems are built to send pulses because it saves a lot of power, and makes the transmitter harder to detect, but it would work just as well if you used a constant signal. 

It turns out we do something like this all the time. The sun is always emitting a very strong electromagnetic signal, in plain visible light. The "bounce" signals are the reflected light we see when we look at planets or asteroids with telescopes, or even the naked eye. 

Yeah, your very own eyeballs are passive radar sensors. Isn't science awesome?
> if we were to place a grid device (similar to the anti-scatter grids often used with x-ray image receptors) on the tube itself, you could allow only x-rays traveling in the desired direction to exit the tube.

This is what is done currently.

 > would it be possible to design a larger-format anode target?

The beam would still be divergent, because x-ray production is roughly isotropic (equally likely to be emitted in all directions).  So, if you make the target larger, what you would see is a superposition of many divergent point sources.  The result of that would be an extreme loss of coherence, and the image quality would be degraded.  It would be a similar situation to taking a pinhole camera, and making the pinhole very large.
The calculations performed while determining your position from satellite signals are extremely expensive. Its all that computing power being used that drains the battery.
[Quarks](_URL_2_) are fundamental particles that make up protons, neutrons and a whole load of other (unstable) particles collectively called [hadrons](_URL_1_).

There are six types of quark:

Name | Electric Charge | Mass 
:---: | :---: | ---:
Up | +2/3 | ~2 MeV
Down | −1/3 | ~4 MeV
Charm | +2/3 | ~1.3 GeV
Strange | −1/3 | ~100 MeV
Top | +2/3 | 173 GeV
Bottom | −1/3 | ~4.3 GeV

They interact via the [strong nuclear force](_URL_3_), which is mediated by massless particles called [gluons](_URL_0_). The charge of the strong force is called [colour charge](_URL_4_). Quarks can have one of three colour charges, called red, green and blue. Antiquarks can be anti-red, anti-green or anti-blue. All 3 (anti)colours sum to zero, and a colour with its anticolour also sum to zero. Gluons carry a colour and an anticolour, and can therefore interact with eachother.

Because the strong force is so strong, you will never observe an unbound quark on its own*. You can only observe states with zero overall colour.

Normally** there are two categories of hadron: [baryons](_URL_5_) which are made of 3 quarks***, and [mesons](_URL_6_) which are made of a quark and an antiquark.

The only hadrons that matter for chemistry are the proton and the neutron.
The proton is made of 2 up quarks and a down quark, whereas the neutron is made of 1 up quark and 2 down quarks.


---
*With the exception of extremely high temperatures in a [quark gluon plasma](_URL_2_%E2%80%93gluon_plasma)

**I say "normally" because there can exist exotic hadrons such as tetraquarks and pentaquarks.

***Anti-baryons are made of 3 anti-quarks.

---

Note: the quark content of hadrons has been simplified. In a more complicated partonic model, the properties of the hadron are expressed by the 2 or 3 permanent "valence" quarks. Gluons can spontaneously become quark-antiquark pairs ("sea" quarks).
I've noticed this phenomenon while biking too. I came to the conclusion that it's because less of the perspiration is evaporating post-exercise due to your reduced speed, not that you're sweating any more, so the perceived effect is that your perspiration levels increase.
It's thought that perception of time is closely linked to neural network speed.

In studies, it has been shown that older humans have a very poor judgement of how much time has passed, and they are always slow.

For example. We take a 18 year old. Have them close their eyes and inform us when they think a minute has passed. typically, they will be quite close. 58 seconds, 62 seconds. etc.

We then take a 90 year old, and have them do the same. Very reliably, they are late. 68 seconds, 75 seconds, etc.

The assumption we draw from this is that the perception of time goes by faster for older individuals. It is thought, then, that human perception of time is closely related to how many processes a human can do in a given time span. 

Let's say a person when they are 18 can do 50 processes per second. They live their whole life thinking that every time they do 50 processes, it has been a second, and this is true for pretty much their whole lives. However, as they get older, it starts taking them longer than a second to do 50 processes, as their brains slow down, but they do not readjust their perception of time, so they then think a second has passed, even though more than a second has.

THAT BEING SAID. The answer to your question is more or less no.

Let's take a very intelligent person. A genius. They can do 150 processes per second, whereas our average person could do only 50.
The very intelligent person simply has a different primer for what 1 second is. Every time 150 processes have happened the person thinks 1 second has passed, and the man ages as the first example had.

So... Does time pass slower for the individual? Well, not exactly. The subjective experience of the genius is certainly different, and could perhaps be thought of as "slower", but the real difference is only how many things can be done in one second.
According to this link there is probably not much difference between the two scenarios in terms of energy consumption: _URL_0_

"Despite the harping of moms everywhere, standing in front of an open refrigerator while you ponder its contents will not drive up your electricity bill, Blasnik said.

The moment you open the door, the cooled air rushes out, and it's a fairly trivial loss, he said. Most of the refrigerator's coldness is held not by the air but by the contents, and those contents won't warm up significantly in the time it takes you to decide between the leftover pizza and last night's meatloaf.

Obviously, leaving the door open all the time would waste energy, because your refrigerator would never stop running, Blasnik said. But closing the refrigerator door quickly will save you a dollar's worth of power a year at most, his research shows.

Instead of policing your teenager's refrigerator habits, he suggested changing a behavior that really does waste electricity and money: putting food into the fridge while it's still hot."
What kind of noise? What do you keep in the bottle? Is it only with certain drinks?
Pure hydrogen will not ignite in any way, nor will hydrogen-air mixtures richer than ~75% H2.  The reason is that you need a fuel source and an oxidizer to allow for combustion.  Without any (or enough) oxidizer, hydrogen will just stay hydrogen because there's nothing preferable for it to turn into (or there's not enough oxidizer to keep the reaction going).

I've never heard of a freon hydrogen mixture having special properties, but it is possible that hydrogen could co-condense with a freon compound and stay in the liquid phase at appreciably high temperatures.  It's pretty difficult to say without more information.
The second law of thermodynamics is perfectly fine with order appearing. To say otherwise is simply wrong, although distressingly often stated anyway as part of a false argument against evolution. Perhaps that's where you heard it.

The important missing part is that a *closed system* tends towards disorder. The Earth is not a closed system, and is very far from it, as it receives massive amounts of energy from the sun. This energy is the basis for almost all life, and the order increase here is more than offset by the massive entropy increase from the sun.

There's also some life around thermal vents in the ocean which seems disconnected from the sun, but this relies on the (again entropy increasing) heating of the Earth).
Paleontology is a very odd flair for this question.

Typical household mirrors reflect (nearly) all light in the visible wavelength range and in the infrared/UV range close to visible light as well, how much exactly depends on the mirror. Animals using these wavelength ranges wouldn't see differences. UV with shorter wavelength will typically be absorbed by the glass in front of the mirror.

Some animals can detect polarization, that could have interesting effects in some special cases.
Speculative, as I know of no scientific study on the subject, but I am an electronics technician and electronic warfare specialist who works with various radio and radar equipment.

It's possible that the angle of your radio station relative to the freeway (and thus overpasses) is such that just as you go through the underpass, you are being hit with radio waves reflected off the concrete  &  steel structure above you, in addition to the ones travelling on the same plane (horizontal) as you are.

That is simply a hypothesis however, and would require testing to prove empirically.
The JWST project seems to be going forward well at the moment and is aiming for launch 2018 according to the latest budget. For the project to succeed it will need continued support from NASA. NASA budget is evaluated in US congress every year and this means that the project could be cancelled. 

As for the parts contributed by Europe and Canada these parts are fully funded already. Typically in Europe a commitment to an international collaborations is considered binding. This means that politicians can not easily withdraw funding from a project such as this.
Neurobiologist here. We don't have a good or complete answer. This is one of the most mysterious questions not only in neurobiology but in all of science because we can't confidently answer it without knowing what consciousness is, and we don't. 

So the short and unsatisfying answer that we have for now is that a thought is, at least in part, a spatiotemporal (i.e. distributed across space and evolving in time), electrochemical pattern in your brain. Synchrony between neuronal action-potentials in groups of neurons may play a critical role, but we're not certain. I wish I could offer something better but I can't.
First, as you say, there are post-exposure rabies shots that can very effectively prevent the disease from ever starting.  These include both pre-formed antibodies against the virus, as well as a standard vaccine that causes the body's own immune system to make anti-rabies antibodies.  (The pre-formed antibodies fill the 7-14 day window between vaccine administration and enough antibody generation.)  This is very effective, but it's not what you're asking about.

If someone is exposed to rabies, and misses these post-exposure shots, is it treatable?  The answer is maybe a tiny bit yes, but almost entirely (and maybe entirely) no.  

Until recently the answer would have been unambiguously no.  There was no treatment for rabies following onset of symptoms; the patient inevitably died, often in excruciating pain.  In the late 2000s, though, a possible treatment for rabies was suggested, the "Milwaukee Protocol".  This basically involves putting the patient into a coma and including intensive life support systems.  A tiny handful of patients have apparently recovered after this protocol. (A paper from late 2015, [Temporal evolution on MRI of successful treatment of rabies](_URL_0_), claims to have had the third rabies survivor following this treatment.)

However, the consensus among doctors and researchers is now that the Milwaukee Protocol not only only useless, it's worse than useless.  For example, [Update on rabies diagnosis and treatment](_URL_1_) says *"The Milwaukee protocol involves induction of therapeutic coma; however, there is no clear rationale for a neuroprotective role of this therapy, many reports exist of its failures, and its use should be abandoned."*, [The "Milwaukee protocol" for treatment of human rabies is no longer valid.](_URL_3_) says *"The so-called "Milwaukee Protocol" ... has been proved not beneficial ... and has resulted in severe complications"*, and [Critical Appraisal of the Milwaukee Protocol for Rabies: This Failed Approach Should Be Abandoned](_URL_4_) says *"None of these therapies can be substantiated in rabies or other forms of acute viral encephalitis. Serious concerns over the current protocol recommendations are warranted. The recommendations made by the Milwaukee protocol warrant serious reconsideration before any future use of this failed protocol."*.  

Why did people survive rabies after the Milwaukee Protocol?  It seems that spontaneous recovery is possible (but very, **very** unlikely) after rabies.  As of 2009, seven patients were believed to have survived rabies, including some who didn't receive the Milwaukee Protocol ([Overview, prevention, and treatment of rabies](_URL_2_)).  (To put this in context, that's seven patients in the past 50 years or so, while over 50,000 people *per year* die of rabies.  That's not a very good survival rate.)  So maybe these patients were going to survive anyway, and the Protocol was irrelevant.  

So, TL;DR, it's technically possible to survive rabies post-onset, but it's not something to count on.
> What does this stuff actually look like? 

"clear to pale-yellow, viscous liquids (highly chlorinated mixtures are more viscous and deeper yellow)."

 > Exactly what is it about either the structure or the elements in it that make it so dangerous?

According to this [paper](_URL_1_) they can bind to [aryl hydrocarbon receptors](_URL_0_) or AHR. AHR is a protein that has a critical role in gene transcription. It is possible that affecting PCBs binding to AHR would alter how DNA is read in certain cases. In any event it is known that PCBs are highly toxic as you state.

 > I've also found information that some companies still make this stuff and are granted exceptions by the EPA to do so. If so, why and what are its current uses?

Some of its past and present uses: 

 > "PCBs were used as coolants and insulating fluids (transformer oil) for transformers and capacitors, such as those used in old fluorescent light ballasts. PCBs were also used as plasticizers in paints and cements, stabilizing additives in flexible PVC coatings of electrical wiring and electronic components, pesticide extenders, cutting oils, reactive flame retardants, lubricating oils, hydraulic fluids, and sealants (for caulking in schools and commercial buildings), adhesives, wood floor finishes (such as Fabulon and other products of Halowax in the U.S.), paints, de-dusting agents, water-proofing compounds, casting agents, vacuum pump fluids, fixatives in microscopy, surgical implants, and in carbonless copy ("NCR") paper."
It’s not. Necessarily. 

Well kinda. Are we talking about a solid metal pole of the same mass as the hollow pipe or a solid metal pole of the same diameter as the hollow pipe. 

A hollow pipe has a better strength to mass ratio as compared to a solid pole. That doesn’t mean that a hollow pipe will be stronger than a solid pole of the same diameter.
[Here](_URL_0_) is his original derivation. 

Basically, he utilised the conservation of momentum and energy to derive the equation. The c^2 term wasn't included. 

There are many criticisms about that derivation and there have been more rigorous derivations since that lead to the same formula.
When the temperature of a gas changes, its density changes. When its density changes, its index of refraction changes. When light passes from one substance to another substance with a different index of refraction, it travels at a different speed; and when it meets the interface between two such substances at an angle, it bends.
On a computer hard drive they don't really get removed, unless you take extra steps after hitting delete. The computer takes those files and tags them with a special character that says "these files are not needed, do not display them, and it's OK to write over this". If you want to *really* erase a hard drive you can get free software that will overwrite the drive with random data multiple times to destroy all past evidence. On a calculator it simply stops supplying the relevant RAM with electricity for long enough for it to reset to a neutral state.
It is very difficult to tell if an animal is "bored" since we don't have much in the way of direct insight into their behavior. But one can see that one gets similar behavior that looks like boredom in many birds. Ravens for example will get unhappy if they are locked in a small area with no toys. Parrots get bored in a similar fashion and without stimulus in the form of other birds or living things or toys and will then pick at themselves. In general, boredom is a behavior set that seems to be connected to smarter beings. Thus one gets behavior like an [octopus figuring out it can short out a light with a burst of water]( _URL_0_) and doing this repeatedly. Why smart beings become bored seems to be a distinct (and tougher) issue.
I don't think anyone has a conclusive model about how prions convert normal protein into prions yet.

Protein folding is a little better understood, but not completely. Most proteins spontaneously fold, driven by bonding interactions. Some proteins require chaperones to fold. I'm not aware of any proteins that use other already synthesized proteins as a template to fold.
I have a two part answer:

The general spirit of the factoid is true: If you have water that has bacteria in it, they will stay in the liquid water as the water vapor escapes. This is because evaporation is a molecular process: molecules leave essentially one at a time. And you must realize that, while we thing that both bacteria and water molecules are "very small", bacteria are *gigantic* compared to water molecules: The average bacterium is [about 1 micrometer](_URL_2_) (0.0001 centimeters, or 0.00004 inches) across, while water molecules are [0.3 nm across](_URL_3_) (0.00000003 centimeters, or 0.00000001 inches). That's about 10,000 times smaller! So you can see why bacteria don't "float away" with evaporating water: they're just way too big!

However, condensation, as well as rain and snow, are by no means devoid of life. There is actually [a species of airborne bacteria](_URL_1_) that is quite important as a form of [ice nuclei](_URL_0_), sort of acting as "seeds" which cause ice crystals to grow rapidly an them, eventually becoming snow, rain, or some other form of precipitation. They can actually be quite harmful to plants, causing frost to form a lot easier than it would otherwise, potentially killing the plant. So bacteria in condensation and falling rain are actually quite common, but only because those bacteria were already there, and the water condensed *onto* them. But that amount of bacteria is relatively quite small, so for all intents and purposes, condensation is sterile.
This is a common misconception, birds ARE dinosaurs, more specifically they are a stem group of the [theropods](_URL_3_). In the [family tree](_URL_1_) at the bottom of the wiki page you can see birds (Avialea or avidae - extant birds) as part of the group. 

Dinosaurs with bird-like characteristics and birds themselves also evolved before the KT extinction event. While it is debated, one of the earliest birds or the sister group to all birds is [archeopteryx](_URL_0_) which lived about 150 million years ago, well before 65 million years ago.

Also, [mass extinction events](_URL_2_) do not kill everything, they typically result in a loss of over 50% but closer to 75+% extinction of life forms. This includes everything from invertebrates, vertebrates, plants, to fungi etc. The worst extinction event was the Permian, about 96% of marine groups went extinct, meaning about 4% made it through.

How these groups are distributed, how specialized they are and how susceptible they are to disturbances effects their chances of survival. Land and sea reptiles did not fare well at the KT extinction, and for whatever reason - a stem group of dinosaurs made it through this extinction event, in the same way that our mammal ancestors did - these would become the extant birds we see today. Over the millennia birds have acquired new adaptations which specialize them to their many niches - thus they are not exactly the same as those that survived the KT, but then again mammals today are not the same as the ones that made it through the KT either. We too have acquired many new adaptions.

Edit: spelling
That picture comes from here:  
_URL_1_  
There are two possible pentaquark "styles." The latter is a tightly bound group of four quarks and one anti-quark—they would be bound by the color force whose field is the gluon field. The second "style" which is physically different is kind of like a hadronic molecule, a bound meson (quark-antiquark) and baryon (three quarks). Again, they would be bound by the color force, but in a different manner much like how electromagnetism causes both electrons to bind to nuclei to make atoms as well as atoms to other atoms to make molecules.  

Here's a comment about potentially telling them apart:  
_URL_2_  

We don't exactly know *which kind* were found. From the article:

 > *“The quarks could be tightly bound,”* said LHCb physicist Liming Zhang of Tsinghua University, *“or they could be loosely bound in a sort of meson-baryon molecule, in which the meson and baryon feel a residual strong force similar to the one binding protons and neutrons to form nuclei.”*  

Here's the original paper:  
_URL_0_  

***It's an exciting day in physics!***
I haven't had an oncology course in a while, so this may or may not be entirely correct, but I hope I remember enough so that the general concept is true even if all the details are not.

Im guessing your question refers to the difference between a benign and a malignant (cancerous) tumour. This isn't really a black and white scenario, but should be seen as more of a spectrum.

Benign tumors form when cell cultures exhibit hyperplasia, an excess of cells. Basically something in the cell cycle has gone wrong and they're dividing excessively. Malignant tumors on the other hand are what we call tumors composed of cells with altered morpholigies. Basically these cells are so far mutated that they no longer resemble the culture they originally came from. As well, they display invasiveness, breaking through the basal lamina, and display potential to spread to other areas of the body.

Basically, a benign tumor has acquired some mutations, but nothing harmful and cells are simply dividing a lot. Malignant tumors are further mutated (there's some name for this spectrum that escapes me, and it defines a few more stages than these two, but oh well), and display altered cell characteristics.

EDIT: Just realized I only partially answered your question haha. They know the difference by taking samples of the tumors (a biopsy), and looking at the cells under a microscope to see if the cells are malignant or not. As well, further non-invasive methods exist to see if a tumor is potentially harmful. Looking for unique or out of place characteristics in a certain tissue type, seeing if the tumor is consuming an excess amount of glucose, seeing if angiogenesis has occurred, as well as other fMRI techniques. (Angiogenesis is the process by which tumors 'recruit' blood vessels to supply themselves with oxygen) etc. etc.

From what I understand (again not actually an oncologist), benign tumors are left alone if they're not causing any issues, but still kept an eye on incase further complications arise. If they're in the spinal cord or brain or something and exhibiting pressure on the nerves, or blocking major ducts etc etc. obviously they're gonna be removed though (if possible).
The geological/geographical definition of the north pole is the spot where Earth's magnetic field lines are vertical with respect to the surface. These magnetic field lines continue into the Earth where, at some depth, things become complicated. The field is generated by the motion of a conductive fluid, mostly molten iron, in the outer core. The field we see at the surface is an average expression of the chaotic fluid motion deep down.

So long story short, the pole is both geologically and geographically at the same place...on the surface of the Earth where the field lines point down.
Sound waves definitely *are* affected by the wind. Since sound waves travel through a medium, and wind is a bulk flow of the medium, the sound speed in a windy environment (which is normally the same in all directions) suddenly becomes direction dependent. Specifically, the speed of the wave becomes the speed of the wind in that direction plus the speed of sound at rest. Moreover, since wind tends to move faster the higher you move up from the ground, there is usually an effective sound speed gradient as well. In the presence of a sound speed gradient, sound waves tend to refract towards regions of lower sound speed. As a result, sounds sent out against the wind will tend to refract upwards, and sounds made with the wind will tend to refract downwards. Sounds made cross-wind will tend to refract downwind and up. And since your listeners tend to be near the ground (relatively speaking) the net effect is that sounds carry further with the wind than against it.

As for electromagnetic waves (light/radio), I don't believe there is any notable effect, but I would wait for verification from someone with more experience in the field.
Tornadoes are localized wind vortices that form around storms or weather fronts. Hurricanes are weather systems that increase in intensity when warm most air rising off of a body of water strengthens the convection within it. Land does not provide that warm most air.
Sure. The Moon is tidally locked, so you always see Earth nearly in the same position in the Moon's sky. As the Moon moves along its orbit you'll eventually see a crescent Earth, full Earth, waning Earth...

The fun fact is that they are exactly opposite to Moon phases on Earth, i.e. you'll only have a full Earth if we have a new Moon. Their relation to eclipses stays the same, though (i.e. you'll only see a new Earth covering the Sun, never a crescent or full Earth).
No, you cannot change the interference pattern.

In fact, in the quantum eraser *there is no interference pattern* with particle B. That is, there is none until you correlate it with the measurement of particle A.

You can measure particle A but you have no control over the result. Until you communicate the result to the guy watching particle B, he will have no idea what you measured, and until that point, all he will see is that particle B has landed somewhere completely random.

To put it simply, quantum entanglement is correlation, not causation.
The first issue is actually collecting it. The Sun belts out a truly mindboggling amount of energy, but in order to use it we need to build solar power plants to convert it to electricity. These can be photovoltaic cells or solar-powered boilers (or, arguably, wind or hydroelectric generators, since these are initially driven by the Sun's energy, but I digress), but they need to be built and you need a lot of them, which requires a lot of land and a lot of resources.

The second issue is distribution; obviously, only half of the Earth is lit by the Sun at any one time, and there's a lot more solar energy available at the equator than the poles, and some places it's cloudy, and so on and so forth. This means you need to distribute power from wherever it's available at a given time to where it's needed, and/or store it when it's available for when it's not. Unfortunately, neither long-distance power transmission or power storage are particularly cheap or efficient.

There are proposals to deal with this sort of thing, e.g. geostationary power generation satellites, which would be lit by the Sun for the majority of the time and not affected by weather, and beam diffuse microwave power down to huge receivers strung over farmland/etc on Earth near where it was required. These are engineering megaprojects requiring an enormous investment and amount of collaboration to get started, and so somewhat infeasible, even if they'd work.
We can already see the effects of restricted content on academia through the paywalled publishing practices of most journals. The high cost of institutional licenses or large-scale purchasing of individual articles can be an overwhelming expense for new companies or smaller universities. Science relies upon the free flow of information and knowledge between persons and institutions around the world. Ending net neutrality puts that at risk.
Because it's more efficient to flush things with spinning water, the drains are constructed to let water in at an angle. This makes it spin and gives the direction - same drain will spin the water in same direction at any place on Earth. Coriolis effect from Earth's rotation is too weak to influence the direction water flows in small drain in normal conditions. Only when you get to large atmospheric currents, then the effect adds up.
I don't believe that has been studied or considered before.

It is a reasonable question, but there are as far as I'm concerned too many yet unanswered variables for the beginning of an answer to be formulated. The most glaringly obvious are as follows:

* What commodities would be mined, and in what volumes, at what rates?

* Would ore be treated in space and only the finished product be imported to Earth, or would the ore be brought down in bulk and refined here?

Those questions need to take into account the following considérations:

* there are very few commodities for which space mining might make economical sense considering current prices for displacing a unit of mass in space.

* we do not currently have proven technology to mine ore in space. Anybody saying current methods used on Earth are directly transferable to space is either ignorant or delusional, as our mining and drilling methods depend on large amounts of liquid water and gravity. This also holds true for mineral separation, ore treatment and refining/smelting.

* Until proven otherwise, it might very well be that the only method within our current reach for mining météorites is crashing them on Earth and mining/smelting them here. In that context, your questions makes a lot of sense.

Things like Phosphorus might (perhaps) be a problem, especially if massive amounts of meteoric material are pulverised as dust at landing and settle in océans of watersheds. Might encourage algal growth, and accelerate eutrophisation in some environements.
Ripples are waves, and all waves exhibit the following five behaviours:

Interference, reflection, refraction, diffraction, and polarization.

The first one of these, interference, is what occurs when two sets of ripples meet. When two wave peaks coincide, they interfere constructively to create an instantaneous peak which sums their amplitudes, creating a much larger instantaneous peak. Similarly, when two troughs coincide, you get a much deeper instantaneous trough. If you have a peak and a trough coinciding, and they have the same magnitude, they will sum to zero.

The speed of propagation is unaffected.
What you’re referring to as the beak is technically called the ramphotheca. It’s made out of keratin, like hair or fingernails. It’s one piece of the larger structure of the beak. Below the ramphotheca is skin (dermis and epidermis). It supports blood vessels, nerves, and connective tissue, and also houses the cells that grow the keratin that makes up the ramphotheca. That layer is adhered to the bone of the beak. 

There is a lot of variety in bird beaks, so this is a general overview!

Source: Van Hemert, C., Handel, C. M., Blake, J. E., Swor, R. M.,  &  O'Hara, T. M. (2012). Microanatomy of passerine hard‐cornified tissues: Beak and claw structure of the black‐capped chickadee (Poecile atricapillus). Journal of morphology, 273(2), 226-240.
Studies suggest this is what causes Schizophrenia. Also, this is where "amphetamine induced psychosis" comes from. Though the nomenclature is a little misleading, it can stem from any dopamergic stimulant, including cathinones and morpholines such as methcathinone or methylphenidate.
Over-the-counter eye drops advertised to treat red eyes are typically decongestant, meaning they work by constricting blood vessels. Look at the main active ingredient, then search for the mechanism of action of that drug. That will give you more straightforward information than just searching by the trade name of eyedrops.
Yes, as you start moving faster the wind will start pushing back against you which will cause as a resistance. As long as you are moving at the same speed as the speed of the wind then there will not be any resistance.
From an undergrad perspective, we treat inertia almost like an axiom. That is, there is no explanation given, we assume that it is a fundamental property of nature.
Many simulations can give very accurate predictions of some subset of the universe. But a perfect simulation of the entire universe is impossible for a few reasons:

1. The universe is fundamentally quantum (i.e. probabalistic) and not deterministic. Put aside the problem of simulating the whole universe for a second. Even a single atom is impossible to simulate perfectly because it has innate quantum uncertainty so that we do not know exactly what it will do next. Classically, if you know the exact position and momentum of every object in a mechanical system, you can exactly predict the behavior of the system out to eternity. But quantum uncertainty means that it is impossible to know the exact position and exact momentum of an object at the same time.

2. A perfect simulation of the entire universe would require simulating the computer itself that is making the simulation, and simulating its simulation, which would require simulating a simulation of the simulation, and on and on in an infinite regression. An infinite simulation (infinite because of the regression) would require infinite processing power\*, which cannot exist.

3. We can have no knowledge of the parts of the universe outside the observable universe, as observable from Earth. You can't wave this problem away with technology. The finite speed of light places a fundamental limit on how much can be known about the entire universe. There are simply points that are too far away for their light to have reached us yet. This problem is the same everywhere. Every point in space has its own, finite observable universe which is a subset of the entire universe. Since spaceships cannot travel faster than light, flying to different observation points won't help much.

With all of that said, we can make excellent approximate simulations if we are clever. For instance, the exact location and momentum of every ant and flower on Earth does not effect much its orbital motion through the solar system. We can use a more course mass model for Earth and still get good simulation of its orbital motion.

\* UPDATE: I should have said "infinite processor memory" and not "infinite processing power". Given enough time, you could in principle run the largest of calculations on the slowest machines.
Its enzymes.

When you eat food and it goes through the digestive tract it normally goes through the processes of digestion in which enzymes and such break down the food and in the process degrade.

When you have diarrhea the food has been rushed through the small intestine which means the enzymes haven't had a chance to degrade. This causes that burning sensation.
Yes, receptors at the synapse are frequently recycled, downregulated, and upregulated. This process contributes to synaptic plasticity, the strengthening or weakening of synapses.

For example, if a neuron fires frequently on another in a memory region, that synapse will be strengthened: more AMPA/NMDA/Glu receptors will be inserted into the synapse and enhance firing, decreasing the threshold for an action potential (firing). The synapse will be strengthened. Conversely, a synapse can be weakened as receptors are downregulated. When you get out of practice at something, this is partly due to synaptic plasticity that weakens the association from lack of use, whereas while practicing the skill/technique/whatever, long term potentiation and other processes at the synapse will enhance its connection.

For instance, this is how opiate addicts develop tolerance and withdrawal. Generally speaking, constant use of a drug will, over time, result in downregulation of the receptor for that ligand and change the firing properties of those neurons. Thus, the brain acclimates to the presence of the drug (in this example an opiate) and downregulates neuropeptide and mu opioid receptor production, because it doesn't need to make as much--you're supplying it--and so the effects diminish and you just feel "normal." Certain side effects can still increase as more of the drug is taken, because at a high enough dose, the drug will start to activate other off-target receptors that would normally not be stimulated much.

During opiate withdrawal, the body suffers from a lack of neuropeptides, hormones, and receptors, that would normally be made if the opiate wasn't always in the system and suppressing their production. So, withdrawal symptoms are usually the opposite of a drug's effects on the body, and a lot of this has to do with receptor upregulation, downregulation, and recycling--your body's way of replacing receptors at specific times.
Wet skin has a higher coefficient of friction. See [here](_URL_0_). Theoretically that would result in more energy transferred. 

It also has an inverse relationship to the normal force, so a glancing blow would result in more force from friction. That would support the idea that the skin-surface sting from a slap comes specifically from friction.
The Real Time clock does the magic: _URL_0_ . It is powered by a battery while your computer is off.

In some very old desktop PCs you may find that the RTC's battery has been degraded by too many charge/discharge cycles. Those computers lose the date and time every time you turn them off and wake up "believing" it's the year 2000 (or whatever epoch the manufacturer has chosen).
This happens routinely and is part of the normal immune defence to a pathogen you've never seen before.

1) Pathogen enters the body by whichever route you prefer - let's say you stand on a thorn or something.

2) The pathogen triggers the innate immune system which detects highly invariant molecules which are **never** found within your body such as lipopolysaccaride, a component of bacterial cell walls. 

3) The innate immune system kills the pathogen through various methods which I won't explain here

4) Bits of the pathogen are digested by your immune cells and presented on the surface. They then travel from the site of infection to your lymph nodes or spleen. Here they wander about waving their bit of pathogen they have on their cell surface to all the naive T cells present in the lymph nodes. 

5) One of the T cells will recognise that bit of pathogen, activate, and clonally expand producing millions of identical T cells specific for that bit of bacterium

6) One of those activated T cells will go and find a B cell also specific for that antigen and activate that B cell. The B cell receptor is a membrane bound antibody and upon activation, the B cell also undergoes clonal expansion, becoming an antibody secreting plasma cell.

7) Antibodies are secreted on a huge scale and clear the pathogen from the system, again by various methods.
The unit lumen is based on the associated unit candela, which is essentially the amount of light emitted by a "standard candle", which used to be a form of providing a standard for luminous intensity. Lumen is the unit that measures the total luminous intensity of a light source, while candela are used to express the luminous intensity per unit of solid angle (which is somewhat equivalent to the surface area of a part of a sphere at fixed radius).

The luminous intensity is not exactly the same as the power output of a light source, because the luminous intensity is weighted with the sensitivity profile of human vision. Light sources that human eyes are sensitive to will count more towards the luminous intensity. The peak of sensitivity of human vision is at a wavelength of 555 nm, which corresponds to monochromatic green light.

So if we want to make a light source that maximizes lumens per Watt, that light source would be made to emit only green 555 nm light. Light itself has energy, so when you produce light, you convert some of the input energy into the light itself, while the rest ends up as heat typically. The more efficient the light source, the more of the energy ends up as light and the lower the heat production. An old tungsten light bulb will become hot far more rapidly than a modern LED bulb that emits the same amount of light.

As it turns out, the amount of power that is needed to make a green light appear just as bright to a human observer as the aforementioned standard candle (which is just a candle with certain predefined properties regarding composition, size and burn rate. Not terribly accurate, but historically the best way to at least get some amount of standardization in units) is about 1/683 Watt, so 1 Watt worth of light would generate luminosity equal to 683 standard candles..

The definition of lumen/candela got shifted around a bit as time went on, because people realized that using a specific candle as a benchmark unit for luminosity wasn't that accurate. Ultimately, the definition of a lumen is now simply "the luminous intensity of a monochromatic 555 nm light source producing 1/683 W of light".

So the simple answer to your question would be: Because by definition 683 lumen contains 1 W of light, so producing 683 lumen at perfect efficiency would require 1 W of input power.
Your motor neurons become completely active usually an hour after you wake up.

EDIT: "The release of certain neurotransmitters, the monoamines (norepinephrine, serotonin and histamine), is completely shut down during REM. This causes REM atonia, a state in which the motor neurons are not stimulated and thus the body's muscles do not move"

So yes, your motor neurons aren't "awake" because these neurotransmitters are still active.

[Source](_URL_0_)

EDIT 2:

So yes, your motor neurons aren't "awake" because these neurotransmitters **aren't** active.
This is not possible to calculate because there is not enough data. You need to ask a narrower question.
The stomach does, to a certain extent, prepare for food to be consumed by using your cirrcadian rhytems. If you eat on a regular schedule, you body will tell you stomach to begin secreting the mucus lining that will coat the entire inner surface of the stomach. this mucus is special as it contains granules of bicarbonate to dampen the acidity of the stomach acid. All you body cares about is food, and unless you are a person that is changing form being a vegetarian to a omnivore, then there really isnt much of a difference  in how your stomach prepares. The only time it does make a difference is if you regularly eat a small meal in the morning, but then one day you eat an entire buffet instead, and in that case you may feel indigestion as you body thought you were only gong to have a Muffin.
Couple things... 

 >  what is the absolute most weight I could bench press before my body physically could not take the weight?

You'll never reach this because you are limited by your central nervous system. Your bones and muscles are a lot stronger than how much you can consciously use them.

Furthermore, every 6ft 200lb man will have different bone thickness, density, muscle mass, genetic differences in muscle utilization, metabolism, recovery, etc. 

Basically, there is no way to calculate this accurately. The best thing would be to look up your weight class and see what the current powerlifting maximums there are.
There would be no pattern that would be beneficial over any other pattern. The most the parents could do is have as many children as possible, make their children have as many children as possible, etc.The problem with small populations is the tendency towards loss of heterozygocity. Meaning that, over time, individuals will end up with two copies of the same (often harmful) genes, thus driving the population to extinction.
I would suggest that a mutation is more like changing a single letter on a page in a book. These mutations typically do not have positive impacts, most are deleterious, and more effective organisms (virus, bacteria, cells, etc.) out compete the now less-effective mutated organism. If the mutation is serious enough that the book is illegible (continuing your analogy) than the organism will die out and we'll never hear of it.
There are techniques that can assess the amount of aberration in the cornea, regardless of whether or not the patient is communicative. One technique is to use a specialty piece of equipment called an [autorefractor](_URL_0_). 

As noted in that link, autorefraction is more properly a starting place than an end point for an exam, and that a thorough clinical exam is required for the best correction possible given the lack of feedback from the patient. Sometimes "close enough" has to be "good enough".
You'll find that most solutions are _neutral_, so whatever method by which you drop the pH to 1, you'll have a corresponding amount of counter-ions such that the overall charge is zero.

For example, at pH 1, your H^+ ion concentration is 0.1 M (**not** 10^14 M). If I make that solution with HCl, I would have approximately 0.1 M Cl^- ions in the solution as well.
I've not heard of this particular object before, however, a quick search kicks up [this paper](_URL_0_). It sounds to me that it's probably some sort of accreting black-hole system (which is what I would have guessed anyway), with the radio emission coming from jets of matter fired out of the system.
The output of a laser is generated by [stimulated emission](_URL_0_). At the level I work at, the best answer to your question is, "that's just how stimulated emission works". 

A slightly deeper (but really pretty hand-wavy) explanation would say that the incoming photon in stimulated emission excites an oscillatory behavior in the atomic system, and naturally the phase of the resulting oscillation has a fixed relationship with the phase of the photon that produced it. (Imagine if you were pushing someone on a swing, you'd expect the motion of the swing to have a fixed phase relationship with the force you apply, otherwise you'd get whacked in the face when the swing swings back while you're pushing forward)

Possibly a particle physicist could make an even deeper explanation, but that's beyond what I need as an engineer to understand lasers.
Corking a bat does not allow a ball to be hit farther.  by changing the center of gravity of the bat, making it easier to control, it may allow a batter to more easily hit the ball.  But if you take two perfect swings and and change only whether bat was corked, the ball hit by the uncorked bat will almost certainly go at least as far if not farther. 

The distance a ball goes mostly relies on the memetic energy of the bat and the kinetic energy of the ball when they collide, plus angle of launch, wind, many other factors.  But let's assume a perfectly square contact with no wind, spherical cows, blah blah blah... And just look at the bat.

You can increase a bat's KE a few ways.  

* Keeping its velocity constant and increasing its mass

* Keeping its mass constant and increasing its velocity

* Doing both

One can lower its KE by doing the inverse of the above.  As it turns out, corking a bat does allow a player to swing the bat faster, but the  increase in velocity is offset by the decrease in mass. 

Some may argue that the bat becomes "springier" because of the cork, but this will only be detrimental because it decreases the electricity of the collision.

For maximum energy transfer in any collision, you want the collision to be as close to perfectly [elastic] (_URL_2_) as [possible] (_URL_0_). Any compression of either the bat or ball lowers the elasticity of the collision.

That is to say some of the energy is transformed into heat and sound because you deform the material.  The more you deform it, the more energy you lose.  

The misconception here is often the result of observing that rubber balls seam to bonus higher than, say, steel balls when dropped or thrown at surface.  It is assumed it is because their springiness imparts more force by helping the ball by actively pushing of the surface.  This is isn't the case.  

What is really happening is the deformation of the rubber will allow the impact to occur without damaging the surface because the impact force is spread out. When a steel ball impacts a surface, it's often the surface that is caused to deform destructively.  This steals a lot of the energy from the system, much more than when a rubber ball deforms on impact. 

Now, let's say we we throw bother the steal and rubber balls at a hardened steal plate on the ground.  Assuming the hardened steal can withstand the steal ball without deforming, the steel ball will bounce higher every time.  It loses less energy in deforming than the rubber ball. You can see in this video that ball bearings can indeed bounce very high. [Bouncing Bearings] (_URL_1_)

You may say "Wait, I can jump much higher on a trampoline, it it is essentially a spring, the same as the corked bat!"  Sure, you can jump higher, but not on the first jump. In fact, your first jump will not get you nearly as high on a trampoline as jumping on solid ground will. On subsequent jumps, however, it allows you to use the energy you've gained due to gravity more effectively.  

This happens for two reasons

* By slowing down your deceleration, the impact with trampoline allows you to remain more rigid than you would if you hit the ground, thus you bounce higher.  If you could remain perfectly rigid, you would bounce quite high when you hit the ground, but you can't

* By slowing the deceleration, the trampoline allows you to impart more of your own force by pushing with your legs as the trampoline reaches the bottom of its deformation.  This then allow you to add your force to that you gained from gravity, jumping higher on consecutive jumps until you reach the maximum amount of energy the trampoline can store.  

What this means is: if you didn't deform when you hit the ground and could push of fast enough and fat the right time, then you would be able jump higher on the ground than you could on a trampoline because the trampoline would steal more energy through deformation.

Since a uncorked bat hitting a ball is more rigid than a corked bat hitting a ball, and nothing in the system requires extra time for the energy transfer, there is no benefit to the increased springiness of corking.  In fact, corking is detrimental because of this added springiness.

What is the moral of the story?  While you may increase bat speed by corking, the increased deformation of the bat plus its lowered mass negate any improved launch distance. You may, however, be more likely to hit home runs because you gain bat control.

Edit: formatting.  It's too late Cleen up anything else right now!

Edit 2:  fixed my edit.
So these "big" non-luminous objects are called [Massive Compact Halo Objects (MaCHOs)](_URL_0_). And it seems by trying to measure the gravitational lensing of these large objects, we've put a limit on just how much of the galaxy's total dark mass can be in the form of these MaCHOs. Thus, we tend to think that dark matter is instead probably in the form of WIMPS, Weakly Interacting Massive Particles, the "cold dark matter" hypothesis.
Yes and no. You can have cuts that don't affect major blood vessels such as veins and arteries but without bleeding at all the cut can not go deeper than the epidermis (outer most layer of the skin). Once a cut reaches the dermis (2 mm at the most) you will start to notice some bleeding from sub vessels In the dermis.
It depends on the cause of the subfertility.

If IVF were done with donor egg *and* donor sperm, the child would be at no higher risk for infertility than average.

If only the father was subfertile, the IVF would have been done with donor sperm, and again the child would be at no higher risk for infertility than average.

If the mother was subfertile and the IVF done with a donor egg, same as above.

If the mother was subfertile and but IVF was done with mother's egg, it would depend on the cause of the subfertility. For example, if the cause was blocked Fallopian tubes (which is usually due to previous pelvic infection), the chance of subfertility in the child would not be increased. On the other hand, if the cause were polycystic ovarian syndrome, there would be an increased risk of a female child having the same condition.
I could flip the question and ask why Felidae (housecats to cheetas to tigers) has so much more variety of physical characteristics than Canidae (wolves to foxes to dogs).
They don't know what you're putting inside of the cabinet or where you're putting it.  Those will significantly affect how much it can cool.  They *could* give the cooling rate in Watts under some reference conditions, but then it's up to you to know how those reference conditions compare to what you're doing.  By specifying in W/K they shortcut that part.  If you know the ambient air temperature and the power of the equipment inside the case then you can quickly compute what the temperature inside the case will be.  From there it's up to you to decide if that's an acceptable temperature.

Yes, if you use wild numbers like 100 C then you'll get some absurd heat transfer rates.  You should avoid applying numbers like this far outside of the range where they're designed.  While some processors may reach 100 C under extreme load and some cabinets may be stored at 0 C for some reason, this is so far from typical that the 100 W/K may not be a reasonable value under those circumstances.
Well we don't really have many digestive enzymes for vegetables. Vegetables which get broken down to cellulose which are  β-linked D glucose chains contain a lot of "potential" energy. We do not have the enzymes necessary to break those bonds and extract any glucose.
Your body, however, will generally produce less of an enzyme if it is not being utilized as it exhibits and unnecessary metabolic cost.
Silicon crystals are produced in a process where they are continuously rotated and grow outward radially. It produces ingots like [this](_URL_0_), which are then sawed into the circular disks. As far as I know there is no process for growing similar quality crystals with a square cross section.
Pressure change is what crushes you - if you were always born at that depth you're fine. Conversely - when deep sea creatures are brought to the surface too rapidly, they explode. Remember that subs are crushed because inside they're pressurised for humans to survive, therefore there exists a huge pressure differential between inside and out the sub.

Pressure notwithstanding, there are many challenges of living in the deep ocean, many of which we're only beginning to appreciate now.
There's a lot of stuff going on.

Proteins are denaturing in the heat, sugars are caramelizing, sugars are reacting with amino acids in [Maillard reactions](_URL_0_) to form delicious brown stuff, stuff that's actually burnt is undergoing hyrdolysis and oxidation.

As a specific example, egg white is mostly a protein called albumin. Proteins are made of complicated folded strands of amino acids, and heating it causes it to "denature" and partly unfold. Denatured albumin is white instead of clear, leading to cooked eggs.
> I heard it's only really deadly if you happen to be allergic to the venom.    
    
This *may* be true for some of the widow's cousins. The *Steatoda* are the false widows and are usually said to be harmless, but [recent research](_URL_0_) has found that they're more dangerous than we think and that the widow antivenom works when the false widow bite does require treatment. But others have suggested that it would only be a problem for some people, and not everyone. So in the false widows, it's not clear and may only be medically significant for vulnerable people.         
   
And I think that's probably what you're talking about here. Black widow venom affects any person that is injected with it. It may affect some people more or less than others, but it's not an allergy kind of thing. It's pretty toxic. 
      
 > he was bitten by a black widow in his sleep ... and that it just caused agonizing pain all over his back.      
     
This is not implausible. Some of the earliest symptoms of lactrodectism (black widow envenomation) would be pain and cramping in the (abdomen, chest and ) back. Also the spiders can vary the amount of venom injected, so he could feasibly have gotten a low dose of venom and only showed the mildest symptoms.     
     
 > So are these spiders really as a deadly as people say?   
     
I don't really know much about the recluses, so I'll not comment on them. But I live in black widow land. It's just packed with them here. I get brown widows and false widows in my side yard and my back yard is full of black widows. So I've paid some attention to the research on these species. Seems like the false widows may be more dangerous than we would expect. Seems like the brown widows aren't very aggressive and they don't seem to pose much threat to people. Black widows seem to have gotten a somewhat undeserved reputation. It's *very rare* for anyone to die from black widow bites. To a large extent people won't even be given the antivenom because you can only have it once. So unless you're actually in a life-threatening situation they will save it in case you get bitten again in the future. So it's less dangerous than people would think, but it's not something you want to try to handle.
Generaly founded is just that enough people are there and those residents decide they want to be a town.  planed cities do happen but they are very rare and usually  due to the wims of a ruler.  The examples I am aware of are DC, st. Petersburg,  and Brisilla.
> Is it done by one antibody "arm" on an antigen on one cell and its other "arm" on another cell, holding two cells together?

Yup, that's pretty much it....but it's not just cells. They make clumps of other stuff, too.

Keep in mind that some classes of antibody are better at some effector functions than others. For example, [IgM](_URL_1_) is much better at agglutination than [IgG](_URL_0_). 

Also keep in mind that each particle has more than one instance of the antigen on it. That allows for more connections than just the one. If one particle has 25 instances of antigen on it (an absurdly low number), and 10 of them have an arm of IgM bound, then each IgM can bind several other particles. Each of those particles can have 25 more binding sites...10 of them are bound....and so on.
I'm going to share some of what I picked up in two quarters of biochemistry. The TL;DR is that when you're talking about things like cells, viruses, even organ systems, it's about feedback and feedback loops. A happens, A causes B, B causes C through G.

On a cellular level, there is guidance to what happens, but not perfect control, and not conscious control. Proteins have sequences that fold in predictable manners governed by thermodynamics. These proteins can be changed by reactions with other proteins, processed and altered, but reactions in the cell are still chemical reactions that depend on energy sources and favored directions of reaction. A lot of these things are controlled by moving compounds around in the cell, but again this is accomplished with specific pathways. Keep in mind that all of these processes are not accomplished by conscious choice, but by proteins, membranes, and other molecules that function according to the laws of physics. Our genetic code orchestrates all of this, but you can be sure that a strand of DNA isn't thinking, it's simply being read and transcribed.

Our bodies do a magnificent job of keeping us from reaching equilibrium, but the cells make no choices about it. Viruses don't choose what cells to infect, they move around the body and then they come in contact with the right type of cell and they attack. Your stomach doesn't choose to digest certain proteins, it releases enzymes that degrade the proteins that they contact.
Sight and hearing come from well studied receptors that work along easily measurable physical scale, i.e. light and sound frequency. 

However, for taste and olfaction there are are thousands of receptors each keyed to different molecular structures that don't follow patters (at least not in the way light does). In addition, these receptors, unlike those for sight, are not necessarily shared by all humans ([e.g.](_URL_0_)).

So think of this analogy: there's a painting you like it, but your friend doesn't. Unless one of you is colorblind, you are probably sensing the same lightwaves reflected from the canvas and paint. Now imagine if you could see colors your friend can't (or vice versa). The latter would be like taste/olfaction.

Then, there's not only a subjective side to flavour, but also a physical inability for some people to sense some tastes or smells you can.

EDIT: To clarify, I also talk about olfaction because smell works along with your taste buds to create flavours. Just try covering your nose while eating, the flavour can change quite a bit.
Red blood cells are quite large compared to other cells, so haematogenous spread isn't really limited by blood vessel calibre. But the blood brain barrier coats the brain capillaries and it's not exactly well understood how different types of cancer cells negotiate their way through the blood brain barrier.
[Here's a site which goes through some of the mechanisms involved, but I'm not going to comment on it's accuracy or thoroughness.]
(_URL_0_)
The easiest answer is polarizability and nuclear charge (i am sure there is some issue with nuclear spins as well but I will assume they are comparitively weak).

The metals, the solid metals, really want to delocalize their charge as the charge ratio is not perfectly even.  Where as hydrogen is exactly charged balanced since it has 1 electron one proton, and they both see each others full charge.  Well you then think that Li has 3 electrons and 3 protons why isnt it charged balanced?  Yet the problem is that the nucleus does not see 3 full electrons.  Instead some are on average further away from that positive center diminishing their total attractiveness to the nucleus.  This makes the total energy much more stable if the atom can then share its electron among a bulk to receive the exact charge ratio it desires.

This increasing distance from teh nucleus also leads to polarizability, the ability for an external charge or force to cause distortions in the electron cloud giving an uneven charge distribution to the atoms.  Larger systems are more polarizable as their electrons further from the + center and thus  are more weakly bound.  This induced charged can lead to internuclear attractions between atoms further stabilizing bulk materials.
Most of it does not undergo fission, only a small fraction actually does.

It takes about 0.22 Moles of fission reactions to produce 1 kiloton of energy. So for the nuclear weapon that destroyed Nagasaki (21 kilotons) it took about 4.6 Moles of fission reactions to produce that amount of destructive power. Pu-239 has an atomic weight of... 239 amu, so 4.6 Moles would weigh very close to 1.1 kilogram (about 55 cm^3 or just under 2 tablespoons). In comparison, about 6.2 kilograms of Plutonium were used in that bomb, so only about 17% of the fissionable material underwent fission.

In the case of the "Little Boy" bomb dropped on Hiroshima about 0.8 kilograms out of 64 kg of Uranium fissioned, or about 1.25% of the total.

As far as a bomb going off near fuel rods, there are interesting aspects to that. The thing that makes a fission bomb "go" is that the population of neutrons grows at a geometric rate. And this occurs because when the bomb has been assembled and is detonating a neutron produced via fission is about as likely to run into another fissionable nucleus as it is to escape from the core. This is as much a function of density as it is of sheer size, which is what implosion assembly (which increases density) is all about.

So if you have a hunk of fissionable mass nearby that isn't at the proper density you aren't going to magically be able to get it to start magnifying the number of neutrons just by throwing more at it. If that could happen it could happen with the number of neutrons already in the reactor.

However, that doesn't mean nothing happens, something actually will happen. If you increase the neutron flux on fission fuel rods you won't get them to spontaneously have fission chain reactions that spiral out of control. But what could happen is that for some of the neutrons that left the bomb they could multiply their energy release by causing a fission reaction.

In general you'll have on the order of as many neutrons produced as the number of fission reactions that have taken place, so if you have a huge amount of fissionable material packed tight around a bomb you could easily double the yield in this way.

Note that because we're just talking about one off fission reactions from fast neutrons and not multiplying neutrons this can happen even with natural U-238.

And this is exactly how real bombs work. Often they use a "tamper" of heavy metal round the core of the bomb to delay the bomb blowing itself apart quickly (so that the nuclear reactions have more time to run before everything is too spread out). And if a natural Uranium tamper is used then you can increase yield.

The Tsar Bomba bomb, for example, was designed to use Uranium around the 3rd fusion stage, to make use of the neutrons produced by the fusion reactions. Such a design would have doubled the yield of the bomb (to 100 MT). But in testing the Soviets decided to avoid producing so much radioactive fallout from all of those fission reactions and instead used a different material for the tamper, resulting in a 50 MT yield in the test.
Beauty in many cases correlates to how livable somewhere is, such as the many paintings of forests and nature scenes you can find.

Also, much of it contains patterns that our brain notices and understands, though we actively cannot pick them out.
There isn't a straight answer to your first question. Some diseases are self-limited, some are not. Things like TB, Histoplasmosis, Mono, HBV, HCV, and Herpes lead to carrier states after a primary infection. Thinks like Rhinovirus, food poisoning, cellulitis, and Whooping Cough are one-and-done, usually with the establishment of moderate to long term immunity

For your second question: Vaccines are sometimes used to treat the acute infections, but some bugs like HBV will persist after a vaccine is given, but getting the vaccine before exposure prevents infection. Odd, I know, but it has to do with the specific immune response generated, which is outside the scope of this question. 

The infectious period is different for every bug. If you're asking about Pertussis (Whooping cough), then you're infectious for the first stage (Catarrhal) and part of the couging phase (Paroxysmal stage).
It's due to the application of a type of algorithm known as [subpixel rendering](_URL_1_), which is a form of [anti-aliasing](_URL_0_). It makes small text easier to read on LCD and OLED displays.
The metal bearing act as a magnet. One could see it as if it has a lot of little magnets inside. Usually these magnets point to all directions, so it acts as if it is a neutral object. These little magnets are called [magnetic domains](_URL_0_).   
If it is attached to the north-pole of one magnet, these little magnets will all be aligned to the same direction. It takes a bit of force to do so, but when they align the bearing becomes a weak magnet itself.  
When you bring the other magnet over with the same pole, the little magnets, also aligned to the north-pole, all start to repel the new magnet.  
Until you get close enough, and then the little magnets are able to switch their direction. Now each little magnet close to the new magnet, receive a magnetic force from the old magnet (together with the other little magnets) and from the new magnet. This causes many little magnets close to the new magnet to to switch direction. And some in the middle fall back to their neutral direction.  
So when you are very close to the bearing with the new magnet, they suddenly snap together.
Research has shown that the protozoan parasite *Toxoplasma Gondii* changes human behavior. Crazy cat lady gags aside, it's a very interesting read.
_URL_0_

Edit: formatting fail
Lots of reasons.  You probably had a low "binned" chip, meaning it wasn't at the top of its category.  

For example, I know Intel has been known to sell multiple of the same product, but sell the ones that get binned higher as the "Extreme Series" and have them clocked higher by default.  

Now why are they binned?  There is a lot of error from batch to batch or even from die to die on the same wafer.  One die might have been exposed better in litho and more closely meets the spec / "theoretical maximum" for functionality - this could be due to less undercutting from an etchant that is isotropic which leads to less resistance, proper functioning of gates, etc.  Other things, such as surface imperfections, doping imperfections and other random bugs could result in some batches or chips functioning better than others.  This effect is typically more pronounced from batch to batch (wafer to wafer) however, as more error is introduced.  

The main factors that determine performance are:  Internal resistance, capacitance and inductance (parasitic effects included) that deviate from the ideal due to production error. Doping inconsistencies that cause the properties of the actual transistors are also to blame, as they can impact switching speed, heat tolerance, voltage needed for RDSon, etc.  

In your specific case, your motherboard may also be to blame -- if it is delivering inaccurate or drifting clock signals, jittering or unstable power, etc, the CPU may not function properly outside of its rated frequency range.  You also might not be providing the CPU with sufficient voltage in BIOS, or might have some settings set incorrectly.  Intel and AMD both have gotten the process down pretty well and there isn't too too much variance between CPU to CPU with the same model number.
Being overweight is associated with increased risk for gall stones, diabetes, hypertension (high blood pressure), heart disease, high cholesterol, gallbladder disease, osteoarthritis and stroke.  Obesity increases those risks even more so. 

[Here](_URL_0_)'s an excellent article from JAMA from about a decade ago.

Article (in case the link gets messed up somehow):

Must, A., Spandano, J., Coakley, E.H., Field, A.E., Colditz, G.,  &  Dietz, W.H. The Disease Burden Associated With Overweight and Obesity. *Journal of the American Medical Association*. 1999, 282:1523-1529

*Edit*

Also, from [this study](_URL_1_):
 > During 10 years of follow-up, the incidence of diabetes, gallstones, hypertension, heart disease, colon cancer, and stroke (men only) increased with degree of overweight in both men and women. Adults who were overweight but not obese (ie, 25.0≤BMI≤29.9) were at significantly increased risk of developing numerous health conditions. Moreover, the dose-response relationship between BMI and the risk of developing chronic diseases was evident even among adults in the upper half of the healthy weight range (ie, BMI of 22.0-24.9), suggesting that adults should try to maintain a BMI between 18.5 and 21.9 to minimize their risk of disease.

If you have access to a university library or other service providing access to academic journals, I suggest reading that article as well.
This is a really good question, and ultimately comes down to the concept of [criticality](_URL_0_). Criticality in day-to-day systems such as liquids and gasses is quite complicated, but the idea can be understood fairly well with a simpler system, the Ising model of ferromagnetism.

The basic idea of the Ising model is that you have a collection of magnetic moments which point either up or down at any given moment. They generally want to point the same direction as their neighbors, but as long as there is any energy in the system they'll flip back and forth more-or-less randomly as the temperature allows.

What we find in such a system is that at high temperatures there is no large-scale organization. There may be some small correlation between the spin at one point and the spin of its neighbors, but this correlation disappears as soon as you look at spins with a significant separation. At low temperatures, we find that the whole lattice of spins is likely to be in the same position, with a few spins possibly reversing for a moment, but only ever temporarily.

One might expect that, while these descriptions apply to extremely high or extremely low temperatures, there would have to be a smooth transition between them for temperatures in between. In fact, this is not the case. The system undergoes a clear, demonstrable phase-change at a precise temperature, known as the Curie temperature.

To understand *why* critical behavior occurs requires more advanced mathematics, and I'm not personally familiar with the theory.
Because there's no such thing as raw energy.

When say "extremely dense energy," you probably think of a bunch of little packets of energy swirling around to being packed into a very small nugget to make a quark, or an electron, or something.

This is not a valid way to view reality.

Energy cannot be isolated. It's not a *material,* if it was, it would be called matter.

Energy is a property of matter. If something has a velocity, we say that it has *kinetic energy.* When we say that something "releases energy," what we are really saying is that it transfers or transforms the energy into another object or another type of energy.

Energy doesn't exist if a field, as you suggest. Rather, it is attached to fields. You may have heard of the electromagnetic field, and how light is just a wave travelling through it. The wave itself is energy, but the field that it travels through and needs to exist is matter.

Another example is the electron field. Electrons are waves too, they are called "standing waves" while bound to atoms. These waves are, once again, just fluctuations in an electron field. And those fluctuations are energy. What gives the electron it's property we all know and love is how that specific field interacts with other fields. For example, the electron field causes negative charge in the electromagnetic field, but doesn't interact with the strong nuclear force field, so it isn't found in the nucleus.

When we make matter from energy in places like the LHC, we put the energy we want converted into protons in the form of kinetic energy. We use this energy to smash two protons into each other. These Protons then strongly interact through the strong, weak, and electromagnetic forces which proceed to gobble up the proton's kinetic energy and convert it into more fluctuations in other fields.

So as you can see, it is wrong to think of matter as being composed fundamentally of the exact same "energy-stuff." Rather, energy is just an aspect of matter.
First thing to think about, is big fleets like exelon are running their plants at big profits. 

Second thing to recognize, is the majority of a plant's costs are for the staff on site. Fuel and capital make up less than 20% of the yearly budget. This makes it beneficial to keep the plant online even if they are selling power for a loss, as they will always make more money than the fuel costs. This gives them an edge compared to coal plants which have to go offline when electricity costs are too low.

As I've said in another post, nuclear plants are also performing uprates and increasing core thermal power by fair amounts. The equivalent of 10-15 nuclear plants has been added to the grid by increasing power production on existing plants. This significantly increases revenue and usually profits.

Additionally, plants are running at much higher capacity factors now. Where 20 years ago, the US fleet averages were 60-75%, many current plants are running at 93%+ capacity factor, breaker to breaker, for 2 year cycles. Scrams per year are about .7-.8 per plant.
The key here is whether or not the object-to-be-orbited (which I'll call P for Peter) is isolated or not.

If P and the apple are well separated from anything else, then there is effectively no lower limit to the mass of P for P and the apple to orbit. There is a semantics caveat: when the mass of P becomes comparable to the mass of the apple, then you'd see that the apple and P would orbit their common [barycenter](_URL_1_), like the Pluto-Charon system. If P and the apple are not well separated from everything else (like, say, P is standing on Earth), then the apple will not orbit P since it will feel the gravity of the other thing(s) (i.e. Earth) much more than it feels the gravity of P.

Ok, so what is 'well separated'? - >  To have nice regular orbits the apple must be well within P's [Hill Sphere](_URL_0_). If P is on Earth, it's Hill Sphere has effectively zero size and the apple can't orbit P, if P is isolated then it's Hill Sphere will be large enough for the apple to be able to make orbits.
Yes, it can affect their ability to fly/walk/not fall down. 

My parrot had an accident where she hit her head and had some temporary (thankfully) brain damage. She had a lot of trouble with falling down a lot and staying stable when walking or standing. She's already terrible at flying and scared of it, so she didn't even try that. Luckily, with some medication and at home PT she's fine now. 

Birds can actually be pretty tough. They are more susceptible to issues related to trauma and shock than a lot of other animals. During my parrot's accident the main thing she was treated for was shock.
The key is that you have a birefringent crystal, that is the speed of light in the crystal is different for two axes.  You have the light incident at 45 degrees to these axes such that it's a superposition between the fast and slow axes and by the end of the crystal the slow axis component has slowed down it's now 180 degrees out of phase, which gives you the overall 90 degree rotation when you recombine the fast and slow components.

To think about relating this to a crystal structure, the index of refraction is often heavily dependent on the dielectric constant.  This is the response of the material to an electric field and it may not be bad to just think of the electrons as springs.  If one direction is more "soft" than the other, i.e. the electrons in the crystal are more easily pulled in one direction and than the other, you will get the electric field of light propagating more slowly when it is aligned with that direction.
Efficient? ~~Yes~~ **Maybe**, depends on method. Powerful? *Not Really*.

You can get tabletop sterling engines which will run on the heat of your desktop computer. They are really nifty heat engines which usually turn a little piston or wheel. While many of them can be **incredibly** efficient, even approaching Carnot efficiency which is the highest possible efficiency for heat engines at whatever set hot and cold temperatures, the issue is that you don't really get too much power out of them.

You run into engineering problems when you try to make a big one, too many parts and faults and efficiency drops. Most power plants in the world are essentially heat engines, even nuclear, but what they sacrifice in efficiency, they make up in shear raw power output. (though some processes are actually pretty efficient themselves)

The several degrees difference between your server rack and ambient room air temperature is too small to really get a large power output from. While the idea isn't impossible, it's a bit unwieldy and would provide only a little power at a high cost.
It's a really difficult topic to assess due to the limitations on data (a common issue in obstetrical research).  Generally, studies agree that home birth leads to fewer medical interventions (c-sections, operative vaginal deliveries, episiotomies, etc).  There are some variations in terms of outcomes - [for example, this meta-analysis found reduced rates of maternal morbidity with increased rates of neonatal mortality](_URL_1_).  [This prospective cohort study](_URL_2_) found that around 13% of women who intended to deliver via home birth ended up requiring transfer to a hospital at some stage of their delivery.  [This prospective cohort study](_URL_0_) noted that nulliparous (first time) women had poorer outcomes and higher rates of hospitalization compared to multiparous women.

Data here are difficult to interpret because there have been few/no robust RCTs, which makes confounding a larger issue.  For example, women who choose home birth tend to be white non-Hispanic, older, multiparous, native born, living in a nonmetropolitan county, and nonsmokers.  Data from deliveries also tends to be compared with those hospital deliveries that are deemed 'low risk', as anybody with increased risk factors should not attempt a home birth.  So is home birth for everybody?  Absolutely not.  But in some cases it may be a reasonable alternative to a hospital birth, but this decision should be made in conjunction with a medical professional to risk stratify.  As with everything in medicine, it comes down to weighing the risks and benefits as a team and determining which values are most important.
Deer and most mammals you are familiar with (including you) are placental mammals meaning the babies grow inside.  Kangaroos are marsupials which is why they have the pouch.  Genetic analysis suggests a divergence date between the marsupials and the placentals at 160 million years ago.  So you and a deer are closer relatives than a deer and a kangaroo.

_URL_0_
Sure, many parts, even some cells. Red blood cells don't contain a nucleus, for example. Tendons and ligaments also don't contain dna, except via any randomly present cells nearby. Which is why they can be donated without the receiver needing to go on immunosuppressive drugs.
[Sclerenchyma](_URL_0_) is the type of cell that gives pears their gritty texture.
It will sink if it's denser than water, it will float otherwise, just as if it didn't have a hydrophobic coating. The buoyancy force on the ball depends on the amount of water displaced by the ball, and if that's higher than the gravitational force on the ball, it'll float. But the amount of water displaced doesn't change very much with a hydrophobic coating - it might slightly increase, but not by much.
Both contribute.  

* About +45 microseconds per day from gravitation.

* About -7 microseconds per day from relative speed.  

This means that [roughly 38 microseconds of extra time](_URL_0_) passes on a GPS satellite per day compared to the Earth. How this difference is handled is that the clocks were just built to run at a slightly lower frequency.
It's incorrect to think of quantum mechanics and Newtonian mechanics as two distinct sets of physical laws. The more accurate description is that quantum mechanics is the more general theory, with classical mechanics being a good approximation under certain constraints. However, I'll emphasize this again, quantum mechanics already contains all the laws embodied in Newtonian mechanics, in fact for the kinds of conditions largely relevant for describing the behavior of objects you encounter on a day to day basis, it simply reduces to classical mechanics, a process called the [correspondence principle](_URL_0_). Thus in principle you could solve any classical physics problem in the framework of quantum mechanics and get the right answer, it's just that this would be much more cumbersome, and in most cases the increase in accuracy would be negligible.

Also, let me make a quick aside about when quantum effects are relevant. We usually think of quantum mechanics as only introducing significant corrections for very very small things. However, it's important to realize that quantum mechanics can also produce macroscopically visible effects. [Quantum levitation, which relies on superconductivity is one such example](_URL_1_). This phenomenon, even while producing an obvious effect on a visibly big chunk of material cannot be meaningfully explained in the framework of Newtonian mechanics, because it is quantum mechanical in origin.
They are highly energetic charged particles which originate outside our solar system. They are primarily protons.

When they collide with atomic nuclei in the atmosphere, they produce showers of other particles via nuclear spallation and other high-energy reactions.

There are muons traveling through your body right now which were produced as a result of these showers. For example, a high energy proton could collide with a ^(14)N nucleus, producing a shower of pions. The charged pions can then decay into muons, which travel toward the ground at a speed very close to c.

Time dilation extends the lifetime of these muons in the laboratory frame so that they live long enough to reach us. They are traveling through you right now, losing a very small amount of energy every once in a while.

It's extremely hard to shield against these muons. But they don't significantly harm us, and they can be pretty useful (for calibrating detectors, for example).
There are very few pure Freudian psychotherapists that are practicing, but they do exist. The id, the ego, the superego, the whole nine yards. My guess is they are ridiculously expensive. 

However, Freud's theory of psychoanalysis is most definitely used in modern day psychology. Freud wasn't the first, but he was certainly one of the more publicized psychologists to develop the idea of the unconscious - the idea that pathological behaviors manifest themselves into the psyche because of various unconscious issues. There are many different styles and techniques of psychotherapy that psychologists employ today, but the foundations for these therapies can be traced back to Freudian theory. However, most psychologists realize that humans are extremely complex and diverse and that various forms of therapy will be more effective for different people and different psychological problems. 

As far as the Oedipus complex goes - No, I have never heard a practicing psychologist say something like, "He appears to be exhibiting symptoms that point to an Oedipus complex..." There are probably some who agree with that, but in *my* experience in the field, that isn't a belief that I've come across. However, the theory of the id, ego, and superego do come up depending on what kind of psychologist you are talking to.
As a structural engineer, I have to point out that *architects* don't normally worry about this sort of thing. Engineers do. ;) 

Geotechnical engineers are the real wizards here. And, yes, I'm pretty sure there's a bit of wand waving in what they do. (usually called "being conservative") These guys have a lot of geological data that they start with in the first place. When you have a site selected, you hire them to go out, collect samples, and perform some experiments to better understand what you're dealing with. They'll let you know how much pressure the ground can take before unacceptable settlement happens. They'll let you know how deep the groundwater and bedrock are. This sort of thing. The engineers responsible for designing the foundations will take their report and use it to determine the best kind of foundations to use, and so on.

It's also not uncommon to actually bring fresh dirt onto a construction site, and pretty much any new building is going to require some level of earthwork. Part of this process is getting the ground compacted to a certain degree, and just generally preparing the soil properly. The geotechnical engineer is involved in this process, and has the knowledge to specify what should happen here... And a manmade island is ultimately not much different. Piling up that much... *land* and making sure the ocean won't erode it is certainly a massive feat. But when it comes to sticking a building on top of the finished product, it's nothing new.

I'm not particularly knowledgeable on what foundation strategies are normally used in these cases, but I suspect that ron_leflore is correct about the use of piles. Piles work using one or two methods: friction, bearing, or both. With friction the piles is basically like a nail driven into wood. You design them so that they get wedged into the ground tightly enough that the weight of the building doesn't drive them any deeper. With bearing they rest on underlying rock. Usually bedrock- which obviously isn't an option in this particular case. Though I assume manmade islands are generally built up from a base of larger aggregate- not just sand all the way down to the ocean floor. And I wouldn't be surprised if there is, perhaps, the option to send your piles down that deep in some cases. Maybe.

Sand isn't as great for foundations as other soil types. But it's not as bad as you may think. Especially when you start digging down deeper, where the sand is more tightly compacted. And remember, this soil has been brought in *specifically* for the purpose of having buildings constructed on top. They have foundation requirements in mind when they determine what kind of soil to bring in and how to place it.
>  Does an atom bomb really work by splitting the atom?

Sure does.  Specifically Uranium or Plutonium atoms, though in principle others could be used.

 >  And if so why doesn't that explosion split other atoms around it and create a gigantic chain reaction?

They work based on a chain reaction with the other Pu atoms around:  each fission emits some neutrons which can go on to split a nearby Pu nucleus.  Only rather heavy nuclei release energy when split and only the very heaviest are at all feasible to split, which is why the chain reaction is limited to the fuel the bomb is built with.

 > And how the hell do they split the atom in the first place?!

At the center of a bomb will be a neutron-emitting radioactive source.  The bomb is triggered by imploding the Pu around the source, and when the Pu core is small enough to reach [critical mass](_URL_0_) the chain reaction will run away and the bomb explodes.

Edit:  Holy proofreading batman.
It does absorb (infiltrate) into the ground below, but at some point that soil is saturated. What stops the water from continuously going down is that there are different types of soil/rock and some are less permeable than others. It usually goes along the line of gravel - >  sand - >  clay - >  rock. Below the permeable soil at some depth (depends on the location) there is usually a practically impermeable bedrock. The water 'pools' in the ground above those impermeable formations.

Since it can't go straight down any further, the water in the saturated soil flows downhill just like water on the surface. It looks for places with lower groundwater elevation (or a lower water table). In some cases those places of lower groundwater elevation will intersect with the ground surface and you'll have a spring where water flows downhill and out of the ground (sounds counter intuitive).

[Here's a cartoon like diagram/animation that's pretty easy to follow](_URL_0_). In this example you can substitute the river in the picture for a lake or other surface water body.

As you mentioned in your post, when you dig a hole and fill it with water, the water infiltrates. If you keep digging in most places you'll dig down to the water table like in the diagram above and you'll have a well.

You can find plenty more basic info at the wiki page for [groundwater](_URL_1_) or by googling some of the terms I used above. I'm happy to answer more questions if you have any.

Edit: forgot link
Ready for it to get even more confusing? Well the current flow, IE positive potential (IE greater than 0 volts) to ground potential (0 volts potential), is actually opposite the flow of electrons. The ground is the source of the electrons and the higher potential draws them. You can thank Ben Franklin for that bit of confusion.
The short answer is *Fast twitch muscle fibers*.  But these traits are not exclusive to Africans descendants and they don't seem to have a major influence of performance. 
The gene ACTN3 is the marker for this trait.

_URL_1_

_URL_2_

**Regarding Ethnicity read here** 
_URL_0_ 

Malcolm Gladwell mentioned the possibility of genetic testing as a way to determine which sports your children would be best fit for genetically speaking. Then highlights that is more likely the birth month of your child and the sports "age cut-off" is more of a factor. Pointing to data indicating a huge biased toward Jan. birthdays in hockey.
It depends on your driving style and the traffic you have to negotiate.

In heavily congested areas where you need to make lots of accelerating and braking maneuvers, your engine's power is predominantly used to accelerate the vehicle (which is then dissipated by the brakes, repeat). In this condition, your fuel consumption will be proportional to the mass of your vehicle.

On wide open highways where you can maintain a fixed speed, your engine's power is predominantly used to overcome friction caused by wind and moving parts in your car. At high speed, aerodynamic drag contributes to the majority of friction as the drag force is proportional to the square of velocity (F = 0.5* drag coefficient * frontal cross section area * air density * velocity ^2 ). In this condition, fuel consumption will not be strongly influenced by payload (although a heavier vehicle will have greater friction forces at the axles and more tyre deformation, but that's not your greatest of problems).
Water is not technically incompressible, but it doesn't compress much. Pressure increases ~20 bar per 100 meters of water depth, so on a [scale like this one](_URL_1_) you can see that the density changes (and thus volumetric changes for a constant mass) are relatively small, though you would also have to account for temperature, which will generally decrease with depth, but is not constant with depth because of currents and global circulation patterns in ocean water. Going for the most extreme example we have on Earth in terms of depth, water at the [base of the Mariana Trench](_URL_0_) compresses by just shy of 5% due to both the high pressure and low temperature.
Not an expert, just replying as no one else has and ive seen this question before. Basically, the electromagnetic force rules on our level of interaction.
You are right, that it has to do with the charge carriers, those are carrying kinetic energy. 
Different materials have different band structures E(k), i.e. at a junction the carriers will deposit (or take) thermal energy to match the different structure. 
In semiconductors, where the peltier effect is usually stronger compared to metals,  this is largely depended on the difference between band edge (of the band gap) and chemical potential/Fermi-energy.

For more insight you might want to study more on band structure model (depending how much you have already learned in lectures about it).
Speaking from decades of experience, old style incandescent bulbs radiated 90+% of the energy they consumed as heat.  That meant 25 watts was just mood lighting and 40w could barely be read by.

But if you sat by a lamp with a 100w bulb, your bare skin could feel the sensation of heat and under a 200w bulb your skin would noticeably warm. Also:

* Some light fixtures came with fire hazard warnings not to use bulbs higher than 60w.  

* Bulbs 60w and up gave painful burns if you touched them while illuminated (or even for a minute or two after).

* A children's toy from the 60's-70's, the EZ Bake Oven,  produced simple baked confections using the heat of a 100w incandescent light bulb.
The sound is a result of the rocket exhaust forcing the air out of its way.  In a vacuum it no longer has to do that, so the efficiency of the rocket will go up, because it's no longer spending energy moving air.
in 2007, National Geographic published that "Physicist Antonio Pavão and doctoral student Gerson Paiva of the Federal University of Pernambuco have created orbs of electricity about the size of golf balls that mimic natural ball lightning."

_URL_0_
Fat can be dangerous to remove because it's not avascular - there are still blood vessels running through it and removing it will cause bleeding.  In some parts of the body, there are large, superficial arteries and veins running in fat that a surgeon would be at risk of damaging.  And fat doesn't just lay right under your skin - it can infiltrate deep into some parts of your body, under muscles and near organs.  Some superficial nerves can run in fat, too, and you'd risk losing sensation to your skin or even innervation of some muscles.

And "being fat" is more of a symptom than a cause.  Even if you could remove all the fat that shouldn't be there, that wouldn't mean you just reversed all the bad effects of poor diet.  Cardiovascular disease from atherosclerosis (build-up of cholesterol and fatty acids on the inside of arteries) and diabetes (insulin receptors becoming less sensitive from constant stimulation) would still be big problems.
The gravitational force between two objects is given by the equation F_g = G m_1 * m_2 / r^2 , where G is the gravitational constant, 6.67*10^-11 nm^2 / kg^2, m_1 and m_2 are the masses of the objects, and r is the distance between their centers of mass.

When comparing the strength of the gravity of celestial objects, we speak of their gravitational acceleration, g, which tells you how fast you will accellerate while falling to it. The g of earth is 9.8 meters per second per second on the surface.

g depends on F_g, since F=ma, therefore the g of any celestial object equals (Gm_1 m_2 / r^2) / m_2 = Gm/r^2, where m is the mass of the attractor.

The comet pictured is 67P/Churyumov–Gerasimenko, which weigs an approximate 1.0*10^13 kg. The probe that took this picture is about 30 km away from the comet, so it is experiencing a g of approximately [0.022 meters per second squared.](_URL_0_).

If you were standing on the surface, about 4 km away from the center, you would experience a g of approximately [0.17 meters per second squared](_URL_1_), which is one sixtieth of the g of the earth.

The reason that a comet which is millions of times smaller than the earth has a surface g that is only 60 times smaller, is because r is far smaller on the comet, than on earth.

There would be a graviational pull on you if you stood on the comet, but it would be weak enough that you could escape it by jumping off.
You need to distinguish between spoken language and written language. They are, in many ways, separate from one another, and it is very common to have languages that are not commonly "written" or have no written form at all. Russian, before the 9th century, fell into this category.

Byzantine missionaries in the 9th century brought a couple of alphabets (Glagolitic and Cyrillic) to eastern Europe as a way to notate Slavic languages, which can't be properly notated in Greek or Latin (Slavic languages have sounds not found in Greek or Latin). In this sense, the missionaries brought "writing" to the Russian language -- but the spoken language itself did not change as a result of this.

Over time, Glagolitic fell out of use (except perhaps in some corners of the Orthodox Church) and Cyrillic began to predominate. But again, this change would not have affected spoken language.
The speeds are directly measured by a combination of data from [Hurricane Hunter aircraft](_URL_1_) and Doppler radar measurements when the storm is close enough to land. The aircraft will often launch dropsondes to directly measure the surface pressure in the center, and it has a [variety of other equipment](_URL_2_) to make various meteorological measurements.

When a storm is far away from land, with no radars nearby and no reason to send an aircraft out, [estimation techniques based on satellite data](_URL_0_) are used.
There is the factor of [structural integrity.](_URL_0_) where ants would not be able to support themselves as well the size of elephants. 

Then what do your giant insects breath in our relatively low oxygen world?
Within reason, you want to float as low as possible. Having more mass above the waterline makes it ever more easy to tip the ship over. This is very important, when the entire vessel is totally full of oil.

Cargo ships need to be able to dock where the overhead cranes are, to unload containers that all have different destinations. Totally different to pumping out a ship full of oil.

As /u/Jomeaga says, oil tankers are set up for a different set of conditions, different cargo transfer methods.
You use contextual clues to figure out when memories are. I'm sure that you cannot remember the exact order of every one of your memories (if you can, that's impressive and you should go see the dean at your nearest research university). Sometimes these contextual clues are clear: e.g. "I can remember that just last Friday" or "That was my seventeenth birthday party when I drank for the first time." Sometimes you need to think about it, like "I remember that it was a Friday, because I had school that day and my mom let my friend come over and play" or "If I think about it, that was a day in 2004 that the Orioles beat the Devil Rays 13-2." I'm sure there are memories (especially from when you were a child) where you don't know exactly how old you are and you lack any clear contextual information to find out. I can't remember how old I was when I played little league or if I played for more than one year. But I do know that I played little league after I played tee ball.
Lice infect all orders of birds, and most orders of mammals. Human head lice are specific to humans, are a different subspecies than body lice, and a different species than pubic lice! They're an obligate ectoparasite, so won't survive if they're just wandering around in a house. [*Pediculus humanus*](_URL_2_) is the species in question, split into the subspecies *P. h. capitis*, the head louse, and *P. h. humanus*, the body louse. [*Pthirus pubis*](_URL_4_) is the pubic louse, a completely different family!

You can gain loads of information by looking at lice, and their evolution. We can pinpoint the time where humans became less hairy than their ancestors by looking when head lice and body lice diverged genetically, and that also gives us information on when humans started wearing clothing. We can also look at the genomes of lice to study human population movements. For example, *P. humanus* evolved from a gorilla louse, so we can infer that we were living in close proximity to gorillas at the time of divergence. [Here](_URL_1_) is an article about it. [Here](_URL_0_) is an article talking about human population movements, and how we can get useful information from lice.

I realise that I've gone a bit off topic - the different lice species/subspecies are adapted to suit different diameters of hair (compare hair on different places on your body, and you'll find it has a different texture and thickness), so I imagine this may limit host selection. Different *Pediculus* species are found on different species of monkeys and apes, so it's unlikely that they'd survive very well on domestic pets.

Dogs can get lice e.g. [*Trichodectes canis*](_URL_5_) (the canine chewing louse) and [*Linognathus setosus*](_URL_3_) (the canine sucking louse), and domestic cats are affected by one species of louse, *Felicola subrostratus*. Other species of felid are affected by different species within the *Felicola* genus.

[Cospeciation](_URL_6_) between lice and their hosts is a fascinating topic! I can't find much information on what exactly makes lice specific to their hosts, but I guess it's a combination of host characteristics (e.g. hair diameter), environmental characteristics (does the host adapt to live in a hot place?), and overall allopatric speciation.

Hope this helps!

ETA: emphasised the fact that pubic lice were from a different family than body and head lice
Items float, or sink based on their density compared to the fluid they're suspended within. If the item is more dense than the fluid, the item will sink. If the item is less dense, it will float. However, some items neither float at the surface, nor sink to the bottom. These items are said to be "neutrally bouyant" and have the same density as the fluid they're suspended within.
Because it is a fire hazard. 50000 mAh for a 5V charger is about a million joules (250 watt-hours), i.e. if there was a short circuit such that the power bank dissipated its energy over the course of a minute (say), it would be putting out 15000 watts (the equivalent of 10 electric stovetop heating elements). A fire on an aircraft is one of a pilot's biggest fears because there is no escape from the toxic smoke and no way to land a plane from cruise altitude in less than 15 minutes or so even if the pilot's try to land as quickly as possible. This is why you always hear about not messing with the bathroom smoke detectors. Here are some notable examples of fatal commercial aircraft fires since 1980 or so: [Saudia Flight 163](_URL_6_), [British Airtours Flight 28M](_URL_2_), [Nigeria Airways Flight 2120](_URL_7_), [Air Canada Flight 797](_URL_4_), [Aeroflot Flight 2306](_URL_1_), [South African Airways Flight 295](_URL_10_), [ValuJet Flight 592](_URL_9_), [Propair Flight 420](_URL_5_), [Swissair Flight 111](_URL_0_), [China Northern Airlines Flight 6136](_URL_3_).

Note that ironically [some newer aircraft use lithium ion power banks that have caught fire and lead to the temporary grounding of all 787 aircraft](_URL_8_).
You’re are correct that DI water will shorten the life of the electrode. It is recommended, atleast by a few manufacturers, to store it in 4M KCl solution or if that’s not available store it in pH 4 or 7 buffer
Both branches really, because the judicial system has no power to actually enforce the rulings they make. The Supreme Court telling Andrew Jackson that Non-Native Americans couldn't encroach on Native American lands and Jackson told them to go stuff it and did so anyways. It shows that the Judicial system can't force the enforcement of the rulings they make.

In addition a judge can be impeached by Congress through the same process that the President is. Majority vote in the house, and then two-thirds of the Senate needs to vote in favor.
Might be caused by low sun angles being able to scoot in underneath the cloud layers, basically catching the sunset and diffusing it.
I remember seeing this question answered a couple months back, and the answer was informative and detailed.

[Here it is](_URL_0_)
The temperature dependence is buried in the entropy dependence. Thermal energy entering the system that will increase the temperature increases the entropy.
A parasite has a non-mutual symbiotic relationship with the host; the parasite benefits at the expense of the host. What you are actually looking for is mutualism - where both organisms benefit from their relationship. At the macro level we have domesticated animals like dogs, cats, sheep, etc. that one could argue are in a mutualistic relationship with us. At the micro level you mention gut flora but I am not aware of other types of bacteria that aid us. I hope this points you in the correct direction.
Anti bacterial agents kill bacteria. Antibiotics kill bacteria without causing you damage (excluding maybe a few side effects). For example bleach will kill bacteria. But if you drink it to kill bacteria in your stomach you will also end up killing you. This is why pharmaceutical companies have to try and make antibiotics specific to kill bacteria cells and leave human cells relatively unscathed. 

I don't know much about triclosan but if it's not used as an antibiotic i assume it's because it is also toxic to humans.
> told by parents, teachers, etc.

Ah, okay, an "old wives tale" then. Yeah, there's a lot of scientific evidence related to what I said in my first response. Although, there was, maybe still is, some talk about it being viral (I'm not up-to-date on the research), arthritis is mostly genetic. The only exception I'm aware of is the wear-and-tear form.

Now, for old wives tales that *are* true? I can tell you it's going to rain, and whether it'll be today, tonight, or tomorrow, just by the pressure in my joints.
In an [English auction](_URL_0_), the best strategy is to continue bidding in small increments until the price reaches your personal value for the item.  If you're buying a horse and it would be worth $1000 to you, bid up to $1000 then stop.  It sounds stupid, but that's the optimal strategy.

The only trick is that if you don't know the true value of an item, you may be able to get information about its value by watching how others bid.  In this case your strategy is still the same, except that you update your valuation as the auction progresses.

The optimal strategy assumes that the other bidders are rational, which is often not the case.  Many people will get an emotional attachment to the item or feel rivalry against other bidders, so it's best to avoid any emotional engagement at all with the other bidders.  It only takes one rival to bid up the price, so even if you can somehow scare away most of the other bidders, any tiny risk of attracting a fighter makes it not worth it.  It's better to be cold and disengaged.
If you're asking "Why is there gravity?", the answer is nobody knows.  And we're not likely to ever know.  To answer a "why" question presupposes a certain amount of agreed-upon foreknowledge.  I'll let Feynman explain the difficulty of answering "why" questions, because I'm no Feynman and he does it far better than I.  [So here's a video of Feynman talking about "why" questions.](_URL_0_)

If, however, you're asking, "How did gravity bring together a diverse, isometric gas into clumps," well, there are very good answers for that for laymen.  But as before, a famous popularizer of science has explained it better than I - [here's Stephen Hawking talking about how gravity can bring things together into clumps and galaxies and such.](_URL_1_)
I've never really come cross definitions of the water bodies themselves, beyond being within the relative regions, I.e. the tropics are defined as an area where the sun will be directly overhead at some point throughout the year. The water bodies themselves vary massively within the zones due to hydrology, geology and other such "ology" subjects. Also as part of the "global conveyor" system water masses move around the globe, with cold polar waters sinking due to higher density, and moving into ocean basins, to later be brought up at upwelling zones. The constant movement of the oceans means there is no definable water mass/body that is "tropical" or "polar". They are normally defined by parameters such salinity, density, position within the water column and origin location, for example Antarctic Bottom Water originates in the Antarctic, moves into the deepest parts of the ocean basins, before eventually mixing with other water bodies, before being Upwelled at a location such as the Benguela Upwelling system, where it is cooler than the expected temperature for a tropical zone, but eventually is heated up as it travels across the Atlantic at the equator. Very simplified version and a bitch to type on a Kindle with a keyboard that barely works, but hope that helped a bit.
Edit: I realise having read through it again I kind of jumped from explanation to explanlation, hopefully someone can clarify any questions or I'll have a go later on a laptop.
Such ringing is typically referred to as Tinnitus (the Latin word for 'ringing'), and it's precise nature varies from person to person. I believe it may, in some cases at least, correspond to the pitch associated with a region of hearing loss (e.g. in cases where Tinnitus is induced by noise exposure).

You can listen to simulations of various forms of Tinnitus [here](_URL_0_).

You can find out a great deal more about Tinnitus at the [British Tinnitus Society](_URL_1_)'s website.

EDIT: Clarity/formatting
What you're witnessing is a hygienic behavior called necrophoresis^1, in which colony members actively remove their dead nestmates and place them either in a particular location outside the nest (in a type of "graveyard") or simply scattered about a certain distance away. The idea is to prevent the further spread of any pathogens.

1. Diez *et al.* 2012. Social prophylaxis through distant corpse removal in ants. Naturwissenschaften 99:833–842.
Jupiter is so hydrogen rich that most other elements tend to appear in compounds that are hydrogen-saturated. For example:

- Carbon is most often found as CH*_4_* (methane)

- Nitrogen is usually found as NH*_3_* (ammonia), though also found as NH*_4_*SH (ammonium hydrosulfide)

- Oxygen is found as H*_2_*O (water)

- Phosphorus is found as PH*_3_* (phosphine)

...and so on. You'd initially expect to find boron simply as borane, BH*_3_*, but given the instability of that compound, it would be more likely to be found as diborane, B*_2_*H*_6_*.

That said, I've never heard of this or any other boron-containing compound detection on Jupiter spectroscopically, and a quick lit search only returns a couple of results suggesting it might be possible. Notably, Beer, 1976 ([PDF here](_URL_1_)) suggest that while boranes might be detectable, they also react strongly with ammonia (which is certainly abundant on Jupiter), so they could be tied up in the middle cloud deck.

More recently, Fegley  &  Lodders (1994, [PDF here](_URL_0_)) go into much more detail calculating expected compounds on Jupiter for pretty much every element as a function of temperature, and suggest that boron might be found as boric acid (Fig. 12) close to the cloud deck.
Fast neutron bombardment. There's a high flux of free neutrons which collide with heavy nuclei. By chance some nuclei will be hit by lots of neutrons (hundreds). The nuclei will be transmuted to heavier and heavier isotopes, with occasional rapid beta decays from very neutron rich isotopes along the "neutron drip line" (this is how the atomic number increases). Isotopes along the neutron drip line can have half-lives that are tiny fractions of a second, so the process proceeds rapidly.
Well, that's the problem, isn't it! Optimization is an enormous subject  with tons of literature from the classical to the very modern.

To learn more general info, you can look up texts on mathematical optimization, control theory, and partial differential equations. Again, there are countless textbooks for all sorts of backgrounds and goals.

If you have a particular problem in mind, you'd have to ask a more specific question :)
The current limit on the half life of the proton is that it's at least on the order of 10^33 years. If the proton has a half life of 10^33 years, and you have a gram of Hydrogen (which has a nucleus that is one proton), you only have one decay every 10^10 years. That is, one decay in a gram in the entire age of the universe.

EDIT: I originally said kilogram here. I should have said gram.

Still, there are theories that say the proton should decay. (The Standard Model of particle physics says it's stable, however, and we have little evidence for anything other than the standard model). We're always pushing the boundaries of what is "measurable", but it's safe to say that if "stable" elements do decay, then yes, it's very, very difficult to measure.

For reference, it took a [tank of 50 kilotonnes of ulta-pure water](_URL_1_) to get the current lower limit on proton decay. The [next experiment that might look for proton decay](_URL_0_) will be 200 kilotonnes (if it is ever built). You can fit the entire Statue of Liberty into a tank that holds 200 kilotonnes of water. It's a very difficult measurement to do!
The ice pack floats atop the rising tide and may shift out to sea somewhat. When the tide goes back down, the ice will deposit on the shoreline and might fragment. One instance of tide vs pack ice interplay I'm quite familiar with is the one in the Nunavik Inuit communities. Several of these are subject to insanely high tides in the order of 15 m (say 35 feet). When the tide is low, the pack ice, which is a few meters thick, fractures as it settles on the underlying shoreline, and the Inuit will then clamber down those cracks to eat shellfish (mostly mussels) and seaweed. Then they exit before the tide ice starts shifting as the tide goes back up.

Not my video, but I've spent some time in this community and ate some of those very mussels: _URL_1_

Now, this rise and fall of the ice pack has some consequences on the coastline itself. For one thing, it sort of speeds up erosion as the ice plucks rocks and finer material from the shore line and brings them out to sea during breakup, where they may float away encased in ice and become [dropstones](_URL_0_), or go adrift and deposit far from their site of origin, thus becoming a peculiar type of erratic boulder.
Most of the research on this area has been performed in two fields: scuba diving and astronautics.

At normal atmospheric pressure, in the short term, breathing pure oxygen may not be a concern. If the pressure is increased, or breathing it for a very long time, [oxygen toxicity](_URL_1_) may arise. First signs may appear after a few hours at 1 bar and are mostly respiratory discomfort. More serious damage may follow if exposure is longer.

However breathing 100% oxygen is ok if air pressure is lower than normal (e.g. 0.3 bar). This is the case of astronaut EVA suits, they reduce the pressure to prevent them from becoming too rigid, but increase the % of oxygen to make it breathable.

Another serious concern about a 100% oxygen atmosphere is the [increased risk of fire](_URL_0_) as flames may spread faster.
Yes, it's an active area of development because it is a way to make chemical processes more "green".  Many reactions occur by merely grinding two solids together.  You can purchase reaction vessels called [ball mills](_URL_0_) for running these chemical reactions in.  It doesn't work for every possible reaction, but there are certainly plenty of reactions that could be adapted to solvent-free conditions if we try hard enough.
Well, let's do the math. The dark matter accounts for about 23% of the density in the observable Universe. The total density is given more or less by the [critical density](_URL_2_), 3H^2 / 8 pi G, so we can calculate the dark matter density as about [2.24 x 10^-30 grams per cubic centimeter](_URL_1_). Alright, those aren't the most useful units, since we tend not to measure black hole masses in grams or cosmic distances in centimeters! So (using WolframAlpha) let's convert this to something more useful.

According to Wikipedia (this is fine, since we're doing an order of magnitude estimate) the [Milky Way](_URL_0_) is about 15 kiloparsecs (15,000 parsecs; a parsec is a unit of distance equal to a few light years, not a unit of time, Han Solo!) in radius and 0.31 kiloparsecs thick, at least in the main disk (again, order of magnitude) so we'll say the volume of the Milky Way is [a couple hundred cubic kiloparsecs](_URL_3_). A typical stellar black hole (i.e., one formed from the collapse of a massive star) is ten or so times the mass of the Sun, so a useful density unit might be solar masses per cubic kiloparsec, which we can do quite easily using WolframAlpha to find the dark matter density is about [33 solar masses per cubic kiloparsec](_URL_4_). This means in the Milky Way you'd need about 33 x 200 ~ 7000 solar masses of black holes in the Milky Way, or about **700 black holes** (again, not an exact number!!) to account for the dark matter (probably a bit more as we know that dark matter forms extended haloes far beyond the Milky Way's disk, which is what we used when calculating the volume of the galaxy).
The idea is that high-energy particles are what we call [Ionizing Radiation](_URL_0_). Ionizing radiation has the energy to strip electrons out of molecules entirely. So while infrared light would impart some kinetic energy on molecules, it (usually) wouldn't have the energy to free electrons altogether, and so that light would just end up causing vibrations, just like any other kind of heat does.

But ionizing radiation, as the name implies, can ionize. As you can imagine, taking an electron away from a stable molecule will make it unstable, and it will "try" to fill that gap by taking an electron from anything else nearby whose electrons are more weakly held. We call these ionized molecules [Free Radicals](_URL_2_). Normally your body has ways to deal with these (and, according to some theories, even is able to deal with small amounts of radiation-caused free radicals, see the [Radiation Hormesis Model](_URL_1_)), but large amounts of ionizing radiation can create free radicals in unexpected places, especially near DNA. Those free radicals are then able to "pry" weakly held electrons off of DNA, and damage and potentially change its molecular structure.
TL;DR Yes. It is clearly a plastic process. Usually only minor changes, occasionally major ones caused by toxic environment (in utero, toxic diet early on, etc.)

1) A well studied example is music. Studies in children have shown that if they learn music before the age of 7 (or 10, depending on what point they consider the decrease to have plateaued) they gain more myelin (primarily in certain areas, a bit also in general as well). Adults who pick up an instrument experience a gain only in areas directly related to the learning.

2) On the extreme end: It is known that people with autism often have an oddly large amount of myelin in certain areas, and in schizophrenics, the opposite occurs. In people with Multiple Sclerosis, the reason they get better then worse in cycles is because their nerves are de myelinated (by the body, for reasons we don't full understand), then go through periods of healing. 

Sources: _URL_1_
_URL_0_
Proactiv is - as far as I can tell, a combination of several chemicals mainly used to treat acne, the main being Benzoyl peroxide (BPO) as well as some moisturisers, exfoliants etc.

A quick note on acne itself - generally thought to be caused by blockage of follicular openings on the skin by an increased production of sebum (oily substance used to keep skin moisturised).  The inside of these blocked follicles then gets colonised by skin flora - mainly one called Propionibacterium Acnes.  These cause inflammation and those pesky spots we all hate/hated!

Now BPO is an organic compound which penetrates down into the follicles.  BPO is an oxidising compound and creates free radicals, these free radicals have an antibacterial effect (note: BPO is NOT an antibiotic)  This effect has not been shown to increase resistance of bacteria unlike oral and topical antibiotic therapies.

So now we know that using BPO has an anti bacterial effect and this helps reduce them nasty red spots, coupled with the moisturising effects of the rest of proactiv you are doing quite a lot to treat acne.

As for effects of stopping BPO there is very little literature on the effects on pubmed or on the web in general, apart from anecdotal stuff on forums, my hypothesis is therefore this:  Proactiv and it's constituents do not change the make up of your skin as such and you do not get used to them, however stopping the treatment allows the regrowth of a previously suppressed bacterium and allows the acne to flare back up.  Proactiv is meant to treat the cause of the bad spots while waiting for us to naturally grow out of acne, which can take 5+ years.

N.B. I read Wikipedia before beginning a lit review for this answer, it mentions that BPO increases skin turnover and links to a medscape article.  While the medscape article is a good introductory read I could not find any referenced mention of increasing skin turnover and so have not mentioned it here.
Females don’t need to have a pregnancy to lactate. Just the suckling action can initiate a neural reflex of prolactin production. For much of human history wet-nurses have been used by women who couldn’t ( or didn’t want to ) feed their own children for various reasons.  
_URL_0_
I understand the statement, but I disagree with it, for a few reasons.

The entire argument is about entropy, interpreted as disorganization. The human body is a relatively low-entropy system. But human bodies are only "pockets" of low-entropy, in a high-entropy universe. We are a "random fluctuation" in the overall scheme of the universe, according to this hypothesis.


 > If our current level of organization, having many self-aware entities, is a result of a random fluctuation, it is much less likely than a level of organization which only creates stand-alone self-aware entities" 

They argue that, if a self-aware brain is unlikely, 2 self aware brains are more unlikely (just multiply the probabilities). And 7 billion brains are exceedingly unlikely.

There is a huge flaw in their argument! Their way of calculating probabilities is stupid. They think that having 1000 people is N^10 less likely than having 10,000 people. (N = something related to entropy and probability, and N  <  1). But if you have 1000 people, it's likely that you'll soon have 10,000 people... We reproduce! 

So the sentence you didn't understand didn't make sense in the first palace. Carry on.
Simple explanation: this Nonlinear PDE does't have a closed form solution. We approximate in a way that is analogous to Riemann sums in integral calculus which is what is known as CFD. 

The millennium prize has a good write up on why this hasn't been solved. 

Trivia: A famous physicist once said that when he died he would get to heaven and ask God why quantum mechanics and why turbulence. He emphatically believed there would be an answer for the former but not the latter.
You can use the thermoelectric effect to directly convert heat to electricity, without the steam turbine, but it would be less efficient. Satellites/Space Probes use this method because they can't afford the mass of the turbine.
Here's some of the more recent literature: _URL_0_

The TL;DR version is: 

Yes there are subtle differences between different meal frequencies, however it won't fundamentally change your energy expenditure and won't have significant impact on your weight. However, higher meal frequency (above 3 meals a day) did show improved health markers (including insulin) and appetite control, so indirectly can aid in weight loss.

Edit: Added answers to some of the more popular questions in brackets.
It's not a thing. It's not a stuff. It's all of the possible measurements you can make with a ruler and a clock. And depending on where you are within space-time your ruler may be longer or shorter (as compared to some other location) and your clock may run at different rates. 

The pictures we create to "visualize" it in some way are just that. They're... diagrams. They're not reality. It's like graphing temperature vs time. You can't go out and "see" temperature vs. time. You're just describing it in some graph. So too the pictures of these visualizations.
The process of symbiosis is currently being looked into. [This report-summary](_URL_2_), [which refers to this article](_URL_0_), shows how close symbiotic relationships can lead to genetic transfer. Mealybugs have bacterial DNA because of symbiotic relationships!

[Another example comes from plant-microbe interactions with Rhizobium.](_URL_1_) While it isn't genetic transfer (AFAIK), it does involve a set of synchronized gene activations that is only possible after a persistent-across-generations symbiotic relationship.

These, among other examples, leads to an understanding that symbiotic relationships can lead to cases where the symbiotic partners are increasingly co-dependent, even leading to gene transfers across kingdoms! This provides an evolutionary pressure to make sure that those symbiotic partners are passed down in tandem - this is how that ability evolved. The individual steps might be widely varied, but the evolutionary pressures and possible processes are beginning to be unraveled.

*note - some of these processes can also be found in pathogenic interactions which then may become symbiotic, and vice versa.
Wood is not a single element with a single melting temperature like say iron. Wood is a composite of cellulose, lignin and a whole bunch of other components, all with different qualities. Cellulose isn't a single element with a single melting temperature either, it's an organic compound.

So in short, no you can't melt wood.
Well, this seems quite subjective.  I for one find the return trip 'seems' much longer.
Various anaerobic bacteria are commonly found in stagnant water, this bacteria are harmful to humans.  Well water is ruining and well oxygenated there for most of the time does not have harmful bacteria. But is not always the case. Well water can get contaminated, So it needs to be constantly monitored.
The Penrose process does not say that rotating black holes eventually become non-rotating black holes. All it says is that energy can be extracted from a rotating black hole. If a particle within the ergosphere, but not the horizon, decays into two it's possible for one half of it to leave the ergosphere carrying away more energy than what it started with. The other half actually picks up a negative energy and angular momentum and once it falls into the horizon the black hole adds that negative energy/angular momentum (so it loses energy/angular momentum). There's no reason a particle must decay in the ergosphere so it's more than likely the amount of particles that don't decay will be larger than the amount that do. And the black hole gains angular momentum from all of them. So the likely scenario is that a Kerr black hole will gain angular momentum over time.

That leads into the next question. The theoretical upper limit on the angular momentum of a Kerr black hole is an angular momentum equal to the mass of the black hole. The units probably won't make sense to you, because relativists use units where G=c=1. Just think of it as the rotational energy of the black hole being equal to its mass energy. The reason this limit exists is because at that limit the singularity at the center of the black hole becomes naked, meaning the horizon no longer covers the singularity. And a naked singularity causes all kinds of issues. One example is that causality can be violated with a naked singularity. That's generally a no-no in physics. The Penrose process is one way the black hole could spin down if it started to approach that limit.

As for your last question, it's a bit complicated. When you're referring to a Kerr black hole, the angular momentum is simply a property of the black hole. The Kerr metric is a stationary, vacuum solution of Einstein's field equations. That means the properties of the black hole (angular momentum and mass) are not changing and there's nothing else in existence except the black hole. We don't have analytic solutions for black holes that are not stationary (that I know of). Instead we can do numerical simulations (that are very complicated) that can collapse a star into a black hole or put stuff around a black hole and see what happens. In those numerical simulations the angular momentum is usually defined at the horizon. So using the shape of the horizon to determine the moment of inertia for the black hole. It's not clear that that is a correct thing to do and theorists and those that do numerical simulations still debate the point.
Based on a lot of things in Song of Ice and Fire, it feels to me like seasons in Westeros seems to depend almost more on magic than anything.  The series is set in a world where magic has been sleeping for a thousand years.  The longest winter was when the Others walked the world before the First Men and the Children of the Forest allied to defeat them.  Since then, as magic has vanished from Westeros, the seasons there have seemed sporadic.  This is something that seems to affect the other lands, like the Dothraki Sea or Qarth or anywhere else across the Narrow Sea where magic is still around, if only a little.  So to me, it has always seemed that the randomness of the seasons is something unique to Westeros.  And now, magic is starting to creep back in from afar or so it seems.  The Others are back, the skinchangers are back, and so on, and so winter is coming.  It's just something I noticed while reading the books over the years.  I swear, I'm not a crazy nerd or anything, I just notice things while reading.
Changes in Gravitational, electric and magnetic forces all travel at the speed of light.  You can think of it this way:  any disturbance in a field quantity (electric field, magnetic field, gravitational field) travels at the speed of light, since fields are massless.
I don't see a reason why the horizontal velocity should have an impact on how much a fall hurts; especially when you have wheels on your feet.

The only reason I can think of is that you land differently. Maybe you bend your knees more and your body is not upright, thus is able to buffer the fall better. If you jump with a straight spine and locked knees, you'll hurt yourself whether you're on wheels moving forward or not.

So your understanding that the velocities are independent is correct, and the reason for the difference in the two cases has to be looked for elsewhere.
Before you ask this question, you must remember that breathing is different from respiration. Breathing is the process that moves air in and out of the lungs. Within the lungs, air travels through a network of tubes called bronchioles until they reach collections of very small sacs called alveoli. This is where gas exchange occurs. Only gases can diffuse across this barrier into and out of the blood and this reaction only takes place because of partial pressure variations between these gases (and related ones, like CO). Only negligible amounts of water ever cross the barrier. It it were possible for all other components of air to enter the lungs then it would most assuredly present more dangers than benefits. Unfortunately, I was not able to find any sources about the nutrient composition of air, but remember: everything we consume is broken down into building blocks we can use - all of which would definitely not ever be found just floating around. But if we imagine your theoretical situation is true, and we have everything we need in just the right form and amount there would still be problems because the pH, volume, and temperature of the blood would go haywire and there is no telling how much of these nutrients would be filtered out by the kidneys and spleen.
Residential demand is only a piece of the puzzle. Business demand during the day outweighs that at night.

Take a look at [this](_URL_0_).  Overall demand is higher during the day.  [This image](_URL_1_) shows demand from a hot California day in 1999 -- commercial demand peaks during the day, and even residential demiand peaks at 6pm.
Yes there is a small effect.  Look at this phase diagram for water

_URL_0_

The near vertical boundary between water (green) and ice (blue) has a slight lean to the left indicating that the freezing point goes down as the pressure increases.  By about 2000X atmospheric pressure it is 22 degrees below the FP at normal pressure (251.165 K @ 209.9 MPa versus 273.15 @ 101.3 kPa). 

That is one explanation proposed for why an ice skate works.  The skate edge generates a pressure of ~30 atmospheres (~3MPa) which lowers the freezing point a few degrees. If the ice isn't that cold, it will melt and produce a lubricating film of water underneath so it slides easily.  However, this theory (nor any other to date) do
not completely explain all aspects of the effect - _URL_1_

Edit - added small correction
Earth will continue to slowly drift away from the Sun as the star gradually loses mass in the next ~5 billion years or so, but our orbit will remain quite stable until the point when the Sun becomes a red giant late in its life. When that happens, the Sun will lose about 30% of its mass, which will cause Earth to drift into a new, larger orbit, almost twice the distance it is now.

Our planet will initially escape the majority of the Sun's atmosphere (since it will expand thorough the orbits of the inner planets when it becomes a red giant), however, simulations have shown that there will be enough drag present to cause Earth to start receding back towards the star despite being farther away than it once was.

Eventually, it will recede to the point where it's orbit becomes very unstable and it will enter a perihelion that will cause the Earth to fall deep into the Sun. This basically means our entire planet will eventually be vaporized.
There are thousands of species of bacteria that thrive in humans today that are beneficial. In the intestines alone bacteria help us digest food, build our immune system, combat harmful bacteria, and produce vitamins.

Disease implies dysfunction, thus you are taking a class on the bad ones.
I can't speak to why they thought painting the statues that way was pretty - except that art does change, and fads do go in and out fairly quickly. It's possible that the colors chosen were a result of that, but I'm no art historian.

I can speak (a bit, anyway) to the linguistic side of your question. Color division is arbitrary - there's no reason why we shouldn't have a color named "rorange," instead of two colors "red" and "orange." Why divide the spectrum exactly like that? There's a lot of cultural influence involved in that, and as a result [there's been a lot of studies on how language affects color names, and/or vice-versa](_URL_0_). Ancient Greeks [have been found to use strange color names in their literature](_URL_1_), but this is more than likely because color divisions were different in those times than they are today. There simply hasn't been enough time elapsed for our eyes to have "evolved" enough to see colors that differently over the course of just a few thousand years; it's likely a linguistic change, rather than a change in actual perception.

Whether that affected the way they painted their statues - again, I'm no art historian. Someone else will have to address that. 

I hope this helps.
She'd have to transfer her angular momentum to something else, and then separate herself from that something else. Like, if she had been holding a bicycle wheel, she could have started spinning the bicycle wheel while holding it at just the right angle, and then released it.

I don't recall if she was holding onto anything in that scene that she could have used; obviously, it would have had to be something external to her suit, so she can release it.
The same reason (fundamental) neutral particles don't feel the electromagnetic force. Leptons have no charge under the strong force, so they have no interaction. 

The fact that the strong force exists has some residual effect on leptons. They feel the electroweak forces, as do quarks, so there are some higher order ("virtual particle") strong force effects. These should be very small though.
The diproton almost always decays to two protons. The beta decay to a deuteron has a very small branching fraction.

 >  How do they get back the energy that they lost earlier through gamma radiation when fusing into a diproton ? 

They don't.
Even some major arteries vary a lot from one person to another, so veins, especially smaller ones, vary a lot too. There are of course veins, that are in the same place in the majority of people, but a majority is still not much. Smaller veins (arteries too) vary so much, that nobody even puts them in anatomy textbooks. But that's not surprising, because the large and supposedly important blood vessels are very diiferent among humans. I mean there are people who are alive and well without one of carotic arteries, and there are people who don't have veinous dorsal arches on their hands, that are relativelly large. In general vascularisation is not constant, and the anatomy of arteries and veins is more or less just a guideline rather than a concrete thing.

Fun fact - only ~25% of people are anatomically correct.
In principle, atomic or nuclear growth does happen, although it isn't considered "radioactive growth." Radioactivity is a tendency toward emitting energy. First, understand a couple basic ideas about the nucleus. An nucleus will be radioactive, or tend to emit particles, only if it's parts, basically protons and neutrons, are in an "unstable" configuration. Unstable basically just means the forces between particles holding the nucleus together are not balanced with each other well, although this is a great simplification. If that instability can be eliminated by losing some particles, then the nucleus will tend to be radioactive. However, by thinking about the tendency to "balance out" the internal forces in the nucleus, one can imagine that there are unstable configurations of nuclei that can be made more stable by gaining particles. So, certain kinds of nuclei might have a strong tendency to "grow" by capturing neutrons, electrons, or even other entire nuclei. These processes are called [radiative capture](_URL_1_) (a kind of opposite thing to radioactivity), and nuclear fusion, respectively. Effectively, these comprise nuclear growth, but it differs from radioactivity because that means the tendency to lose energy through emission.

In fact, our understanding of how the chemical elements distributed about the universe were [formed](_URL_0_) is exactly through these processes of nuclear growth. The heavy elements are formed from the growth of lighter nuclei, originally all from the lightest nucleus, a single proton, or a hydrogen nucleus.

Tl;dr - Basically, yes, atomic nuclei can absolutely undergo a kind of opposite process to radioactive decay, or growth, and there are several kinds.

EDIT: Wikipedia links
I think what you're missing is that binding energy is negative. For example,

m*_tritium_*c^2 = m*_proton_*c^2 + 2m*_neutron_*c^2 - U*_binding_*  
m*_tritium_*c^2 = 938.3 MeV + 2(939.6 MeV) - 8.48 MeV = 2809.2 MeV

So when you add up the energy on the left side, you get the mass energies of two protons and three neutrons _minus_ 10.7 MeV of binding energy, for a total of 4684.5 MeV. On the right, you have the mass energies of two protons and three neutrons minus 28.3 MeV of binding energy, for a total of 4666.9 MeV. The difference, 17.6 MeV, has gone into kinetic energy.
Martin White has a [nice description](_URL_0_) of the physics of baryon acoustic oscillations (BAO).  Roughly speaking, the BAO scale was set when the universe was about 1/1000 of its current size, and since this feature is comoving with the expansion of the universe, its scale at that time was about 1/1000 of its current size.  And yes, that was approximately when the universe became transparent, i.e. when the cosmic microwave background was released.
There's no reason not to stand in front of the microwave. You'll notice a piece of metal with small holes in it in the window of your microwave, that's there to form a faraday cage around the microwave so that only a negligible amount of radiation leaks out.
We understand subatomic particles as excitations of fields. The simplest example is to look at the surface of a lake. When you throw a rock into a lake, rings of excitation flow out from the point where the rock hit. 

In quantum field theory, there are additional characteristics and rules that say that these excitations must be of a particular size or type. So when you "throw a rock" into a quantum field, it produces a reaction similar to the water wave, but it can't be a circular wave, it has to just be a little ball of energy moving about the surface.
There's a thing called Nuclear Winter, which would be cooling following a large scale nuclear exchange.  It would be the result of lots of smoke, not just from the initial nuclear exchange but also from  many, many fires afterwards.

There was no noticeable climate change from nuclear testing.  First, all tests after 1963 were done underground.  Second, the aboveground tests of the 1950's were isolated tests that produced a mushroom cloud, but no substantial fires afterwards.  So the clouds were minimal.

Finally, the little ice age occurred around 1700, well before the first nuclear explosions. _URL_0_
Apply a known force and measure the resulting acceleration. From Newton's second law, you have m = F/a.

This can be done with a dynamometer, a ruler and a stopwatch, for example.
_URL_0_
Saturn's moon Titan is believed to have high enough atm pressure to support a water cycle in methane, complete with methane clouds, methane oceans, methane icebergs... if we ever send a ship there we know what not to name it.
Lake levels can be pretty dynamic things and will be influenced by a couple of different parameters including: rate of inflow of water, rate of outflow (in the forms of rivers or ground water), rate of evaporation, the surrounding topography (is there a defined sill point that if the lake height reaches, it will over top and spill into a neighboring basin), and rate of sedimentation into the lake. An additional important process in controlling lake level is the isostatic flexure of the earth's crust, at least in the case of large lakes (e.g. the Caspian Sea or former large glacial lakes like [Lahontan](_URL_1_) or [Bonneville](_URL_0_)). As more water and sediment flows into a lake, this will cause the earth's crust to sag under the weight a bit and the margins to rise (imagine piling sand on an elastic sheet), which will help to keep the lake isolated and accommodate slightly larger volumes of water/sediment. This forms a positive feedback loop (though not without end, obviously) and can also function in the opposite direction (less water coming in causes a rebound of the earth's crust and lowers the margins of the lake potentially promoting spill over events). Lake levels are typically described as non-linear (i.e. chaotic) systems and thus a small change depending on the previous conditions can potentially create a very large response. 

In terms of your question, the important processes would be (vaguely in order of importance, but will depend on particular lake): evaporation, outflow, and then subsidence (the flexing of the earth's crust in response to the weight of water/sediment).
The mind appears to be an emergent property of the complex system of the brain - not an interchangable name for brain.  The emergent system has the ability to be self reflective and to evaluate input.  ie.  I can see that I have cut my hand, and thus I can tell that I am experiencing physical pain.

To say that the mind is nothing more than the brain is to shortchange both - the brain is a physical organ of incredible complexity, capable of changing its wiring over time.  The mind is our interpretation of the attributes of which this organ is capable of achieving.  In reality, they may be the same thing, but the words have a lot of connotations to them, which may be the death spiral of the average philosopher.

Our brain's ability to reflect on it's own behavior allows us to separate (easily) the source of pain, given that the brain has access to inputs that are not just inside the brain.  Thus, the brain may physically interpret the message as generic pain, but we can filter that input by seeing whether there is an injury or not.  

(Of course, there's always phantom pains and other oddities, where we can see the pain is not real, but the brain/mind are complex organs and they don't always interpret signals correctly.)
Zirconium alloys are used in nuclear reactors to coat the fuel rods and prevent contamination from fuel leakage because they are corrosive resistant and absorb little of the thermal neutrons. The coolant water around the fuel rods is also meant to moderate the thermal neutrons released during fission.

The problem with zirconium is that at high temperatures (e.g there is not enough coolant or the coolant got too hot) reacts with water to produce hydrogen gas and zirconium oxide. Hydrogen gas is very flammable and liable to explode when mixed with the oxygen in the air... which is exactly what happened at Fukushima and Three Mile Island as well back in 1979 though to a much lesser extent.

**Edit:** I mistakenly wrote the wrong chemical reaction down. It's fixed now.
Vocal chords are like any other instrument. Some people were obviously born with better instruments than others. Jimmy hendrix on some nasty cheap beat up guitar still has a pretty good chance of sounding better than you on a 68' Strat though.
>  Why is it not better to remove the multiplexers and just have a control input ANDed with the clock and use the output signal as the clock for the flops?

A _multiplexer_ selects one out of two or more input signals to output based on an other (control) signal. Any way you choose to implement it, it will be a multiplexer nonetheless.

The reason you don't want to use the output of a multiplexer as the clock of the subsequent flip-flop is it's introducing unnecessary propagation delay. Also, since the output of a multiplexer can be something other than your clock signal, you have to introduce additional circuitry to cover what happens when the clock is not selected - because if all inputs of the multiplexer were the clock, then it is useless.

What you are describing in your question is conflating the usage of a multiplexer and a clock. Their purpose and functions are orthogonal.

 >  Wouldn't doing this remove the constant reassigning of the flop's output back through the design?

You have to be more specific - what exactly is the circuit you have in mind? And what exactly do you mean by "constant reassigning of the flop's output"?
No not really. When satellite orbits are being designed, they are carefully checked such that they don't intersect with the orbits of existing satellites or debris in space. 

One special orbit is the geostationary orbit where satellites must be 35,786km away from the Earth and directly above the equator in order to stay in the same position in the sky with respect to the Earth. This means there is a limited amount of space in that orbit for satellites to position themselves safely without risk of radio interference and collisions. The ITU (International Telecommunication Union) oversees the slots in the geostationary orbit and the radio frequency bands being used.
Well, "listener fatigue" is a thing, though apparently not well-understood: _URL_0_

But aside from that, everyone knows how irritating it can be to sit with a fire alarm or jackhammer going nearby. On the other hand, something like the ocean or a waterfall is usually considered soothing, regardless of volume. So it's not volume alone that matters. Intensity of sound does not increase the energy required to process the sound, any more than brightness increases the energy required to process something we're looking at. It's about the content, of which volume is only one part.

The energy required to process the input is going to be related to the *information content* of the signal, which is essentially how unexpected/interesting/complex/important it is. If the signal contains coherent information that is of interest and is unexpected, it will activate more neurons as your brain tries to analyze it. Well-known information, even if complex, will only strongly activate the structures that previously "learned" it.

Now, even that happening a lot will not necessarily "tire you out". Feeling "tired" has many possible physiological causes, but the perception of being tired out by something like you describe is largely going to be psychological. Is it a boring lecture you have to try to understand? That's going to feel effortful and "tiring". Is it a podcast about a subject that excites you? That's likely to feel energizing, even as your brain expends energy learning.

When it comes to the jackhammer, though, the problem is likely that it is too extreme for your brain to tune out. It strains your attentional mechanisms, forcing you to constantly exercise willpower to keep your train of thought. That's something that is known to be effortful, and could account for the phenomenon. It's kind of like the boring lecture; you have to keep exercising mental resources to keep your brain on topic, and that is definitely tiring.
Simply the bio-availability of the drug via (nasal) inhalation is significantly higher and more rapid than through ingestion. Which means the drug's onset is more rapid and the high is likely higher as more drug can quickly accumulate in the blood stream with less time for the drug to metabolised (typically in the liver) 

Basically your gut/stomach are carefully designed to break down and (somewhat)  selectively uptake the things you ingest. This means that any drugs taken via an oral route often take longer to enter the blood stream and a measurable portion may be broken down and otherwise rendered inert before then. Also as the drug takes longer to enter the blood stream there is more time for the liver to metabolise some of the drug so the peak accumulation of the drug will be somewhat reduced. 

This is not the case with the membranes of the nasal cavity and lungs. The mucus membranes of the nasal cavity are highly perfused and they are not designed to prevent the passage of small molecules (such as drugs). So users get a faster hit and more of the drug enters their bloodstream much more quickly.

To my understanding there is a rough hierarchy of bio-availability. With intravenous injection being the most rapid and highest availability administration route, followed by inhalation, followed by the rectal/suppository route. Then ingested dissolved in a small volume of a hot liquid and the least available route is ingesting as a dry solid. 

Study of these issues is part of the field of pharmacokinetics. 
_URL_0_

Edit: as rightly pointed out I've corrected Lung mentions to Nasal Cavity.
[Excerpt from Virginia Department of Forestry](_URL_0_)
Trees grow OUT

Tree trunks and branches grow thicker as new cells are
added beneath the bark. These cells make up vessels,
called xylem and phloem, that carry water and food
throughout the tree.

Xylem carries water and nutrients from the roots up to
the leaves. Active xylem is called sapwood. Old xylem no
longer carries water. It forms the heartwood of the tree
and may be a different color from the sapwood.

Phloem, also called inner bark, carries food from the leaves
to the branches, trunk and roots. Outside the phloem is the
outer bark, which protects the tree from injury.

The cambium is found between the
phloem and xylem. If you looked at
a tree stump, you could not see the
cambium, because it is only one cell
layer thick. The cambium’s job is to
make new xylem and phloem cells.

TL;DR : Trees grow Out
Momentum conservation. 

A neutron at rest, decays into a proton, an electron and an anti-neutrino(electron variant). 

Since the anti-neutrino is *almost* massless, it carries away some percentage of energy off, and carries huge momentum. The proton and the electron soak up that momentum in the opposite direction, and their total momentum must sum to 0, because the original neutron had no momentum, it was at rest. Usually, that means that the electron is given a huge kick, so huge, it ejects the electron out of the nuclei to the continuum. 

[Electron energy distribution](_URL_0_)
[Conservation of angular momentum](_URL_0_) is the proximate cause of Earth's rotational and orbital speeds, and the solar system's orbital speed.  Basically, small motions in the gas clouds which formed these systems become large motions when they collapse under gravity, because as mass is pulled inwards, its angular velocity increases.

As a side note, your judgement of "incredible" and "ridiculous" speeds is simply a subjective human response to speeds that are unlike those you encounter directly in your rather incredibly ridiculously slow life as a water-filled meat bag on this planet.  The speeds in question are perfectly normal.
You don't get diseases from air, you get diseases from inhaling viruses or bacteria that are *in the air*.

Any "air borne" disease can become suspended in air and can be transmitted by inhaling air that contains that particle. 

Some examples include the common cold, flu, and yes, TB.
When salt dissolves in water, the resulting solution has lower energy (which is why it happens in the first place). In corollary, it takes more energy to remove water (forming vapor) from a water+salt solution than from pure water. Salt water has a higher boiling point than pure water.

So the equilibrium that exists between liquid water and water vapor gets shifted in favor of liquid water being in the solution with the salt, compared to pure water. The partial pressure (proportion) of water in the air above a water-salt solution is lower than for pure water, when they're both at saturation pressures (100% RH). 

In other words, 100% RH above a water-salt solution means a lower concentration of water vapor than 100% RH above pure water. So your hygrometer, whose scale is calibrated for pure water, is going to show a different value. (The exact number is indirectly a measure of the salt's solubility, or alternately, how much energy is given off when the salt is dissolved)
The smallest scale on which the expansion of the universe has been measured has been to galaxies outside of the Local Group.  The expansion of the universe cannot be measured inside the galaxy because the galaxy is a gravitationally bound system, and so it does not expand with the rest of the Universe (in the parlance of astronomers, it is not in the "Hubble flow").  Even if systems in the galaxy did expand with the Universe, the expansion would be so small that measurement errors and random motions would completely dominate the signal.
Other organs are simple.  The heart is just a pump.  Kidneys are just filters.  The liver's a chemical factory (and the most regenerative internal organ in human beings).  Lungs and kidneys are basically bags.  They're not all that physically complicated and we manufacture things like them every day so we intimately understand how to make them and how they work.  The brain is spatially and temporally complex. Temporally because of how it develops.  Most of the connections and architecture are put in place in the first two years of development.  After that the organ undergoes a massive reorganization whereby it losses a lot of neuronal connections.  This can be thought of as an adaptive optimization phase (the organ builds lots of connections, evaluates which ones are most optimal and prunes the rest for efficiency).  A bolt on addition to the mature human brain wouldn't be able to take advantage of this phase.  Mature human brains have some limited plasticity, but it would probably be difficult to overcome this.  Second, the spatial complexity. The brain has more neuronal connections than there are grains of sand on earth.  It's not like we could wire in a replacement because of the scale of the problem (there are too many connections and they are too small for us to mechanically make). Those connections aren't just necessary for function, they're actually how the brain encodes data.  So if you force the connections, you may alter the person or their memory.  So the best case scenario is that you drop in a replacement and hope that useful connections get made.  Finally, the brain is the most protected location in the human body.  It sits behind the blood-brain barrier (think about it as the most exclusive night club in town, with the best bouncers).  That means even getting things into the brain cavity (drugs, cells, etc.) is really difficult.

So basically repairing the brain requires rewiring the most complicated computer we've ever seen, and that we don't fully understand, on a scale we cannot work, blindfolded.
> vitamins and a high concentration of calories

What would you call this substance, if not food?
Water molecules interact with each other by forming hydrogen bonds between the hydrogen on one water molecules and a lone pair of electrons on the oxygen atom of another water molecule.  In liquid water, there are 1-4 bonds being formed per molecule, and they aren't in any particular order.  This lack of ordering forces the molecules to be more spaced out, and density can be considered a way of counting how many molecules of something are in a given volume.  Fewer molecules, lower density.

The other contributing factor to density is molecular motion.  If molecules are moving and vibrating faster, that pushes them apart and lowers the density.  This motion is fueled by heat.  As a matter of fact, temperature can be considered a measure of how fast molecules or atoms are moving.  As the temperature decreases, this motion also decreases and the molecules can pack more tightly together, increasing density.  

Water is almost unique in that its solid phase is less dense than its liquid phase.  This is because in solid ice, each water molecules wants to form four hydrogen bonds.  For this to be geometrically possible, the water molecules have to space themselves out regularly at a distance that is larger than the average distance between liquid water molecules.  The stability gained by forming all the possible hydrogen bonds outweighs the extra spacing between the molecules.  

At 4 deg C, that is where the balancing point between ice, liquid water, and molecular motion allows for the closest spacing of the water molecules, thus leading to the highest density.
It depends what kinda of dizzy you're talking about. 
The typical dizziness related to spinning in circles is caused due to the balance sensing structures in our ears. Essentially, theres a little fluid filled area within our ears with a bunch of little, flexible hairs in it. As you move, this fluid sloshes about, causing the hairs to move according to your movement. Nerve impulses are sent from these hairs to our brain, where it uses the information to determine our orientation, and keep us balanced. When you do something, such as spin in a circle, you pin all the hairs in one direction. You observe the spinning with your eyes and other senses, so your brain kinda understands youre spinning. When you finally stop moving, your eyes are telling your brain that you're stationary. However, all the little hairs take some time to "reset" to their neutral position, so your brain is still getting the "were moving left" signals. Now your brain is getting two conflicting sets of information, and the resulting sensation is dizziness.

Another kind of dizziness is due to a sudden drop in blood delivery, and thus oxygen, to the brain. If you're sitting down, the heart doesn't have to work as hard to pump blood due to the lessened effects of gravity. When you stand up, the heart suddenly needs to work harder to properly perfuse (deliver adequate oxygen to) the brain. The heart can't instantly react to this change, so while its ramping up intensity, your brain experiences a period of low oxygenation, and its not able to function at 100%--hence the dizzy sensation. Once the heart ramps up, oxygen is delivered properly, and the sensation disappears.

I don't have an answer to the second question, but I hope these other explanations are helpful.
Be careful when you use "reason" when referring to evolution. There is no such thing as a determined purpose other than relative fitness.
Human bodies can't just be scaled up; a larger body requires specialized adaptations to handle the greater weight and bulk. Growth also costs energy, so once we reach the optimal size for our ecological niche it's better to conserve that energy for other uses.
Fastparticles has provided a great answer regarding thermochronology, but the plate tectonics story as a whole uses a lot more data than just that. Perhaps the most straightforward example is that of ocean floor dating. We radiometrically date the crust at locations around the ocean basins, and what we find is this: _URL_1_

Now, in oceans where there is no subduction (e.g. the atlantic) that gives us very good constraints on when it started opening.

Other data can be used as well. For example, by measuring the remnanat magnetism in some rocks (for example, in some lavas the iron minerals grow alligned to the magnetic field present at the time) relative to the current magnetic field we can work out how much the rock has rotated and at what latitude it was formed. If we do this for lots of different aged rocks on a single continent, we can track how that continent has moved across the surface of the planet over geological history. _URL_0_

You can tie these observations together with hot-spot tracks, as well as dating of tectonic related geology (for example, a fault related to mountain building might cut a 100 million year old sediment, but not cut a 90 million year old one, telling you that the tectonic activity occurred int hat 10 million year time window). By gathering lots and lots of these data you can build up a very detailed picture indeed.
"Major" changes don't really happen when you're talking geological timescales. The tectonic plates are always moving.   Tomorrow's map will be different from today's map, just on miniscule scales. 

The fastest moving plates are the Cocos and Nazca plates which are moving at the breakneck pace of about 6 inches a year. 

Even with such tiny movements, we can still make predictions about the global map over the course of the next few hundred million years. [Here's a nice animation](_URL_0_) that goes up to 250 million years into the future. Even then you can still kind of recognize some of today's landmasses.
> Theoretically if it were possible to move away from the observer fast enough, would your body heat become microwaves to those observing stationary behind you?

Yes.  In fact we observe this in every direction in space as cosmic microwave background radiation.  Light from objects that are so far away the expansion of the Universe is causing them to travel at speeds such that they are red shifted out of the visible spectrum.

 > Or in another way, if a Ship were traveling at a proportion of the speed of light towards a Star, would there be a Gamma Radiation spike hitting the front of the ship, when it reached a certain speed?

Yep.
Well, the conductivity is great for moving heat through the diamond, but you still have to dispose of the heat somewhere.  If bulk diamonds ever become cheap, you could make a diamond block with water pipes through it.

It's not obvious that compressed diamond powder, e.g., is any better than copper -- the conductivity is for bulk material only, which is still very expensive (though I've heard tell of people making silicon-wafer-style polycrystalline diamonds, for solar-blind UV detectors (diamond is a semiconductor and its band gap is quite high, so detectors made from it are blind to visible light, which is an advantage if you want to detect, say, far ultraviolet from the Sun -- or the center of a nuclear explosion, as a f'rinstance.)
The impactor was around 15 km in diameter.

If it was an asteroid then we would almost surely have identified it already, and have decades of warning. Near-Earth asteroids basically cruise around in low-eccentricity orbits in the inner solar system, so they're relatively easy to spot. With such advanced warning a spacecraft could be sent out to give it a small push, with either a rocket engine or an explosion, sufficient to make it miss Earth.

If it was a long-period comet, on their very eccentric orbits, on the other hand we would get much less warning. Comets spend most of their time in the outer solar system where the dim light makes them hard to see, then come screaming in towards the inner planets at great speed. We could well have mere months, maybe a year or two, of warning. This then means a much bigger push is required to make the comet miss Earth, and the interception spacecraft is likely to fly past (or impact) the comet at great speed with no chance for an orbit or soft landing. While a smaller object could be deflected with nukes, I don't think we have the launch capacity to send enough nukes quickly enough to deflect (or blow to smithereens) something Chicxulub sized with only 2 years notice.
I hope you'll pardon me some very light math. You know the first law, right?

dU = Q - P dV

This is just the definition of pressure (among other things). dU is the variation of total internal energy of a system, Q is the heat you supply to it, and dV is the variation in volume. P is the pressure. Let's restrict ourselves for a moment to transformations that have Q=0, these are called adiabatic and involve no heat transfer. So rearranging a bit:

P = - dU/dV (in adiabatic conditions)

in words this is:

Pressure is how much energy you need to supply to a system to make it contract by a unit volume. (With no heat exchanged).

It kind of measures the "resistance" a system develops to being compressed. It generates an outwards force that seeks to enlarge the system; this force acts on the surface of the system and orthogonally outwards, and its value is

F = P * A

Where A is the area of a given portion of the surface. (If you're clever and remember the relationship between force and potential energy, you'll be able to derive the latter equation from the former).

Pressure, like all thermodynamic quantities, can have a microscopic origin which is elucidated by statistical mechanics. This can vary wildly from system to system. However for *substances*, so essentially systems made by molecules, the common interpretation is the following:

* there is an actual microscopic force between the molecules (for example, they don't like to compenetrate and their respective electronic clouds repel). This will clearly yield a contribution to pressure I guess you could call volumetric, since it arises from molecules literally not wanting to be packed in a smaller volume. This is the behaviour typical of a solid or liquid.
* there is also a contribution from the thermal agitation of the molecule because of a nonzero temperature. Molecules move randomly and bounce on the walls of the system generating a pressure. This is what you'd call an entropic force. The system wants to get a bigger volume because there are more ways to arrange molecules and so entropy is bigger. The push towards larger entropy manifests as a contribution to pressure. This is the typical behaviour of an ideal gas.

Typically fluids will display both. It might also not be immediate how to split the pressure into these two effects, but these are the basic ways in which pressure arises in a system of particles.

Now typically if you have two systems in contact (like two elements  of fluid) if one has a larger pressure than the other, the interface will be pushed from one to the other with a force proportional to the  difference of pressures. In a certain sense the overall system will strive to equalize pressure.

However if there are other forces acting this can change. In general the pressure difference will settle in a value such that the pressure force balances with the external force. In this case we speak of hydrostatic equilibrium, and therefore there will be variations in pressure. For example: Earth's atmosphere; there is the gravitational force - the weight of the air itself. So there must be a pressure force to balance that and so a pressure gradient (variation). Indeed air pressure decreases with altitude.

So in a fluid (and other things) pressure can depend on the point. If you have hydrostatic equilibrium in a fluid, then if you have a discontinuity in the pressure then the derivative of pressure is infinite there, so it must be balanced by an infinite external force. Assuming there are no infinite forces around pressure must be continuous.

However this is strictly at equilibrium. Examples of pressure discontinuities can be found outside of equilibrium, like shock waves such as those produced by a supersonic aircraft.
Mass turning into energy: The conservation of quantum numbers, like baryon and lepton number. Particles like electrons, myons and taus and their corresponding neutrinos have a quantum number called lepton number, which is conserved. So a myon can decay, but only into an electron, a neutrino and an antineutrino, keeping the total lepton number at 1. The electron is the lightest charged lepton, so cannot decay. It works similarly for hadrons, like the proton.

Energy turning into mass: The conservation of momentum. A free photon cannot create an electron-positron pair, as that would violate the conservation of momentum. For this to happen a second particle is required to take a part of the total momentum, like a nucleus. So pair production happens, but not in vacuum.
Inside the bulb, a cathode heats up and emits electrons (thermionic emission).  Due to the voltage in the bulb, these electrons gain energy.  When one of these electrons hits a mercury atom, it knocks one of the mercury electrons out of its orbital.  An electron eventually falls back into this vacancy - and in doing so a photon is emitted.  This photon tends to be in the ultraviolet range, so the bulb is coated in a [phosphor](_URL_0_) which absorbs UV light and emits visible light (via a similar process).
We wouldn't do as bad as you may think. First off, we already have new antibiotics in the works. Some are similar to traditional antibiotics, and some are more exotic and use metals like silver to kill bacteria. That being said, we still have other options right now. Russia has been using bacteriophages (bacteria killing viruses) to treat infections for years now. While they're not tested to the level most antibiotics are, they are proven to work. Also as a plus, it's difficult for bacteria to develop resistance to them.
You're right to think that this would be redundant. Body parts (and at much smaller levels, tissues, cells, and molecules of proteins) are not individually coded for in this way. 

Rather, the genome encodes developmental genes that pattern the body from the earliest stages of development. There are many mechanisms, but in general, these proteins will be expressed in *gradients*. That is, there are more of them in some places, and less of them as you get farther away from their source. In this way, cells can tell how far away they are from specific points, and alter their gene expression accordingly. 

A protein that is only expressed along the midline will help nearby cells (which will receive a lot of stimulation from it) start along the pathway to becoming vertebrae, whereas cells that don't sense so much of it will instead activate other genes, perhaps turning into limb buds. The left and the right sides will *both* be similarly far away, and so similar developmental processes will create limbs on both sides. 

So, it's not about telling cells they should start making a left arm, but rather letting those cells know they're in the right place with the right potential to form *an* arm. There will ideally be other cells receiving the same message on the right side. This is why it is much more common for a genetic defect to affect both sides similarly: a defective morphogen won't work on either side. But accidents of development can occur due to chance environmental causes, and this is what's more likely to be responsible for a defect on just one side of the body.
CPUs use IEEE 754 floating-point math. That standard specifies that any positive value divided by zero is +infinity, a negative value divided by zero is -infinity and 0/0 is NaN (not a number).

The CPU will likely just test if a divide by zero is being attempted, then check the numerator for being  < , = or  >  than 0 and return the appropriate value.

A trap event can also be raised so you can detect that it's happened if you want to execute some code on a divide by zero (and some other events that can happen).
Short answer: Yes, but it depends on the drug in particular. If this process, presents a real hazard the drug in question usually doesn't make it to market. It can happen over time, elevated temperatures, moisture, acidic or basic conditions can make it happen faster. Really depends on the drug though. 

Long answer: Chirality comes (...most of the time) from a carbon atom that has 4 distinct things bonded to it. For each carbon atom like this - called a stereocenter - there are only two possible ways to arrange the four things bound to it, we call one arrangement 'R' and the other 'S'. 

R and S centers can interconvert, but it requires breaking chemical bonds. The most common ways this happens is when an H atom leaves (making an achiral intermediate) and then comes back on in such a way to form the other configuration. If this happens at some, but not all of the stereocenters in the molecule, you get a diastereomer; if all of them do it, you get the enantiomer.
Stars like our sun do an hours-long transition to helium burning called a [helium flash.](_URL_0_)

The helium flash is guaranteed to not destroy the Earth because there is almost no immediate outward change, and because Earth was already destroyed several billion years ago much earlier in the [red giant phase](_URL_1_) before the sun fully ran out of hydrogen in its core. The huge release in energy from helium fusion switching on is quietly absorbed by a change in the density in the core.

The sun will continue to be a red giant for a while after helium fusion starts. Eventually the outer layers boil off, and the core that is left is a big dumb lump of carbon and oxygen made by fusing helium. The remnant core is called a white dwarf.
A close look a the shell of an egg reveals thousands of tiny pores through which carbon dioxide and moisture in the egg may exit over time, allowing air to enter. As a consequence, the air pocket developed during laying increases in size as the egg ages. During the boiling of an egg, the increase in temperature causes the air pocket to increase in volume. But, so long as the boiling does not occur too rapidly, the expanded air volume can diffuse through the porous shell. The larger the air sac; the more the egg contents must contract to accomodate the enlarged air sac. Additional cooling in cold water causes further contraction which leads to an easier-to-peel shell.
Imagine you had 60 scientists all set up in a row across a distance of one light minute (the distance light travels in one minute). The first turns on his headlights, and the second sees this (one second later) and turns on his headlights, then the third, etc.  Neglecting reaction time, it will take one minute for this "wave of headlights" to travel the light minute, and this wave will travel at the speed of light. 

Now, imagine that all of the scientists have synchronized watches and each one turns on his headlights one half of a second after the last. This wave of headlights will appear to travel at twice the speed of light. 

But it required synchronization ahead of time. No information was transmitted faster than the speed of light.
The observation (measurement) causes the wavefunction to collapse immediately, and is not retroactive into the past.  Consequently, when two particles are entangled, and you measure one, the second particle only adopts its state once the first has been measured, and has not been in the state all along.
Im no physicist (math major) but from my rudimentary understanding at a certain point the energy density becomes so great that elementary particles are formed. So at that point, is the limit to the amount of light that can exist in a given area.

Also, light can be defined a bunch of ways.  If youre speaking of things that travel at c, then youre talking about the entire spectrum.  You would need a lot more "light" at the infrared spectrum to reach a sizable energy density than say, gamma rays.

Go ahead physicists, kill my selfesteem.
Cancer is caused by unregulated cell division.  Cancer cells are your cells, with your DNA in them, just growing and dividing faster than they are supposed to.  You have genes that normally work to regulate cell growth.  They tell your cells when you grow, when to divide, and when to stop.  You also have genes that destroy cells that aren't growing the way they should.

Certain things, like ionizing radiation or some toxic chemicals, can damage your DNA.  If the damage happens to occur in the genes that are responsible for regulating cell growth, that cell can start to grow in ways it shouldn't.  It divides, and now there are two cells with the damaged DNA.  They divide, and now there are four.  Eventually there are so many that they form a tumor.

So there are a couple of reasons why things now can cause cancer years later.  First of all, the DNA needs to be damaged in several places in the same cell.  Your DNA has multiple lines of defense against cancer, and all of them need to be damaged.  It can take time, including multiple rounds of exposure, for enough damage to accumulate in a single cell.  [This article](_URL_0_) says that most cancer cells contain upwards of 60 different mutations. This is one reason why workplace exposure to low levels of things like asbestos over many years is often more dangerous than a single high level exposure.  Also, once a cell has been damaged enough to start dividing uncontrollably, it still takes time for that one cell to grow and divide into a tumor that is large enough to detect.
We made an indie movie with this theme. _URL_0_
**TL;DR: In the vast majority of cases, we don't really measure the composition; we just use the process of elimination to guess while knowing only the planet's mass, radius, or density. In rare cases (but gradually becoming more common) we can get more information on actual elemental composition via spectroscopy of the planet's atmosphere.**

Planets in general are discovered via either the [radial velocity](_URL_0_) (RV) or [transit](_URL_1_) methods. If you use RVs, you learn the mass but not the radius. If you use transits, you learn the radius but not the mass. It seems like in general a planet starts rocky and if it gets too big, it will attract a gaseous atmosphere and turn into a ice/gas giant planet (think Neptune up to Jupiter or bigger).

That limit seems to be in the 5-15 Earth mass range if you have mass from RVs, and ~1.5 Earth radii if you get the radius from transits ([Rogers 2014](_URL_4_)). So if you discover a planet that's pretty far away from those transition regions, you can safely assume it's either roughly rocky or a gas/ice giant. If your planet is pretty close to that transition, then you're out of luck without more information.

The next most common thing is if you find a transiting planet and *also* are able to measure the RVs and get a mass. Now you have both a mass and a radius, which means you automatically get the average density of the planet (which is M / [4/3 pi R^3 ] ).

Once you have an average density of a planet, you can use models to figure out [what it's likely made of](_URL_2_). For instance, we know the Earth has a density of about 5.5g/cc. Gas giants are closer to 0-2g/cc. If you find something with a density of 5+, it's probably almost entirely rock, and something with a density of 1 is a gas giant. The figure above takes the Earth but turns various percentages of its mass into a gaseous envelope like a gas giant and shows how its radius puffs up and gives it a lower density. You just plot your planet's mass and radius on that diagram, compare it to model planets, and figure out what its likely composition is.

Just getting average densities like this is still cutting edge for exoplanets, particularly small ones. But knowing a planet is mostly rocky or gaseous doesn't answer questions like: does it have an iron core? Is the atmosphere the same as the Earth's? How much carbon and oxygen are there on this planet? Does the planet have clouds?

We're getting close to answering those types of questions with spectroscopy. You can do this via [transmission](_URL_3_) spectroscopy and start to figure out what elements are in the atmospheres of planets. Right now it's almost entirely done on Jupiter size planets (with mostly inconclusive results), but we're starting to creep into Neptune sized atmospheres recently. Lots of work is going into improving the techniques of exoplanetary atmospheric spectroscopy and that's where the field is headed in the next couple decades.
Quantum mechanically (which is the more accurate description compared to classical mechanics) it just doesn't take 5eV energy to traverse a 5eV barrier. A particle with 2eV has a nonzero chance to do it (ie it's wave function extends through the barrier and has nonzero probability density on the other side of it.)

This can be calculated and is experimentally confirmed. There's no reason to assume it would take minimum the energy corresponding to the height of the potential barrier.
Really depends.  Work in healthcare and been in plenty of OR’s to say yes yellow for the most part.  Yellow tends to be common.  Beige and brown are also very common colors.  The color has more to do with where and also how lean a person is as well as diet.  Uber muscular people who are incredibly lean will have more brown than yellow.
First off, brainwaves really aren't "waves" at all: they are not electromagnetic radiation and do not propagate in free space (like wifi, et al). More accurately, they are (repetitive) current dipoles in a volume conductor. 

Second, there is a substantial frequency difference between brainwaves (or brain "waves", cf. previous paragraph) and wifi et al. For example, classical gamma-band is around 40 Hz, whereas wifi (IEEE 802.11) is in the GHz range. There's really nothing going on in the brain above a kilohertz or so.

Third, the signals transmitted using technological devices carry actual information that we know how to decode and use. For all of the research into oscillatory activity, we still don't know what, if anything, oscillatory brain activity is for. There's a lot of talk of binding problems this and REM sleep that, but no universally agreed upon function for the stuff has been established.
So the description of wave propagation through a medium is described by the dispersion relation. The dispersion relation relates frequency with a wave vector.  In order to localize information, a wave packet is prepared by a distribution of frequencies around some mean value.  In free space the dispersion relation is linear which means all frequencies travel at the same group velocity (i.e. the speed of the information being sent).

In a more complex medium, there are different velocities for different frequencies.  Since a bit of information is usually sent as a wave packet (which consists of a range of frequencies), this wave packet spreads out over time.  Since a wave packet spreads out over time, one cannot send two packets too closely together since both packets will merge which results in a loss of information.  So depending on the distance one sends this information, the amount the wave packet spreads over the course of its travel determines the spacing, and thus the time between two bits of information.    

_URL_0_

If we could ideally represent information as a single photon, the speed is obviously limited by special relativity (the speed of light).  We can use the polarization of light to represent information.  A signal could then be multiplexed by sending different frequencies of light in order to maximize information transfer.  The ability to multiplex a signal in this way would also be limited by nonlinear effects such as harmonic generation where the interaction of two photons in a medium combine and create a photon of a higher frequency.
Chelation implies removing metals so doesn't apply to DDT and PCBs which are organic molecules.  From what I can read from a quick google search, there is no known way to speed their exit from the body and limiting ingesting them in the first place is the best prevention. 

_URL_0_

"Is there any treatment for PCB exposure?
No. Once PCBs enter your body, there is no way to remove them. They will naturally be slowly 
eliminated from your body. There is no evidence that weight-loss programs or saunas can speed up their 
elimination."
I'm a medical student, I don't have any articles, but just general knowledge I've gained from school... As far as age goes, MS is thought to be an autoimmune disorder, and autoimmune disorders tend to happen after age 20. This is because our immune system starts to break down after about age 20. Why? Not too sure,  but a theory one of my professor's had is that humans were not designed to live long lives, so the immune system 'evolved' accordingly.  When we are younger our bodies have good control over being able to kill immune cells that are self reactive (autoimmune cells). As our immune systems start to break down, we have less control/regulation of auto-reactive immune cells, and an increased chance of developing an autoimmune disease.. And thus as we get older, people are more likely to get an autoimmune disorder.
The whole woman thing, I'm not too sure of, I do know a general rule of thumb is that most autoimmune disorders affect women more than men. It could be that since testosterone can suppress the immune system, men don't get as much of a chance to get an autoimmune response.
The Planck length is not the smallest possible length. It's the characteristic length scale at which gravitational effects are comparable to quantum effects, where our current understanding stops making sense. Maybe there will be meaningful lengths all the way down to zero, but we can't say for sure.

The Planck time is the Planck length divided by *c*, the speed of light, it served a similar purpose to the Planck length.

The photon is not "the smallest possible amount of energy". Energy can be arbitrarily small; a photon has an energy *E* = *h f* where *h* is Planck's constant and *f* is the frequency, so a sufficiently low frequency photon would have an arbitrarily small energy. 

It's better to think of the photon as being a smallest possible wave that can exist in the electromagnetic field, like a tiny little ripple. The best understanding we have of the universe is called quantum field theory, the key idea is that *everything* (forces and matter) is made up of *fields*, and these obey the laws of quantum mechanics. What we normally call particles are *quanta* of these fields, which means the smallest possible ripples that can exist.

As to *why* we have this quantization, that's really a philosophical question and it depends what type of answer you want; it's basically just the way the universe is.
I'm not quite familiar with the mechanisms of bows, but from what I can gather through descriptions of bows, if the force needed to draw the bow is 50 lb, then the force applied by the archer would be 50 lb.  That's with a recurve bow, and not a compound bow.  With compound bows, some of that force is applied by the pulleys instead, but it should still have the same force applied to the arrow.

Upon release, the bow should apply all that force unto the arrow until the string has returned to its original position.  This would move the arrow from its drawn position all of 26 inches, with the string.

The energy applied onto the arrow should then be ~147 Joules.  If we assume the arrow is fired horizontally, then we can disregard gravity, so all of that energy will go into the arrow and become kinetic energy.

To find the velocity, one needs the mass of the arrow.  If we take an average carbon arrow with a GPI mass of 8.6 measuring at 26 inches, then the mass should be ~24 grams.

At this mass, the velocity of the arrow leaving the bow would be ~~349~~ ~110 meters per second, or about ~~156~~ ~~49~~ 246 miles per hour.

Sources: [Draw Length and Draw Weight](_URL_1_); [Mass of Arrows](_URL_0_); Wikipedia and Google for various definitions.

EDIT: Apparently I am bad with unit conversions.
Because functions are by definition total and single-valued but not necessarily injective or surjective. If you start with some function on a particular domain, then by definition the function takes every point in the domain and produces an output and furthermore it produces a single output. However, it does not necessarily produce every output, nor does it necessarily produce a particular output in one way. That means when we take the inverse, we have a problem -- what to assign to the points that are not hit (because the function is not surjective), and what to assign to the points that are hit twice (because the function is not injective).
It is exactly the same periodic table as the one you know.

Antimatter behaves the exact same way as matter when interacting with other antimatter particles.
Mutation goes on all the time, as mistakes are made in DNA replication, or DNA breaks are caused by cosmic radiation etc. Most of these events do not result in speciation.

Consider a couple of things. For one, the timescales associated with genotypic change for many species is long compared to the total lifetime of our species. So change can be occurring but not noticeable. There are of course counterexamples to this timescale argument, in particular with really small organisms. 

Second, many organisms have evolved to reproduce optimally within their ecological niche. So there is no evolutionary pressure for them to change much. Even when variants emerge, they are at a disadvantage relative to their peers, making them less likely to reproduce and thus propagate the mutation. 

Just some simple reasons for why certain lifeforms appear stagnant. I'm sure others can give you more detailed responses.
I think you'll want to look into accommodation.  This is the process by which small fibers connected to the lens in your eye change its shape to focus the image on your retina.  

When looking up close, your eyes also converge and your pupils constrict.  

Three mechanisms for accommodation are presented in the wiki article I linked.
This idea sounds very difficult to do. You'll need very specialised laser pulses. For an isolated CO2 molecule, you'll need to make it vibrate with very large amplitude motions so that the oxygen atoms come together and pinch off. Not easy at all - the CO2 will fight with you tooth and nail over this.
There are many ways to analyse an unknown substance, but combining as many as possible gives a better picture of what the substance actually is. More methods = less likely to misidentify.

That being said, in this article they describe the substance as a *Goop*, and from that we can always assume it is a substance which probably is organic. Further they explain it was oil (but that is irrelevant to the question as a whole). And from there we can use the methods that you would use do make sure what you've made isn't a poison etc.

As stated before, data from many methods of analysis combined makes a greater picture of what you've actually made, and the combination of NMR, IR, and MS is for me a very good start. (H)NMR splits the individual hydrogens and make them visible for us on a specta, from there we can start to assume a structure. The MS gives us the mass of the most common pieces in the molecule and IR is good to detect functional groups as alcohols, amines, carboxylic acids etc. Now you can start to build lego with the information you have to perhaps create a picture of what it can be and what it looks like.

From those three we can get a pretty good hint if we've made a poison or not. One can also attach other machines, such as HPLC and GC chromatographers. These are good for separation of the molecules if we have a mixture of several different components in the same blend. They are good for analysis and separation of organic compounds to get qualitative data.

If it would be a macro-molecule, perhaps a protein, we could use electophoresis to compare how big the proteins  to others and such, as well, and use silica gel tubes to separate them.

I hope that answers the question!

Edit: Typo
>  Can someone please explain the supposedly 'circular' logic of dating rocks according to the fossils in them and the fossils according to the rocks that they are found in? 

To answer this, you've got to understand the concepts of absolute vs relative dating.

In and of itself, a fossil does not necessarily provide a whole lot of information about the age of a rock. Not at first anyways (and more about that in an instant - hold that thought). Lets say you are looking at a rock unit of unknown age, and you make the very first fossil discovery in those units, and Woot! It's a newly discovered species, never seen anywhere before ... you celebrate, and once the hangover lifts you get back to work. What does this fossil say about the age of these rocks? Pretty much nothing, really. 

Lets push the example a bit further. Lets say further study brings forth 2 other new fossil species, one of which is only found lower in the sequence (so in older rocks) than the frist one, and the other only in rocks higher up in the sequence (so younger rocks) ... lets call the oldest "A", the middle one "B" and the youngest "C". You still don't know the age of the rocks. BUT you now have a tool to position rocks in time *relatively* to these fossil species. Finding species A, B or C elsewhere gives you a *relative age*. 

This was the state of affairs of geochronology  &  stratigraphy before radiometric dating. Pretty much all of the fossil record was laid out in tables describing columns of rock, and their ranges were expressed in terms of first and last appearance. It was a powerful tool. But it couldn't tell you the date in absolute terms.

That changed with the discovery of radioactivity and when this translated in the first absolute geochronological methods. These are based on physical processes, specifically the ratio of mother to daughter elements in minerals which are highly resistant and not susceptible to leakage of daughter elements. This allows you to pinpoint an elapsed delay since that mineral formed, an actual chronometer by any other name, which provide an absolute age. Although there are [several methods](_URL_0_), my very [favorite is U-Pb on zircons](_URL_1_), which is pretty much our standard in terms of reliability and accuracy. Ever since these methods became perfected, we've been building a database of sites where rocks were dated in absolute terms. These points were then used to calibrate the fossil record in terms of absolute dates, thus converting the fossil record from merely a relative dating tool to a pretty accurate absolute one. 

That being said, don't waste your breath trying to convince creationists. Logic and evidence are not the way to win that fight. There is a growing body of psychology studying that phenomenon, and it might be worth looking into that body of work.

Edit: Typos. I can't spell to save my life before third coffee.
Lighter than helium (at the same pressure and temperature), heavier than nothing.
Most nerve cells are myelinated although a few, typically pain receptors, remain mostly un-myenlinated. For sharp pains (think needle stabbing a finger) you want a very fast signal so that you can react as quickly as possible. However, for some signals it's not necessary that the signal travel as quickly as possible. For example and itchy/burning sensation is going to stick around for awhile and so doesn't need to be transmitted as quickly. One signal is just coming down the same line anyway but doesn't need an immediate change in behavior to change it. Considering that myelination is energetically costly it would make sense to not have myelin surrounding these cells so you felt the burning sensation faster because you don't gain any information when the burning is going to be there for hours or days.

The time difference between these fibers is small in larger time scales. Pressure nerves conduct between 3 and 30 meters/second (Type III) and Type IV travels from .5-2 meters/second. (_URL_0_)

Given an average human height of 5ft 9in (1.7526m).
A fast signal traveling 30m/s would take .05842s to travel the entire length of the body. Or ~58ms.
A slow signal traveling 2m/s would take .8763s to travel the entire length of the body. Or ~876ms.

But it doesn't matter because that burning sensation from that sunburn you got over the weekend isn't going away anytime soon. While you stepping on a needle requires immediate attention.
That is an awesome question. I'd like to know too. I work with a woman who is blind. She is also intellectually disabled but I can say that her inner world is very rich and full of characters who talk to her. It really makes me think about what it means to "hear voices" because this seems markedly different from what I normally think of when I hear someone talk about a person "hearing voices." Her voices (friends) bring her joy and help her do everything from laundry to showering. Sometimes she sits by herself in a quiet room. She moves her head around a lot back and forth in a rocking motion (almost like you would if you were listening to music). She will laugh intermittently. At first I thought it was just "out of the blue" and then one day it hit me like a ton of bricks, "She's listening to a conversation! Duh" Anyway, I'm constantly in awe of her and curious about her experience.
In principle it does, at the frequency with which you shake it.  Visible light is in the 10^14 - 10^15 Hz range, so you won't be able to shake a magnet fast enough to see the radiation.  Also, the power emitted will be very small at low frequencies, which is why we can usually neglect radiation for slowly changing EM fields.  

For comparison, electricity is often transmitted at 60Hz AC.  Radiation from 60Hz wires is usually small enough to be neglected, but can sometimes be picked up by audio equipment.
Yes, proper exercise with resistance will increase the levels of "good" hormones. Not only is there a increase in serum testosterone levels, but in Growth Hormone and IGF-1, a couple of other important anabolic hormones. A study published in [Int J Sports Med. 1991 Apr;12:228-35](_URL_0_) showed this hormonal increase after weight training in both men and women.

 > To examine endogenous anabolic hormonal responses to two different types of heavy resistance exercise protocols (HREPs), eight male and eight female subjects performed two randomly assigned protocols (i.e. P-1 and P-2) on separate days. Each protocol consisted of eight identically ordered exercises carefully designed to control for load, rest period length, and total work (J) effects. P-1 utilized a 5 RM load, 3-min rest periods and had lower total work than P-2. P-2 utilized a 10 RM load, 1-min rest periods and had a higher total work than P-1. Whole blood lactate and serum glucose, human growth hormone (hGH), testosterone (T), and somatomedin-C [SM-C] (i.e. insulin-like growth factor 1, IGF-1) were determined pre-exercise, mid-exercise (i.e. after 4 of the 8 exercises), and at 0, 5, 15, 30, and 60 min post-exercise. Males demonstrated significant (p less than 0.05) increases above rest in serum T values, and all serum concentrations were greater than corresponding female values. Growth hormone increases in both males and females following the P-2 HREP were significantly greater at all time points than corresponding P-1 values. Females exhibited significantly higher pre-exercise hGH levels compared to males. The P-1 exercise protocol did not result in any hGH increases in females. SM-C demonstrated random significant increases above rest in both males and females in response to both HREPs

[Source](_URL_0_)
Your mind eventually becomes quite use to itself. It generates routines and cycles and forms schematic patterns for deciphering input. In this case, the value of a neurotransmitter is relatively stable and thus used as a constant in cognition.

When a large amount of exogenous (external) neurotransmitters (or something that is functionally the same, like a hallucinogen)  are introduced into the brain, that well calibrated system is thrown into disorder. 1 no longer equals 1, it equals something approximately 1. This effect is most evident in the motion perceived through hallucinogens: the tight timing necessary for detection of motion is thrown off and thus everything is perceived as moving. 

In this state, our preconceived notions about reality can be shed and new understandings can be taken on.
Yes.  At the level of a photon, the polarization corresponds to its spin or, more properly, its helicity.

Photons have spin 1, which means the component spin along the direction of the photon's motion can be +1 or -1 (i.e., the spin is parallel or antiparallel to the photon's momentum).  These two helicity states correspond to clockwise and counterclockwise polarized light.

One can alternatively describe the photon's polarization in terms of linear polarization states.  If you have a standard linear polarizer and send a light beam through it, and the beam is low enough intensity so that generally only one photon at a time is goes through the polarizer, then the linear polarizer is measuring the polarization state of individual photons as they either pass through or are blocked by the polarizer.

Incidentally, the fact that the photon is massless, in fact, is why electromagnetic waves are transverse.  A massive spin 1 particle has three polarization states, the third one corresponding to the spin along the direction of the particle's motion being 0 and giving rise to longitudinal waves.
Combustion requires oxygen. And while Jupiter's upper atmosphere is 90% hydrogen and 10% helium with traces of other things there is almost zero oxygen so you can't have combustion. 

So you can't ignite it. And it's too small for fusion to start even if you hit if with hydrogen bombs. But you can make a bang though. When comet Shoemaker–Levy 9 hit Jupiter back in 1994 fragments hit with such mass and velocity as to create explosions with plumes rising 3,000 kilometers high. Waves rippled around the planet at 450 m/s and particles ejected at near light speed. Cool. But it still didn't set the place on fire.
We have no idea if Earth was ever fully molten or not. The leading idea is that we had a global magma ocean but how deep it goes (ie does it go to the core mantle boundary or not) is still up for debate. From the more recent results that I've seen it probably did not go to the core mantle boundary in which case metals would have stayed in the melt and eventually kicked out because they are incompatible (as the melt solidified into our current mantle). You can imagine the crust being the pond scum that floated to the top because the elements are "uncomfortable" in the mantle. 

If Earth did have a global magma ocean and it went all the way to the core mantle boundary then we have a problem because you are absolutely right because they would have gone into the core. In this case we would need to add them back in with meteorite impacts (which did add quite a bit of mass to Earth in the early history). However, I think the best evidence that this did not happen is that the mantle is full of carbon and some excellent work by Dasgupta et al (I can dig up the paper if you'd like) showed that carbon would partition into the core very efficiently and that until 800 million years ago it was too hot to add carbon back in by subduction. This leaves you with a problem if you want the global magma ocean to go all the way to the core mantle boundary since by that idea you wouldn't have carbon left (unless you can think of a better way to get carbon back into the mantle).
Nuclear engineer here.

The RBMK is graphite moderated. This means that the cooling medium and the moderation medium are separated. 

Under normal operation, water enters the bottom of the reactor, and flows upward. The water heats up on its way through the core, and the amount of steam voids by volume increases on your way up through the core. 

Steam is drastically less dense than liquid water, and is virtually transparent to neutrons when compared to liquid water. As such, an increase in voids means that my water is absorbing less neutrons. It also means that my neutrons will have a longer mean free path length, and ultimately means more neutrons will be able to get to my moderator. tl;dr, Increase in steam voids = increase in moderation = increase in power.

Inherently this has stability issues. As I increase my heat output, I'm going to increase the amount of voiding I have. This will then increase power, which further increases voiding. Active control systems which adjust control rods can come down to compensate for this. The control rods for this reactor design drop in from the top, which makes sense as the top is where the highest neutron flux is likely to be seen. Active control rod motion suppresses any power excursions and maintains the reactor in a stable operating state.

At high power levels, you are producing a lot of steam flow, and as a result, you have a high flow of water through the reactor. With high flow rates through a reactor, your boiling boundary remains fairly constant, and it takes quite a bit to have a runaway excursion, as the forced flow of water into your reactor tends to push voids out quickly and ensure cooling water gets to where it needs to be. Additionally at high flow high power conditions, the voids do not have a dominiant contribution to reactivity, meaning small changes in voids have small changes in reactivity.

When you are at low power, you are in a situation where you have low flow. Your boiling boundary is higher. Your voids have a much larger contribution to reactivity in the reactor. At low flow conditions with low control rod density conditions in boiling reactors, we observe the boiling boundary is somewhat unstable. Steam voids take longer to get out of the reactor, and they begin to have a stronger impact on reactivity. As such, anything that changes your boiling boundary even a little bit is going to have an amplified effect on your neutorn flux and power output. Little things like random noise in your pump flow can start these oscillations, causing the boiling boundary to move, which starts causing power oscillations. In the RBMK, to respond to the power oscillations, the control rods will start moving in and out to try and stabilize this. Having graphite tipped rods combined with having a control system with a response time constant in seconds (which is similar to the fuel's thermal time constant) just means that the control rods are going to be trying to catch a power change, but will have trouble keping up because it will be causing some of its own problems. 

All of these things together will drive power oscillations in the core. Under a worst case condition it can drive a runaway condition requiring a reactor scram. When operated appropriately, the reactor scram (even with the graphite tips) will have sufficient margin to preclude a steam explosion. When not operated appropriately (with nearly all control rods out), a power excursion can occur which leads to a steam explosion and loss of the unit.

The reason this is an issue at low power is due to the way the boiling boundary behaves at low flow conditions with low control rod density. The voids have too much contribution to the core's reactivity under these conditions, and small changes in voids drive large changes in flux. Because the voids contribute so much to reactivity under these conditions and the possibility of an instability can occur, RBMKs have a safety limit which requires a minimum number of control rods to be inserted at all times, to ensure that the voids do not carry enough reactivity to drive a core damaging power excursion. At Chernobyl they removed these rods past the safety limit because they were in the xenon pit, and trying to reach a specific power level on their reactor.

I'm more familiar with BWRs for instabilities. Standard BWRs like those in the US have stability issues as well at low flow low control rod density conditions, however the because the moderator and coolant are the same, they tend to be self limiting and are not capable of undergoing a power excursion/steam explosion. Most BWRs also have a system (called OPRM) that detects core stability issues and initiates an automatic reactor scram. What we see in BWRs is an oscillation with a time constant that is usually 1-2 seconds. We will start seeing small oscillations, then the oscillations will start growing. Oscillations in a BWR grow slowly. If the plant has an OPRM, it will scram the reactor prior to it increasing past certain limits. If the plant does not have an OPRM, it will grow over several minutes until the reactor hits either the high or low flux scram setpoints. The main danger in BWR type reactors is that you can cause localized thermal stress and plastic strain on the fuel (localized fuel cladding damage), but no power excursion.

For BWR light water reactors that have it equipped (all US BWRs have this) the OPRM (Oscillation Power Range Monitor) looks for counts (how many oscillations am I getting in a row), period (are the oscillations in the right time constant that is indicative of thermalhydraulic instabililty or is it just random noise), and growth (is my oscillation diverging with a  > 1.0 decay ratio). There is also a confirmation density algorithm which uses a factor that looks at the above factors across the core to anticipate these factors before they start. If the OPRM gets enough counts, on the right period, with growth, it will initiate a scram immediately. US BWRs are forbidden from entering the region where core thermalhydraulic oscillations exist, and are required to insert an immediate reactor scram if they enter the region.

Generally, the only time a BWR enters the instability region is if they have an inadvertent loss of their reactor cooling pumps. 

Another poster mentions xenon. Xenon can cause long term issues which force you into a low rod density low flow situation, however xenon will not cause the instabilities that created the chernobyl event. Xenon does not respond fast enough (hours), while the boiling boundary response is in seconds. He's not entirely wrong, xenon transients can force you into an instable core operating region, but they do not cause the instability. Proper reactivity management can ensure that you pass around the instability region without going into it. RBMK reactors have a very tough time with the xenon pit and are more likely to put themselves into an instability, while other BWR type reactors can deal with it just fine.

I hope this helps. If you have any other questions please let me know.
Yes, laser light is just coherent, monochromatic, but otherwise ordinary light, so lenses and mirrors interact just as they do with ordinary light.
After [reading this summary](_URL_1_), what questions remain?

One thing I can mention is the limitations:

* With two telescopes, you only get good resolution along the axis connecting them.  So you can see fine detail along the Earth-Pluto axis, and it's all blurry at right angles.  This is why [triangular arrangements](_URL_0_) are popular.
* Although what matter is the ratio between the total width and the wavelength, you have to know the relative positions of the telescopes and the phase of the arriving photons to within a small fraction of a wavelength to do the combining.  Thus, the technique is more popular for radio astronomy where this is practical, and is still quite exotic at visible wavelengths where it's fiendishly difficult.
* In addition to the diffraction limit, there's also that annoying *seeing* limit, caused by atmospheric distortion.  If your Earth-based telescope is on the ground, underneath all that air, it gets more difficult.
In fact, the vast majority of particles won't collide in any given "crossing" of the beams - these will just continue to go around the collider until the next crossing point, and then have another go at trying to collide with the opposite beam. The low rate of collisions means that that the beam lives for quite a long time - [this page](_URL_4_) looks at the LHC beam lifetime, and calculates that the time for the beam to reach 36% of its original intensity is 80 hours!

However, as that page continues, the tube containing the beams isn't perfectly empty - despite all the work that goes in, various gas particles still manage to get in the way of the beam (some particles also come from the material making up the tube itself). Therefore, the particles in your collider may accidentally hit one of these gas particles, and hence leave the beam. The linked page says this reduces the beam lifetime to 45 hours.

With all these particles flying around, it's quite important that they are all on the same orbit - if particles somehow enter orbits that involve hitting the sides of the collider, this could eventually start to damage the equipment that makes up the collider. Especially of note is the risk to the superconducting magnets used - high-energy particles hitting these would heat them up, and bad things™ happen when you heat a superconducting magnet enough to stop it superconducting (a cooling fault in the LHC which allowed some magnets to heat up was the reason that it was out of action from Sept 2008 to Nov 2009).

The first reason a particle could end up in a bad orbit is that it is incorrectly aligned when it enters the collider (the LHC has a series of accelerators that feed into it, which could send in "bad" particles); [this paper from CERN](_URL_3_) states that about 2.8% of particles are lost initially, because they aren't where they are expected to be when they turn up the energy.

There are various reasons a particle could become disaligned whilst in the collider. Colliders almost invariably use charged particles, meaning that the particles within a given beam will repel each other - there may also be similar effects from near-misses with the other beam. Hitting (or nearly hitting) the previously mentioned gas particles might only alter the particle's orbit slightly (rather than throwing it out completely). There's also the chance that the operator or collider aren't doing their job entirely correctly!

The way of stopping all these incorrectly aligned particles is through the use of collimators, and those in use at the LHC are discussed in [this paper](_URL_1_) (the previously linked paper discusses various losses due to collimation) - it has a lot of good diagrams and pictures, and is written so that (I think!) most people can understand at least the concepts behind it.

Eventually when running your collider, your beams will reach a low enough intensity (due to collisions and the other losses) that the collision rate is too low to be useful - you would like to get rid of the current beam and get a new one in. However, you somehow need to get rid of a lot of very energetic particles; you can't just stop them in the beamline and empty it into a bucket!

Instead, you build some big blocks of absorbing material, and when you're done with the beam, you direct it into these. A *huge* amount of energy is released, much of it in the form of dangerous radiation of one sort or another; the [LHC's beam dumps](_URL_2_) consist of a 7m long cylinder of carbon, 70cm in diameter, surrounded by successive layers of steel, concrete and iron shielding, with the shielding totalling 750 tons. In addition, the entire thing is water cooled, to stop it from getting overly hot when the entire beam hits it. *Even then*, you still need to take further steps to prevent damage to your beam dump, as it is unable to take the entire energy of the beam in one spot - I have heard the energy compared to an aircraft carrier sailing at several knots, and the beam concentrates that energy into a circle of radius 0.3mm! The LHC uses a series of "kicker" magnets to spray different bits of the beam onto different areas of the beam dump, ensuring that the energy is dissipated over a larger area. [This graph](_URL_0_) shows the pattern that is created.

I think that covers most of the possible end states for your particle, whether accidental or intentional - I'll await corrections from various particle physicists! Almost all of what I said is based on the LHC, but I would imagine that the principles are almost identical no matter where you go.
RN here. There is data that coincides with this, but does not back it up. People who are unconscious are much more likely to die than people that are conscious. But this has nothing to do with consciousness, but rather that people who go unconscious tend to have sustained more severe injuries (more brain damage, more blood loss, etc). 

So, staying awake is correlated with better survival, but not because wakefulness has anything to do with it. More that being unconscious just usually means you're badly injured. 

As far as the airway comments, these also fit into the above model. If you are lightly unconscious, you'll keep your airway, but if you are profoundly unconscious and not arouseable, you'll lose the airway an increase pneumonia risk. But again, profoundly unconscious is associated with greater injury.
There isn't a clear-cut limit. Signals gradually become fainter as you get away, but the problem can be mitigated by designing the communications subsystem for distance. So the actual number depends on the details of each particular spacecraft.

High-gain antennas (usually parabolic) send a signal in a defined direction. They still fade (disperse) with the square of the distance, but with a much smaller constant factor. By decreasing the data rate, in the case of digital signals, you can increase the energy per bit. This makes it easier to distinguish from background noise. And of couse, you can always transmit at a higher power and use a large diameter for the receiving high-gain antennas.

IIRC Voyager has a 10W transmitter and a 2m radius antenna. That's huge. So it can transmit a detectable signal at 140 AU. Satellites designed to operate in Low Earth Orbit might no longer be detectable if you take them away at a tiny distance compared to Voyager.
No, the gravitational pull of the earth would be, if my back-of-the-envelope math serves right, about 90% what it is on the surface of the earth. Gravity does decrease with the square of distance, but the surface of the earth is about 6400 km from the center of mass. The ISS is about another 400 km, or an increase of about 6.25%.
This redundancy varies from company to company as well as on the type of data. A company like Google or Dropbox has datacenters all over the world that store your data. WhatsApp, on the other hand, does no data storage so will not have such a replication. Typically, your data is replicated across datacenters to ensure that a copy is always available if the primary datacenter goes down.

For example, If you are in Germany, Google may serve you webpages from it's servers in Iceland (example). Any data you upload will reside in those Iceland servers. As a backup, this data will also be replicated in servers residing in Denmark (example). If the disks in Iceland carrying your data fries on 1 fine day, google server in iceland can ask a redundant copy of data from denmark servers. Usually, large data transfers on a  network can clog the bandwidth, so it is very much possible that the iceland server may forward the entire request to denmark servers for processing.

There a whole range of tools from [RAID](_URL_1_) to [SRDF](_URL_0_) and lots of custom DB replications. The choice depends on the hardware and nature of business.
Following your definition of intelligence, I looked for evidence of tool use in both types of animal.

Ravens: [One](_URL_4_), [Two](_URL_0_)(ish), [Three](_URL_3_).

Dolphins: [One](_URL_2_) example is all I could really find.  However, [they do appear to recognize their own reflections](_URL_1_), which seems to impress a lot of people in terms of their intelligence.
Dissolved gasses come out of solution.  Your tap probably has an aerator/diffuser that adds more air when you pour the glass.

_URL_0_
The core is solid due to the pressure conditions,  not temperature. the temperature of the earth increases with depth.
Something basic or acidic, perhaps. It would act just like soap and rip apart bacteria that touch it.

Another possibility is that it's not antibacterial, but has a pattern imprinted on its surface which prevents the formation of colonies, which are some of the more common causes of infection in this form.
The sound originates from the lightning channel. Once the channel is formed, a high current runs through it. This current is what generates the light you see and while the channel is rather conductive, the current is high enough that a large amount of heat is also generated.

This heat causes a rapid expansion of the air in the lightning channel and this rapid expansion causes a shockwave which is the origin of the thunder sound.

Thunder is a rumble rather than a short bang, because the shockwave is created all along the lightning channel and different parts of it will arrive at your ear at different times. In addition to that, local differences in atmospheric conditions as well as the sound reflecting off the landscape further causes the sound to be distorted and stretched in duration, ultimately leading to the rumbling sound that we hear.
> Photons are said not to experience time.

In special relativity, *c* is not a valid frame of reference, and we can learn why if we look more closely at one of the major axioms of relativity - the notion that all *valid* frames of reference will measure the same value for the speed of light in vacuum.  When we travel at some velocity v, other objects traveling with the same velocity will appear at rest in our reference frame and we will measure photons in vacuum to be traveling at *c*.  If v=*c*, we will again measure all objects traveling at the same velocity to be stationary, which means objects moving at *c* will not be moving relative to us, this of course violates the notion that we must measure photons in vacuum to be traveling at *c* since they will instead appear at rest in our frame of reference.  So we must conclude that *c* is an invalid frame of reference.  A bit wordy but I hope it's clear enough for my point to come across.

When light travels through matter, it travels slower than *c*, so there is no conflict with special relativity.
I can't say much for the differences in sensitivity among species, but other animals do have sensitive testicles. If you have ever seen a bullfight, they tie up the bulls testicles  before they release him (one of the reasons he is so angry). From an evolutionary standpoint I imagine our biology has learned that the reproductive organs are the most vital to continuing of the species, and thus we have developed a low threshold of pain in those areas so we protect those organs.
A lot of VPNs authenticate clients using certificates issued specifically to them, which enhances security because the actual certificate file would have to be stolen (it's very hard to crack one) to thwart the access control. Often the cert is also encrypted with a password to secure it further.

I've never been administrator for a VPN that authenticates using only a password, but I have seen them before. Usually it is a web-based login that launches the VPN software as a sort of plugin. In this case, the password can be guessed by a malicious party. Even the strongest passwords aren't going to match up to a 4096-bit asymmetric key (i.e., certificate) but it's still "hard" provided that the password isn't dictionary-based or very short.

Ultimately, it comes down to how good your password policy is. If you let people choose "password123" as their password, some unauthorized person is going to get in.
The pressure will be the same. If you look at the equation for pressure, its:

P = F/A. 

In this case the force is the weight of the water on top of you, 

P = mg/A

Where m is the mass and g is earths gravity. The mass of the water can be found using the volume and the density. But the volume is the area times the height (depth) 

P = hApg/A. 

The areas are the same so the equation becomes 

P = pgh


the size of the body of water doesn't matter
It could be because of the cone response: 

_URL_0_

The 400 nm blue peak is at the left edge, and you can see that there's some increased response for the red cones, so it might explain the reddish tint of hues nearing ultraviolet.
Well at earths surface, you're talking about approximately 50 microteslas. This just simply isn't strong enough for any serious biological effect on humans. 

Some MRI machines operate around 2-4 teslas, and even these have no negative biological effects.
It is a combination of factors. First, they are both in the Hadley Cell's subsidence zone around 30 degrees south, which is an area where generally there is downward motion in the atmosphere, suppressing clouds and rain. Second, they are along oceans, but the eastern side of the oceans, which are ruled by [eastern boundary currents](_URL_0_), which transport cold water towards the equator. The cold water cools the air as well, and since cooler temperatures mean less water can evaporate, we are left with desert despite being near the ocean.
The Earth's rotational and orbital speed stay so constant due to the conservation of momentum- the only way it would change is if the energy in the Earth system could be dissipated somehow.

The length of the day (i.e. how long it takes to rotate once) is actually getting longer due to energy being converted into heat via tidal friction. Interestingly, day length isn't as exact is it might seem, and with precise measurement of the Earth's rotation we know that there are slight variations in day length throughout the year/millennia. This is likely to be down to changes in mass distribution of the Earth, which means that the conservation of momentum law isn't broken. Earthquakes can change the rotational period of the Earth this way.

[Here's a JPL article on it.](_URL_0_)
The Earth's interior dynamo is almost certainly driven by spin-related shear in the liquid metal core of the planet.  The main anisotropy in the system is the axis of spin, so it's reasonable to suppose the magnetic poles spend most of their time pointed close to the axis.  

That said, the Solar dynamo has broadly similar properties (though much shorter time scales) and the surface magnetic field of the Sun is far from simple -- even though the main dipole moment of the Sun spends most of its time pointed close to due Solar North or due Solar South.  

How does shear make a dynamo?  Start with an electrically conductive liquid and induce a shear flow.  A peculiar property of electromagnetic induction will make magnetic field lines that happen to thread through the fluid tend to stick to individual pieces of fluid.  A field line that crosses the shear layer will get stretched like a strand of taffy on a taffy puller.  Stretching the field line increases the sum total field line length inside the volume of interest, which is the same as creating new magnetic field.   If you have a big ol' mass of highly conductive stuff (like solar plasma or like the nickel-iron core of a terrestrial planet) and it has any sort of driven flow (perhaps from convection in a spinning system), it can amplify microscopic magnetic fields (which could be initiated, e.g., by Brownian motion) until they become large.

Human scale dynamos work the same way: you run an electrically conductive system past a fixed permanent magnet, and field lines from the magnet get stretched by the conductive system.  That's another way of saying you've induced a current in the conductor.  You can then run that current through devices like blenders or light bulbs to do some useful work.  But you can make a dynamo with no permanent magnet at all - you build a regular dynamo, then instead of a permanent magnet you use an electromagnet powered by the dynamo itself.  If you turn the crank on the dynamo fast enough, you'll get electricity ... eventually.  Small fluctuations eventually induce enough current in the electromagnet to get the whole system working, and away you go.
It mainly depends on the total energy imparted by the "hitting", and how resonant the bell is.  

First, total energy imparted:  This is not so much about area as about how hard you hit the object and how efficiently energy gets transferred from the mallet (or whatever) to the bell.  "How hard" is controlled by the mass and velocity of the mallet.  How efficient the energy transfer is depends on the materials properties of the mallet and the bell.  In a basic sense, squishy things don't transfer energy as efficiently as hard things, but that glosses over a lot of details.  

Second, how resonant is the bell:  This is a bit more subtle.  By "how resonant" I mean that certain materials and certain shapes "like" to ring more than other materials or other shapes.  Bells tend to be made out of brass rather than aluminum, for instance, because brass "likes" to ring better at audible frequencies.  Squishy things do not like to ring at all.  Hard objects like to ring.  But if a hard object is too brittle, it will just break instead of ringing.  The details of why specific fairly similar materials (like, say, aluminum and brass) don't resonate the same amount has to do with the details of the metal at the atomic level.
I'm assuming you mean percent identity between humans.  I'm not sure exactly what the percentage identity is across the entire chromosome, but I wouldn't be surprised if it were 99.9% even including non-coding regions.  Chimps and humans are somewhere around 99.5% identical in protein coding regions.  It's hard to assess it with clear metrics, though, because what's the percent identity for a gene that's present in one species but not the other, or has a 3 amino acid deletion in one species but not the other?

Humans are not very genetically diverse compared to most species.  It has been hypothesized that we nearly went extinct about 50-70,000 years ago, possibly due to the Toba supervolcano eruption.  The human population at that time was estimated to have dropped to only a few thousand individuals.  So we pretty much all share common ancestors not much further back than that.
Waves are generated by wind blowing on the ocean. There are two main factors: 1) the speed of the wind  and 2) the distance over which the wind acts, i.e. the 'fetch'. I'm not aware of any reliable estimates for wind strength during the time of Pangaea.  There are scientists who run atmospheric circulation models for alternative continental configurations such as Pangaea but I doubt we could say with much certainty that the winds at that time were either significantly stronger or weaker. 

 That leaves the issue of fetch. In general, the greater the distance the wind blows over, the bigger the waves but that only holds up to a certain point - about 1000 km. Beyond that the waves are fully developed and additional fetch doesn't lead to bigger waves. The Pacific is about 10,000 km wide so it already exceeds this critical limit. Therefore I don't think there is a strong reason to believe that the waves at that time were any larger than what we see during big storm events now.
There are whole house surge suppressors, but they only protect against surges which originate outside the house.

To protect sensitive equipment from the surges that originate within the house (from air conditioner, refrigerator, other large appliances, etc.) you need a suppressor between the outlet and the device you're protecting. A good suppressor will also protect against surges on phone/data/cable lines as well.
The problem is thinking too classically about a particle! Key point here: momentum is a quantum operator with an expectation value and does not have a classical value.

When a neutrino is emitted from some interaction, it is emitted in a flavor eigenstate. A flavor eigenstate, though, is a superposition of energy eigenstates.

Now, you might think that the neutrino is also in a momentum eigenstate, but that's just not true. The easiest way to see that is to consider each of the possible energy eigenstates being emitted individually (which is not the case) and noting that energy and momentum are conserved at the individual interactions[1].

What you end up concluding is that the other particles in the interaction are also in a quantum superposition of three momentum states, each corresponding to the nu1, nu2, and nu3 energy eigenstates of neutrinos (the three branches of the neutrino wavefunction).

Basically, the neutrino is entangled with the other things in the interaction that produced it. Each branch of the wavefunction individually satisfies momentum conservation.

[1] This is what you would do if you wanted to draw the Feynman diagrams for calculating this interaction -- and then add them with appropriate phases.
To find the wind vector you first find the air speed vector and the ground speed vector. Aircraft are equipped with instruments to calculate the air speed vector. Pitot tubes and static ports can be used to find static pressure, dynamic pressure and air density. With these three values the aircraft can calculate the air speed vector presumably using Bernoulli's equation. The relative ground velocity vector can be found using GPS systems. Once these two vectors are found the wind velocity vector can be found by vector subtraction of the air speed vector from the ground speed vector. 


NASA has written some nice articles on this but not only for the wind speed velocity component parallel to the aircrafts velocity. I also recommend you learn more about pitot tubes if you are unfamiliar with them. Using multiple pitot tubes in different directions is how wind components perpendicular to motion are calculated. 


_URL_0_


_URL_1_
To answer your tl;dr: both. It depends on the conditions and the size of the barriers.

Let's assume that both B and C are lower in energy than A, just for simplicity. If we provide A with enough energy that an average molecule can cross the barrier to either B or C, and enough energy to cross back, you'll reach the equilibrium distribution. This is given by the relative energies of the components. In practical terms, if you heat the heck out of it, you'll get the thermodynamic distribution - and if B is sufficiently lower in energy, then for all intents and purposes you'll end up with just B.

Kinetic control is a bit fiddlier and depends on how much energy you put in, and when you stop the reaction. In the above scenario, say you measure the composition as it evolves over time. You'll find that initially, C dominates, even though molecules can cross to B which is more stable. You'd expect the initial rate of formation of B and C to be proportional to the energy difference between the barriers to their formation, but over time as molecules cross back, for B to come to dominate as above. 

However, say you only put in enough energy for A to cross the barrier to B or C, but not enough to cross back from either. You'll end up with the kinetic distribution, which is again proportional to the energy difference between the barriers to B and C.

If you put even less energy in - practically, you might cool the reaction down - you can reach a point where A can cross to C (the lowest barrier) but not cross back, and can't cross to B. Your distribution is then dominated by the kinetic product.

This has practical consequences. For instance, asymmetric catalytic methods often proceed at low temperatures to exploit the small energy difference between diastereomeric transition states leading to the two enantiomeric products. At low temperatures, you are better able to isolate the kinetic product and maximise your selectivity.

This is all a pretty simplified (and hence inaccurate, really) summary, and I encourage you to go look at the math yourself. [Wikipedia](_URL_0_) has a reasonable overview, but really you want to grab a decent organic or physical chemistry textbook, and go through the math.
Adipose tissue acts as an insulator, reducing environmental heat loss. As a result, exothermic activity heats the body faster. Additionally, the extra mass means that they exert more energy moving around (which is why mobile 'heavy',people like myself have huge, muscular thighs). If you look, you'll find many animals in frozen climates insulate themselves with blubber to keep warm.
Chickens are are the domestic version of Red Junglefowl, *Gallus gallus*.  [Here](_URL_0_) are a bunch of pictures of them. And [here](_URL_1_) is a fairly recent full-text research paper testing multiple subspecies of Red Junglefowl; they found evidence that chickens were domesticated several different times. There are also 3 other species in the *Gallus* genus (known as Lafayette's, Green, and Gray Junglefowl) that may have contributed genes to the domestic breeds.

The paper cited above puts it this way: "Archeological findings have indicated that the 'mother of all poultry' is the Southeast (SE) Asian Red jungle fowl (RJF) (Gallus gallus)." (and this is backed up by genetic data)

[Here](_URL_2_) is a 2010 Nature paper that did extensive comparisons of 8 different chicken breeds to red junglefowl, identifying the loci that were under selection during domestication. Interestingly a major one involved the gene for TSH (thyroid stimulating hormone) receptor, which affects timing of breeding in relation to daylength, and which may be responsible for the ability of domestic chickens to lay for most of the year. Several other "selective sweeps" were identified that involve genes associated with metabolic rate, growth and appetite.  

PS - Red Junglefowl still exist and are quite common in some areas. I've seen them walking around in Hawaii, where they are an introduced species.
There isn't really one statistician responsible for developing the equations used to estimate sample size.

There are roughly 3 equations used to estimate sample size; statistical power, med's resource equation and the cumulative distribution function. I've almost exclusively seen the use of statistical power calculations in biology. This allows you to estimate the rate at which you'll correctly reject the null hypothesis for a given sample size and rate of error.

Wikipedia has 2 nice articles covering sample size determination

_URL_1_

And statistical power

_URL_0_
Technically there are. Each droplet acts as a prism, separating the colors. From your perspective you only see a fraction of that light from each raindrop.

The full rainbow effect is caused by seeing slightly different perspectives from millions of different raindrops. One raindrop gives you red light, while another raindrop (at a different location) gives you green, and another refracts yellow, etc.
Memories and skills are not encoded in individual neurons, but rather connections between groups of neurons.
It doesn't affect how the ball interacts with the air, but it can change how it interacts with the club.

Rubber gets softer as it heats up, and softer rubber compresses more when it gets hit. The general consensus in the golf community is that warm balls therefore travel farther, but I can't seem to find anything about it in a peer-refereed journal.
Your brain can't either. Suppose the random question is:

"Pick a number between 1 and 20, for example 13"

If you ask this question to hundreds of people then you will not get a uniform distribution over the numbers 1 through 20.
This is a really broad question, since excessive cortisol in the human body can have a lot of implications. I'll just talk about one of them.

One area of your brain that has a lot of cortisol receptors is the hippocampus. There is some evidence that excess cortisol can cause the hippocampus to be damaged in various ways. Individuals with excess cortisol have been show to have smaller hippocampi, suggesting that certain cells called pyramidal cells in the hippocampus likely atrophy due to cortisol activity. Another idea is that cortisol in the hippocampus suppresses neurogenesis, or the formation of new neurons. Both of these are likely causes of depression, and SSRIs both work to reverse these effects of excess cortisol in the hippocampus.

Extremely high cortisol in a short time can also impair memory. This is why individuals often can't remember times where they're extremely emotional.
Drugs work (most often) through receptors which mediate various bodily functions and attributes. The problem arises when 1. Drugs are not selective between similar receptors (Beta1 vs Beta2 receptors) or 2. When stimulation or inhibition of the receptor mediates other bodily functions (morphine acts upon the mu receptor which causes analgesia AND respiratory depression, among other things). 

The goal of drug development is to use the lowest dose possible to prevent side effects while achieving the goal of therapy. 

Sometimes you can use lower doses of two or more drugs to achieve the same effect as a higher dose of a single drug. This minimizes side effects. This is common with anticonvulsant therapy among others.
According to [this](_URL_0_) paper, it will depend on the mass area loading on the sail, which is equal to (total mass) / (area of sail)      
but the article says:       
     
 > The slowest
approach in Table 1 passes the 0.01 AU perihelion point and continues out into space at a
constant velocity of 0.0014 С (420 km/Sec).   
    
You should check out Table 1. It has other values up to 0.009 C.
_URL_0_
_URL_1_
Yes, they do sunburn. But given same amount of time, as someone who is white, not as much and its less likely to occur. Just read the wiki article, is quite nice.
There are numerous differences, but they can most easily be seen where things go wrong, such as with diseases which seem to target specific ethnic groups. E.G. Sickle-cell Anemia, Chrons Disease. 

[Here's a great article for you.](_URL_0_)
Other well-documented non-disease traits would be that Europeans have a greater tolerance to lactose than many other ethnic groups. There is even a slightly rare mutation in Europe which grants immunity to HIV. 
Note that genome mapping is making questions like this much more accurately answerable. 

As far as macroscopic organ structure/color goes, things are pretty standard.
Point masses are artificial constructs. When we actually want to understand what happens at that small of a scale, we have to use quantum theories. And, well, we haven't figured out quantum gravity yet. So we just admit ignorance of the answer to this question for the time being.

Perhaps more satisfying to you would be the fact that both electrostatics and gravity follow the same 1/r^2 behavior. We do know how EM works.

QED is the proper theory for the extremely small r regime. At this level, the 1/r^2 disappears and we get interactions between two electrons/positrons and a photon. The attraction/repulsion is entirely probabilistic (with some kinematical limitations). But when we look at a big enough picture of these individual particle interactions, we see things start to slowly resemble this 1/r^2 force. 

QED is quite complex, but if you are interested in learning some, Richard Feynman wrote an incredible layman book on the topic titled: "QED: the Strange Theory of Light and Matter."
I don't have time for a full response with my own personal input, but see below:

The Hasan-Kane review article is a well-known one, and at the end of the intro section it references several of the other reviews: _URL_0_

This article in Physics Today is also nice for a broader, briefer perspective: _URL_2_

I also refer you to the slides from a set of tutorial talks from the 2011 APS March Meeting, especially the first one by Joel Moore: _URL_1_

Materials growers are working hard to get these materials optimized for physics experiments beyond ARPES and STM, and I'm sure any scientists with relevant knowledge (i.e. physical chemists ;) ) would be welcome to help solve these problems. For example, one known issue is that the surfaces of these crystals degrade upon exposure to ambient atmospheric conditions. The electronic surface states are "topologically" protected as promised, i.e. they still exist, but their "quality" is significantly reduced (badger me and I can answer what "quality" means later). Note that surface states in other, "non-topological" systems typically get destroyed upon such exposure, so one of the interesting facts about TIs is the fact that the surface states are "protected", in a sense, regardless of how much the actual surface's structure gets screwed up. 

Hope this helps! If anyone has more questions, or wants more information, I can return later. 

*edit: more info
Utilization is what the amount of filled vs. total disk space is right now.

IOPS measures how much a disk "works" during the day, or specifically the number of **I**nput/output **o**perations a disk makes **p**er **s**econd.

They're two different variables and don't correlate, meaning that a high amount of IOPS does not mean a high disk utilization and vice versa.
By definition.

Basically take [this answer of mine](_URL_0_) and replace the up-type quarks (u,c,t) with the charged leptons (e,μ,τ) and replace the down-type quarks (d,s,b) with the three neutrinos. Exactly the same thing happens with the leptons.

(There's a difference from quarks in that the neutrinos could also be Majorana. Plus other complications. But the general conclusion of that answer still holds in that case. See u/majoranaspinor's comments there).
Consider a hypothetical gene called "Survivor". As it is involved in a compound heterozygous autosomal recessive disease you need one functioning copy of "Survivor" to not be sick.
If you inherit one mutated form from one parent and a "healthy" form from the other parent, you will be healthy. The DNA with the healthy allele will make enough protein of "Survivor" to keep you healthy.
However, if you inherit two mutated forms from Survivor, you will be sick.
In the case of "Survivor" it doesn't matter where the mutation is. So you can inherit the mutation "SArvivor" from one parent and the mutation "SurvivAr" from the other. Since now both alleles are defunct you will be sick. 
You would also be sick with the same two copies of either one.

However, there are some genes where compound heterozygous alleles can work together to keep you healthy.
In those cases the first part of "SurvivAr" can work together with the second part of "SArvivor" thus keeping you healthy.
The first thing that should strike you about those skeletons is that they have exactly the same proportions as those of a normal sized human. This is not physically feasible:

As published by McMahon in 1973 (*), the resistance (_R_) of a skeleton grows in proportion to the cross-section of it's bones with a diameter _d_ (a surface, so _R ∝ d^2_) while the weight (_W_) the skeleton must carry depends on its mass (i.e. its volume, so _W ∝ l^3_, with _l_ being the length of the bone). Therefore, as animal species get larger, the thickness of their bones must grow faster than their length to keep up with the greater weight. Specifically, the diameter of a bone has to keep up with the increased weight it needs to withstand is _d ∝ k·l^(3/2)_. If an animal species is twice the height of another, its bones must be 2.82 times thicker than the smaller species; if the larger species is three times higher, its bones must be 5.19 times thicker.

A good example can be seen in the Wikipedia page on [allometry](_URL_0_), where you can compare the skeletal proportions of an elephant and a tiger quoll (who knows why they chose such an obscure small animal for this example).

To summarise: those bones are not strong enough to support a giant of that size, and are therefore a hoax.


**Reference:**

\* McMahon, T. 1973. Size and Shape in Biology. Elastic criteria impose limits on biological 
proportions, and consequently on metabolic rates. _Science_, 179: 1201 – 1204.
The momentum from the photons wouldn't do much, but the idea still has some merit as the laser beam could burn the debris and the vapour would provide a bit of thrust. 

EDIT: Here is a link:_URL_0_
Yes.

It may lead to wildly different outcomes, but "evolution" (the process) at it's core is simply that the most reproductively fit genotype individuals for their environment will contribute a disproportionate percentage of alleles (gene variants) to subsequent generations, changing the allele frequency at the population level.

Reproductive fitness doesn't necessarily mean "most offspring", though it can. It means the most *successful* offspring, and is often examined in the third generation (how many grandkids, rather than kids). 

More reading: ["Reproductive Success and Fitness are not the same thing"](_URL_0_) by Greg Laden, Scienceblogs
First, the charge of the ion isn't relevant here, as the atoms are covalently bonded to the rest of the molecule. (Hydrochloride and sulfate salts are different.)

The C-S bond is almost completely nonpolar, as the elements have very similar electronegativities. Cl is a bit more electronegative.

Finally, compared to nitrogen or oxygen, chlorine and sulfur aren't very good hydrogen bond acceptors. This is especially important when considering partitioning between aqueous and lipid phases. (However, if there are S-H bonds, then they will increase the overall water solubility.)
While some symptoms do match an [allergic reaction](_URL_2_), it's not an allergy to alcohol but an alcohol [intolerance](_URL_0_) which stems from the lack of enzymes needed to properly break down alcohol. 

The difference is that in an allergic reaction there's an immune response (involving IgE antibodies) against a particle that shouldn't normally trigger a response, which ultimately leads to the release of various compounds that mediate an inflammatory reaction into the bloodstream. This does not happen in alcohol intolerance. 

Source: [Mayo Clinic - Alcohol intolerance](_URL_1_)
Your brain runs around ~~10 Watts~~. 10 Watts = 10 Joules per sec. There are 4.184 Joules in a calorie (little c, not Calorie, which is 1000 calories). That's 2.39 calories per second. There are 86400 seconds in a day. 86400 x 2.39 = 206496 calories, or ~~**206.5 Calories per day.**~~

EDIT: I was going off of some research that I had done about a week ago, namely [#30 in this list](_URL_1_). Being lazy as I am, I didn't read into it and just remembered the 10 watts. As several people in the comments are saying, the power is closer to 20 watts. [WolframAlpha](_URL_0_) is one source of this other wattage. So double my original estimate = 413 Calories.
**tl;dr: 413 Cal, not 206.5**
Well, it's enough that you take into account 'classical' deterministic randomness to make chaotic processes unpredictable in practice. 

If you take into account quantum non-determinism (a matter of interpretation, although most are inclined think it is), then yes, that'd make stuff unpredictable even _in principle_. But it's also possible for it to be deterministic and _still_ be unpredictable even in principle, since there may be limits to how much you can actually know about the system. (This is the case with the Bohm's deterministic interpretation of QM)

At the microscopic quantum level though, QM actually works _against_ chaotic behavior to some extent. A lot of quantum-mechanical systems (say, the electrons in a many-electron atom like Helium) behave non-chaotically in situations where their classical counterparts would.
These guys are really cool, huh ?! 

You have to remember that evolution has no end plan -- it's not a series of successive improvements toward a perfect portrait on the wings. each individual that had something that offered even a slight selective advantage helped carry that trait to the next generation. So, something this complex likely started off as a splotch on the wings. With each little improvement to make it more "insect-like", or wing shaking behavior (likely a combination of both over time - no idea which came first), the owner of said improvement had increased survival/fitness, living to reproduce and pass on that trait. changes that improved on the portrait were selected for, and changes that hurt the portrait were selected against (they likely weren't as functional as their bug-colleagues). so it was a relatively long process over time. and it's important to note that the markings might not have always served the same function - when they were less defined, they may have acted like eye spots or distractions from predation, for example.
No. The weak [equivalence principle](_URL_0_) (which says that the strength of gravitational interaction affects matter in direct proportion to its inertial mass) implies that (from the wikipedia link above):

*"The trajectory of a point mass in a gravitational field depends only on its initial position and velocity, and is independent of its composition and structure."*

This has been tested to be true to within a factor of 10^-13 (see link above).

Of course, mass gives rise to gravitational fields, so particles with more mass create larger gravitational fields. So in that sense indeed certain type of matter generate different gravitational fields: more massive matter generates stronger gravitational fields.
Nope.  Atoms are indistinguishable from one another.  Two C-13 atoms look the exact same.  Even assuming you know the decay constant exactly, it only means you have a knowledge of the likelihood of decay.  Assume you have a single U-238 atom.  It has a half life of 4.5 billion years.  How would you go about measuring its age?  All we know is that a sample will have half of the atoms decay within 4.5 billion years on average.  With very small samples dominated by Poisson statistics, who knows.  Maybe all will decay before 4.5 billion years, or maybe none will.  You cannot tell the age of a single atom.
There's the sacoglossan sea slug, which practices [kleptoplasty](_URL_0_), utilizing the chlorophyll it obtains from algae it eats.
The Bootes Void is consistent with [Lambda-CDM](_URL_1_), the `standard model' for cosmology. Quantum fluctuations in the early Universe result in overdensities and underdensities, that are magnified by [inflation](_URL_0_). After this, gravity makes overdensities become more overdense, and underdensities more underdense.

Overdensities become galaxies/galaxy clusters/etc, and underdensities become voids. Smaller voids close to one another can expand a little bit and merge, creating bigger voids, explaining larger voids like Bootes. I don't think I've ever seen it mentioned as a problem, only as an example (albeit a very striking one).
If by proportionally, in relation to the temperature (ie. 1 degree = x amount of time) then no. Animals undergo extreme body cooling, or torpor to slow the metabolic rate of the cells in the body to conserve energy. The mechanisms of this process are still being researched but the sole purpose of it is to save energy. But yes they would likely have shorter lifespan in a controlled environment because the cells would actually conduct more "work" in a wood frog that isn't allowed to undergo torpor in an experimental environment vs a wood frog that is allowed in the same experimental environment. Note that I specified in an "experimental environment" because animals will only undergo torpor if the conditions demand it--and there are complications and risks involved for the animal to undergo torpor in a natural environment.
Likely they did not. When looking at animals, even ones intelligent enough to form societies and empathy for their fellows, when an individual suffers a traumatic injury, they often die. 

Breaking a bone in a locomotion structure like legs or wings would almost universally guarantee death. Breaking a bone in a less important structure, like a rib or a phalanges may have only resulted in decreased proficiency, and possible survival if the primitive humans we're talking about have evolved enough to care about the survival of their comrades.

To answer the question of if a bone would heal without a cast, yes. Nothing about a cast facilitates the body's healing response to injury, rather only cultivates the best possible environment for that healing to occur. As long as bone is flush with bone, the bones will fuse together. This is true of any bone in any situation. Newborn's skulls fuse together. Old people's joints who have lost the cartiliage and connective tissue fuse together. People with rare bone disorders that end up not having any lubrication in their joints fuse together. 

If, through some range of circumstance, a primitive human in a society conducive to survival of all persons of a group, were to break their leg for example, survive infection, set the bone and other humans cared for the person if the individual was unable to move about on their own, then yes. The human would survive, though would never be back to "normal" is it were.
The biggest problem would be the increase of amperage in the 110v outlet and wiring, and hence the increased amperage within the system.  A 220V 12A motor would draw 24A from a 110V socket in order to achieve the same power output: P = IV.  If you cut the voltage in half, the amperage doubles for the same power output.

Most home circuits are designed for a maximum of 20A draw, many only 12A or 16A; any more than that and the breaker will go, or the wiring will overheat and pose a great fire hazard.  So, rather than increasing the amperage draw of a device for items that have high power requirements, like stoves, air conditioners, and dryers (fridges are usually 110V in North America), 220V circuits using both parts of the phase coming in from the mains, rather than half, are wired with special sockets for these high energy appliances.

Essentially, the appliance itself usually doesn't care, but the rest of the wiring does.

The real problem with motors on AC current is the number of cycles per second it's designed for.  Using a 220V European motor on a 220V North American circuit will cause problems, owing to the fact that the electricity in Europe runs at 50Hz rather than the 60Hz used in North America.  AC motors are typically phase driven by the cycle of the mains, and a 50Hz motor running on 60Hz power wouldn't draw enough power to make a full partial rotation on a single power cycle - it'd run faster than it's made to, in a similar stress state as a "starting" electric motor, wearing out the components much faster and perhaps outside of their tolerances.  On the other hand, a 60Hz motor on 50Hz mains would over run the mains cycle, slow down, and get a continual wasting of power with each cycle lost as heat in the system, which can also be dangerous.  Thats why the only way to transition between the two is to switch the power from AC rectified to continuous DC (not pulse DC - you'll need capacitors to regulate the DC output as continuous), then oscillated back to AC at the operating frequency.

TL;DR - It wouldn't blow up, but it can cause stress problems that manifest as heat, which can be bad.
While we're at it, Science, an ancillary question -- is there a solid definition of what exactly an "instinct" is, esp. in humans?  Can an evolutionary biologist talk to a psychologist or neuroscientist about an "instinct" and they'll all nod and know exactly what they're talking about?
Temperature is by definition a statistical description of a distribution of *many* particles. It makes no sense to talk about the temperature of a single atom. When you talk about a single atom or a single electron in an atom, you can definitely talk about the translational kinetic energy or the vibrational energy of a particle, but such energy discussions only relate to thermodynamic concepts when you have lots of particles present.
20/20 basically just means you can see what a 'normal person' should see at a range of 20 feet, from 20 feet away. It means you have normal eyesight. 

Perfect eyesight can be a lot better than 20/20. You can have 30/20 vision, meaning you can read what a normal person sees from 20 feet away, from 30 feet away, for instance. 

It's not uncommon for people who have had corrective eye surgery to come out of it with better than 20/20 vision.
In a universe where dark energy takes the form of a cosmological constant, things will *not* always fall toward one another. I've been linking to [this FAQ entry](_URL_0_) a lot recently, but it's relevant here as well. In such a universe, gravity eventually "turns over" and gives rise to *expansion* effects instead of attractive effects. As discussed in that entry, if you had a star the mass of the sun sitting out in a cosmic void, the effect of gravity would be to pull objects toward the star only if they were within about 200 light-years of it. Things further away than that will end up being carried away by expansion, which *is*, in this case, still an effect of gravity (relativistically understood).
We needed a standard, so it got set to something nice and even.  It's an arbitrary number.
This is not an absolute velocity. It is 600 km/hr with respect to the unique local reference frame in which the CMB is *isotropic* (i.e., is the same in all directions). Velocity is always relative.

Nothing about relativity is violated. Also, relativity does not say "a preferred reference frame does not exist". There are plenty of scenarios in which there is a preferred reference frame, and the CMB frame in cosmology is one such example. What relativity roughly says is that physics is the same in all locally inertial frames. So in that sense the CMB frame is not preferred.
The answer will to a large extent depend on which desert you are looking at.

Say you take a warm equatorial desert such as the Sahara. Revore the sand and you'll get to bedrock, with some residual patches of sandstone. The thing is that givent the local climate, the bedrock will have been chemically weathered, which translates by leaching and destructiuon of some elements and minerals, which will be partially to completely replaced by clays. Some of the leached elements may have precipitated further down within the water table, after crossing the oxydo-reduction boundary. ([see "Supergene enrichment"](_URL_0_)). The rock at the surface will thus be crumbly, soft and perplexingly "unrock-like" for the first several tens (perhaps even hundred) of meters, before you get into fresh unaltered rock.

In a polar desert, you will find little if any evidence of chemical weathering under the sand.

Furthermore, in either case, you will also be exhuming pre-existing paleo-topographic features such as valleys, canyons, tors, etc. ([example](_URL_1_))
As I understand it, this is mostly driven by the [Hadley Cell](_URL_0_), which is a global air circulation pattern. The warm, moist air at the equator rises in this cell and loses most of it's moisture due to condensation and rain so the air parcels that descend at the other end of the Hadley Cell (about 30 degrees north and south latitudes) are dry. If you look at a map of the globe, many of the worlds deserts, the Sahara included, are in this zone around 30 degrees.
There has been at least one study that has looked at programmers looking at code, and trying to figure out what it is doing, while in a fMRI machine. The study indicates that when looking at code and trying to figure out what to do, the programmers brains actually used similar sections to natural language, but more studies are needed to definitively determine if this is the case, in particular with more complex code. It seems like the sections used for math/ logic code were not actually used. Of course, that might change if one is actually writing a program vs reading the code, but...

Source

_URL_0_

_URL_1_

Speaking as a programmer, I believe the acts of writing and reading code are fundamentally different, and would likely activate different parts of the brain. But I'm not sure. Would be interesting to compare a programmer programming vs an author writing.
* [Wikipedia: Road Statistics](_URL_0_)
* [Wikipedia: List of countries by road network size](_URL_1_)
* [CIA World Factbook: Roadways](_URL_2_)

The last site is source for Wikipedia and has a data download link you can use to get the data into a spreadsheet.

Edit: I've just imported the raw data myself and the sum of the road lengths is 33,503,102 km (20.82 million miles). Which, according to [Wolfram|Alpha](_URL_3_), is 24 times the diameter of the Sun, or a quarter of the mean distance between the Earth and Sun. Interesting to see that it would take a photon 1.9 minutes to travel the equivalent distance in a vacuum, assuming traffic lights are all green.
Because you can not observe an electron (or any other object subject to quantum mechanical effects) without modifying its state. (eg, you have to bump a photon off of it)

But that being said, it technically applies to large objects too. The effects are simply diminishingly small because bouncing wee little photons off of a large object does not hinder it much.
Pathogens are extremely extremely diverse with many different disease progressions, mutation rates, and reservoirs. 

For example, human-only diseases like Polio and Measles are nearly eradicated due to vaccination. 

The flu (Influenza) is easily transmitted and is in constant evolutionary battle with the human immune system, and every new flu season a new strain has evolved to evade our immune systems. There also animal reservoirs that can give rise to other flu strains that can be completely foreign and deadly. 

HIV, is for the most part, completely incurable as its genome is permanently integrated into your infected cell genome, and have an extremely high mutation rate. Vaccines are not close in the near future. 

Additional improper usage of antibiotics or antiretroviral drugs will result in bacteria surviving at a sublethal dose of drugs and develop resistance.

Food poisoning is caused by many different pathogens as well.

There are many diseases that are commonly asymptomatic as well, though this doesnt answer your question.
No, but this guy does a good joerb.

_URL_0_
It would probably work.  But starting the donor car first means that power is available from both the donor's battery and alternator, and that the donor will have a chance to recharge the battery in case it is depleted by the jump start.  Starting a car engine takes a lot of power in a couple of seconds, and car batteries are damaged by being deeply depleted.
Standard (in-out) perturbation theory only tells us the probability of a certain asymptotic state at time T = -inf evolve to another state at time T = +inf. You could in principle try to track the time-evolution, but the machinery for that is much more complicated.
Short answer: no. The key point is that energy is leaving the burner/food system as well. The hotter the food/burner is, the faster it loses energy. To maintain a high temperature, then, you have to add energy in faster. So a "high" setting *does* have a higher equilibrium temperature than the "low" setting. You can verify this on an electric coil range by leaving the burner on at a given setting and observing the color the burner reaches. At "low" it stays how it looks at room temperature, emitting light in the infrared. At "medium" it looks a dull red, still emitting mostly in the infrared but with a tail of the light distribution reaching noticeably into visible wavelengths. At "high" it appears a bright orange, as the higher temperature gives a much stronger tail into visible wavelengths. The relevant physical laws are Wein's displacement law which essentially says "hotter things are bluer and colder things are redder" and Planck's law for thermal radiation, which describes the spectrum of light emitted by an object at some given temperature.
How are you going to pressurize the inside of a popcorn kernel with a bicycle pump?
Yes. There are "voice coaches" who train singers, TV and radio broadcasters, public speakers, etc.
The probability of finding an electron is only 1 when you integrate throughout all space. So that means for any finite volume, there will be a non-zero probability it'll be outside that volume.
a particular reason for why you're using exactly these units? You shouldn't work with numbers greater than 300 or smaller than 0.003 if you can choose the units. I'd rescale the units such that the sun, not the earth, has a mass of order unity, for reasons that will become obvious later.

Also, most importantly: you chose no unit for time! Or are you doing real time?

In any case, once you've fixed the in-game/IRL time ratio, I'd immediately rescale the other units such that `[; G = 1 ;]`. Don't experiment with values, you're *free* to fix this parameter to 1.

 > No matter what I randomly set the initial velocity as, the orbit either spirals toward the sun

that's wrong no matter what. Probably related to at least this

 > F(grav) = 100 N uum2 / uukg2 * (5.97 uukg * 1990000 uukg) / 150000 uum

there's a square missing (and therefore units don't match).

Also does UE allow you to apply directly accelerations instead of forces? (Unity3d does). If so, do switch to applying the gravitational acceleration. You avoid multiplying and diving back by a very small/very large number, you should win in accuracy or at least keep away from numerical errors. (Here lies the advantage in fixing the mass of the sun around 1. The acceleration of the sun is basically negligible, only that of the earth is the relevant one and in its formula only the mass of the sun appears, not that of the earth).
Just as a taxonomic note, the term "fire ant" refers to roughly 20 or so new world species. I assume, however, that you are specifically talking about the Red Imported Fire Ant (RIFA), *Solenopsis invicta.* Is that correct? If so, I can tell you of several reasons for their rampant expansion into new territories.
Wind flows from areas of high pressure to low pressure. The closer the high and low pressure areas are together, the stronger the "pressure gradient", and the stronger the winds will flow. Others will certainly be able to add to this.
When faced with a noxious stimulus you react faster than you register the sensation because the retraction is caused by a reflex. The activated nociceptor in the hand sends signals to the posterior horn of the via a sensory fiber. The nerve then synapses with an ipsilateral (same side of the spinal cord) motor neuron that exits the spinal cord via the anterior horn to your hand to withdraw from the stimulus. This is called the withdrawal reflex. The same sensory neuron also synapses with the motor neurons in the spine and contralateral body to stabilize the uninjured side (this is more notable when the noxious stimulus is in the lower limbs). This reflex is called crossed extension reflex. Both reflexes are carried by fast A group fibers, which are large, myelenated fibers and have the highest rate of conduction. 

Conversely, pain signals primarily travel up to the primary sensory cortex via a mix of A (delta) group and C group fibers. C group fibers are narrower, unmyelenated fibers in the lateral spinothalamic tract. The A fibers carry sharp pain, whereas the C fibers carry dull pain signals. Despite the hot water pain going on fast fibers it is still slower than the short reflex pathways.
Japanese Macaques have been observed dipping fruit in salt water between bites, implying a taste for the improved flavour. This was, however, an acquired behaviour, probably learnt from the researchers who used sweet potatoes to lure them. (Kawai M et al. (1965). Newly-acquired pre-cultural behavior of the natural troop of Japanese monkeys on Koshima islet.)

Leafcutter ants are known to use vegetation as a substrate for the Lepiotaceae fungus which is used to feed their larvae. (The ants themselves feed on leaf sap). 

Ambrosia beetles have also been observed practicing fungus-based agriculture.
It's not. There is not really any evidence it helps the immune system of humans.   
  
Vitamin D, otoh, may be the reason flu season is around the winter solstice.   
_URL_0_
Gravity is the warping of spacetime.  If we see spacetime that's distorted, that's the work of gravity.
When charging a battery that is also in use, the load is connected in parallel to the battery *and* the charging source. The source must be able to support both the needs to recharge the battery, and to run the load (since from the perspective of the charging source, all of it is a load). 

This is also what happens in a gas or diesel car when you have it running. The battery is used to store power while the vehicle is off or when only electrical accessories are used, and to operate the starter. But once the engine is started, the alternator (charging device) turns with the engine, regenerating lost electricity back into the system.
It varies depending on the pathogen. When it comes to syphilis in particular, the outer cell membrane has unusually few of its own antigens (proteins) in it, which makes it harder for the immune system to detect it. It can also vary its antigens by gene conversion.

That said, the immune system can combat syphilis. It's just usually not enough to clear the infection. When the bacteria lyse (split open), their internal components are easily recognized by the immune system - their camouflage is only skin-deep. This can actually be a problem when treating syphilis, as a bunch of bacteria dying at the same time can make the immune system overreact to the suddenly abundant foreign antigens etc.

STDs aren't necessarily anything special in terms of ability to evade the immune system. Lots of pathogens have found ways to do that. Mycobacterium tuberculosis, for example, has a coating that allows it to survive inside the phagolysosome of macrophages; the substances the macrophage pumps into the phagolysosome would kill almost any other bacterium.
Hail is formed by updrafting raindrops into frozen layers until the weight is too much to continue. This updrafting typically precedes tornados. 

Snow is just freezing atmospheric condensation that stays frozen if the ground temps are at or below freezing.
It'd be easy to define kilogram exactly by tying it to some natural constant. We could have easily done this decades (or centuries?) ago if we wanted to. The problem is that we can't measure those natural constants very accurately, but of course we can just define one to some exact value and be done with it. When we later make better measurements of the constant, then what we're actually measuring is our definition of kilogram, not the constant.

But ultimately what we really want is an extremely accurate way to compare masses. The current way of defining kilogram with a prototype object has been the most accurate way of doing it, despite being very arbitrary. The Wikipedia gives the accuracy of the prototype as 20 ppb (parts per billion). 

Some [other proposed methods](_URL_0_) are measuring mass with watt balance or counting carbon atoms. The watt balance method gets you to 37 ppb (bigger is worse of course) and carbon counting to about 50 ppb. So these are both still worse than the existing method. These numbers are off Wikipedia so they may not reflect the very latest developments but the point is that they definitely haven't been better methods for long if they are at all. (Check the wiki page, there are other proposed methods too.)

Furthermore, it's fairly easy to produce accurate replicas of the prototype mass. These can then be shipped all around the world and you can have accurate, maybe not to 20 ppb but still, measurements anywhere. A sufficiently accurate watt balance is not a simple thing to build and you wouldn't have them in every laboratory. So you need to also consider the practicality of the possible new definition. We want to make accurate measurements of mass all around the world.
It might be good to first talk about corners/angles when we just have straight lines, no curves. This should be fairly intuitive. In this context, a corner is a point that has two rays extending from it. There is a little ambiguity from this, since you could have one ray going out in one direction and another going out in the complete opposite direction, essentially making just a line. This shouldn't be an angle, so we should say that a corner is a point with two rays coming out of it in not-opposite directions. We can allow them to point in the exact same direction (so two rays superimposed on each other), but we'll call this a "cusp". To actually measure the "angle" of this corner, we can draw a unit circle around the corner and then measure the arclength of the arc on the circle given by these two rays. The value of this is the angle. (Technically, you can draw a circle of radius r, find the arclength of the arc on the circle s, and then the angle will be s/r. This makes it so that it is unitless, and doesn't depend on the circle you choose to draw.) In this way, a corner is a point with two rays coming from it that make an angle not equal to pi.

This is good for defining corners on polygons and such, but what about for more general curves? Ie, curves with curves?

The idea is to import this concept of "corner" into this more general setting. We need calculus to do this. Now, on a curve ever point has two possibilities: It is either an endpoint of the curve, or it has points on both sides of it, that are arbitrarily close to it. We'll call these "interior points". Endpoints don't really qualify as corners, so we can ignore them.  

If we have a curve and we fix any interior point P on it, then for any other point Q we can find the ray from P to Q. If our point Q gets closer and closer to P from one side, then the ray it makes with P might stabilize towards some fixed ray (ie, the rays get closer and closer to this ray as Q gets closer and closer to P). This can give us two rays corresponding to the point P: The stabilization ray from the right and the stabilization ray from the left. We'll call these the Left Tangent Ray at P and the Right Tangent Ray at P. What we want from our curve is for the Left Tangent Ray and Right Tangent Ray to exist at every single (interior) point on the curve (yes, [you can make curves where this fails)](_URL_2_)). We'll call these "Nice-ish Curves". You're in luck, because every curve you  have described is nice-ish, this was really just an excuse to introduce these tangent rays and to be extra clear on what kinds of curves this will work for.

If we then have a nice-ish curve, then at every point we have a pair of rays coming out of it. These rays then form some angle. If this angle is not equal to pi, then  we can say that the point is a "corner". If the angle is zero, then we can say that the point is a "cusp" (this is when they point in the same direction). When the rays make an angle of pi, then the line that they combine to make is the Tangent Line. You get a tangent line whenever the derivative is defined, and so you  don't have a corner.

Armed with this reasonable definition of a corner, we can answer your questions: A circle does not have any corners, at every point the rays make an angle of pi and you get a tangent line, rather than a pair of tangent rays. If you cut the circle in half, resulting in a half-circle shape, then you have two corners, one at each, well, corner. So this is a two-corner shape. The angle for both of these corners is pi/2. You can make a curve with one corner, just take the graph of f(x)=|x|, it has a corner at x=0 of angle pi/2. But that's like cheating, it goes off to infinity. But something like a rain-drop makes a corner, and you can even make it into a cusp if you want.

Here are some examples you can play with. They're functions instead of shapes, but the ideas transfer.

* [Here](_URL_3_) is a Desmos graph that you can play with in the case where there is exactly one corner at (0,0). You can move the secondary points around, and the corresponding rays will follow. Note that when you move the left/right secondary points towards (0,0), then they both approach different rays and that these rays make a right angle.

* [Here](_URL_1_) is a Desmos graph where you can play with it in the case where you have a cusp at the point (0,0). You can move the secondary points around to see the rays. Note that when you bring these secondary points together, then they both approach the same ray that points directly up.

* Finally, [here](_URL_0_) is a Desmos graph of a circle, where you can do the same thing as in the previous ones. You'll note that when the secondary points (points a, b) get close to the primary point (point c), then the corresponding rays get closer and closer to making a line. This is a tangent line and shows that there is no corner at that point.
Spinning a ring doesn't actually create gravity. If you are on the interior of a spinning ring in space, and you jump, the only thing that brings you back "down" is the fact that you have sideways inertia from the ring's spin. Because of this inertia, your jump doesn't take you toward the center, it takes you to the edge of the ring a few feet over, and the ring spins to meet you there.

If someone who was not affected by the ring's motion were to place an object in space in the middle of the ring, or even very close to the ring, but not touching, it would simply hover. It wouldn't "fall" because there is no force acting on it.
Yes. A standard magnet you encounter is specifically called a ferromagnet, and operates via organized magnetic domains that are locked into place buy the stiffness of the material. A ferromagnet has a special parameter called the Curie temperature, at which the magnetic domains can freely rotate. If no external field is applied, at or above the Curie temperature a ferromagnet will randomize. If an external field is applied at or above the Curie temperature, you can apply an external field in whatever direction you want, and lock that in by cooling the ferromagnet with the field in place.

You can also demagnetize a ferromagnet by physically impacting it or cold working it, which causes the metallic grain boundaries to flow or rotate and will increase randomness of magnetic domains.
That's the regenerative cooling system.

The burned gas flowing through the nozzle is hot enough to melt it - in some rockets even higher than the boiling point of iron. Cooling the nozzle is a major engineering challenge.

The three cooling techniques are radiative cooling (design the nozzle to emit infrared light), ablative cooling (cover the inner walls of the nozzle with a material that evaporates in a controlled way and absorbs heat, like PICA), and regenerative, the one we're talking about here. (This is in addition to a layer of relatively cooler gas flowing close to the nozzle walls while the hotter gas is in the inside.)

The main flow of fuel goes into the combustion chamber, but a small fraction is sent through those small pipes. They carry unburned fuel all around the nozzle. As the hot gas flows, it transfers heat to the metallic body, then the fuel in the pipes absorbs that heat effectively cooling down the metal. This heats up the fuel, but its not a problem because it's flowing, so more cold fuel comes soon. What to do with the fuel after it has flowed through the cooling pipes depends on each particular design, but it's usually fed back into the combustion chamber. Things can get a bit more complicated when the engine has preburners and turbopumps, but that's probably beyond the scope of your question.

_URL_1_

_URL_0_
What you are looking for is a discussion of Nucleation.

_URL_1_

Read the mechanics section.

For Nucleation to occur (and thus phase change) the change in microstructure must be energetically favorable with respect to Gibbs Free Energy.  Adequate deltaG is achieved at specific temperature/pressure/mixture combinations (phase transition temperature as an example) such that it is energetically favorable to transition from one state to another.  

Take boiling for example, as you heat the water up, before boiling the minimum free energy is achieved by remaining as a liquid.  If, hypothetically, a steam bubble attempted to form it would require a certain amount of energy to create the new vapor/fluid interface, but since this takes more energy than is available the vapor bubble will not form (or if it did such as in nucleate boiling it will rapidly redissolve into solution since there is not enough energy to continue growth).

As the water begins boiling sufficient energy has been added such that the difference in energy between vapor and water begins to energetically favor forming vapor nucleates.  The link above describes this in the discussion of critical radius.  Up to a certain nucleate size, it requires more energy to form a nucleate than is saved by the transformation, and the small nucleate will either agglomerate or redissolve.

As to the temperature remaining constant during phase change, this occurs because the added energy goes into breaking apart the inter-molecular bonds required to establish the new phase's ideal intermolecular spacing, vice increasing the average molecular kinetic energy (temperature).

Tldr; there is a set amount of energy surplus/deficit required to cause one state to be energetically favorable than another thus causing phase transformation.  Until this energy surplus/deficit is achieved phase transformation will not stably occur.

Your second question has to do with fluid sheer mechanics and is not related to phase transformations.  As I am not a fluids expert, I will not attempt an explanation of this phenomenon, [but here is some light reading regarding your second question](_URL_0_).  I will let a fluids dynamacist (if one sees this question thread) go into more detail.
Electrons are affected by gravity. In an electron beam, they're just going fast enough that the curve typically isn't very noticeable over short distances.
1.) The important thing in limits is that as "whatever you input into the function" approaches a point of interest, the output approaches some value. In 1D, you can only input a single number, (x), into your function and you can only approach a point of interest from either the positive or negative side. In 2D, your inputs are pairs of numbers (x,y). So even though you are putting in two numbers, your putting in one point. As this point gets closer to the point of interest, say (a,b), then the value of the output will approach the limit. You can approach this point in many different ways, but the important thing is the distance that you are from (a,b). Don't think of it as two things approaching two separate things, think of it as single 2D thing approaching one other 2D point.

2.) [Here](_URL_0_) is a good discussion for limits of indeterminate forms in two variables.
The same effect that causes [mirages](_URL_0_) is at play. Basically cold air is denser than hot air, which in turn causes it to have a higher refractive index, causing light two take different paths depending on the temperature of the air. If there are pronounced changes in the temperature within your field of vision, different optical phenomena will arise. In this case, because the air moves around (e.g. through convection), the image you see when looking through the air will appear hazy and you may see riplles.
Hopping is actually the most efficient, particularly for kangaroos. 

But running is more versatile and requires less specialised muscle and tendons.

Although in the case of frogs, two legs can jump further than one.
And in the case of rabbits they are set up for high speed running, which means they still hop at slow speeds.
Most four legged animals sprint in the same motion as rabbits though, with two back legs and two front legs working in sync to make a galloping motion regardless of whether they walk or hop at low speeds.
Thats the crux man, to ensure better survival of your offspring and your family it is better to be a tight unit, thus ensuring the genes get passed on. If you just dump em off and bail then you better have loads, or have super self sufficient offspring right away.
Orville and Wilber built one of those.  They called it a "wind tunnel."  I hear NASA has a nice one.

Seriously, there would be many easy ways to do this, including one that was purely solar powered.  Build a dark tower, maybe even with mirrors to reflect a bit of sunlight on it.  Build a tunnel that connects one end with the tower.  When the tower heats up, it will warm the air inside.  Warm air rises, pulling cooler air from the end of the tunnel.

No need to "capture" the wind, the tower will *create* wind inside the tunnel.
A gif of it in action came through not too long ago, but I can't find it at the moment. Basically the line on the end of the banner is strung up between two posts and the plane swoops in and grabs it with a hook. Then the banner takes off much like a kite.
In a few different ways. Let me use the concept of phenol for example. Phenol is benzene with an alcohol functional group. Oxygen can use inductive donation to increase the effective concentration of electrons (or it changes the electron cloud to be more dense) in the molecule. This can be useful for chemical bonding (like coordination chemistry) or effect its solubility (phenol is more soluble than benzene).
There are objects for which the particle and antiparticle are the same type of object.  For example, the antiparticle of a photon is a photon, and the antiparticle of a Higgs boson is a Higgs boson.

There are, as you say, other particles in which the particle and antiparticle are not the same: the W^+ and W^-, the electron and the positron, and so forth.  Which we call matter and which we call anti-matter is, of course, a matter of convention.
The duration of a virus's incubation time is unique to itself.  You can't predict an incubation time based on what species the virus came from or where it originated.  It also depends on if the virus has tissue tropism or not.  As virus particles gain entry to the body, they may set up quickly or slowly.  By Infecting cells immediately, the virus is allowed propagate and spread throughout the body, ultimately leading to viremia.  However, if this happens too quickly, the body's immune defenses will wake up and mount a massive counter-attack.  It's advantageous for viruses to try to propagate early without causing too much damage or showing clinical signs - thus the body isn't aware of the virus' presence until it's too late.
This is not a contradiction. The entropy is always increasing in an isolated system, but the TS diagram describes a system open to heat/mass transfer and work. Take the [TS diagram](_URL_0_) of a Carnot energy for simplicity. For a given rate of heat transfer Q > 0 from source at T1 to sink at T2, the total change in entropy is dS_total = dS_source + dS_sink = -Q/T1 + Q/T2  >  0.
Since "better" is ambiguous and relative, there can be many answers to this question.

Many reasons come to mind:

The type of isotope - and more specifically, the gyromagnetic ratio - affects the signal one may gain from NMR. For example, protons are very common spin-1/2 nuclei that give excellent signal in NMR. Carbon-13, on the other hand, besides having only about 1 percent natural abundance, gives signals approximately 1/4 that of equal number of protons.

The aforementioned isotopic abundance also leads to some problems - carbon-13 NMR can often be done without enrichment, but when you get to nitrogen-15 or oxygen-17, the natural abundance is so low you'll have to enrich your sample.

If you venture beyond spin-1/2 nuclei, you start getting quadrupolar interactions - this drastically reduces the available signal as you now have an additional mode of relaxation.

Gaseous compounds are often too dilute to give a strong NMR signal.

Viscous solvents can limit molecular tumbling, thus lead to line-broadening. The same effect can be seen in molecules with large rotational correlation times (i.e., tumbles very slowly).
There are things called flood basalts. Terrifying landscapes of fiery hell. Thankfully rare and infrequent, but sometimes Earth's innards come up to stick around a while. I'm from Oregon and we got some pretty cool landscapes from it, but it might've F'd up the world a couple times. Lookup Siberian Traps..
Follow up question:

At the same task, being harder for the IQ 75 guy, will he burn more calories for the same problem because he has to think harder?

And if so, is it mostly a matter of time? Such as, he takes 30min for a mathproblem the IQ150 guy solves in 3min.
Theoretically yes.

However the speed of a jet (mach 2-3) is minuscule compared to the speed at which radio waves travel, which is the speed of light. Hence the Doppler effect will be negligible, hardly noticeable at all.
[This question was asked elsewhere](_URL_0_). The answers given come down to the heat of vaporization. Basically, it takes a lot of energy for water to change phase from liquid to gas. Conversely, changing from gas to liquid gives off a significant amount of energy. When your body absorbs the energy (has to go somewhere) from the steam, it first changes into 100C water and then it will try to equalize temperature with the surrounding air and tissue exposed to it. So steam will burn you worse (lb per lb) than water due to the release of energy from the phase change.
The words "tired", "sleepy", and "fatigued" are used differently in different fields. In sleep science, they are largely interchangeable. The term "sleepiness" is used in the context of "objective sleepiness" (literally how long it takes an individual to fall asleep, as measured by a multiple sleep latency test) and "subjective sleepiness" (an individual's rating of their own sleepiness, e.g., on the Karolinska Sleepiness Scale, ranging from "Very Alert" to "Very Sleepy"). The term "tiredness" is less common, but also associated with subjective ratings (e.g., on a commonly used scale of fatigue, ranging from "Fresh as a Daisy" to "Tired to Death"). The term "fatigue" is more widely used, applying to both subjective ratings of fatigue and objective ratings of declines in neurobehavioral performance (e.g., impaired performance on cognitive tasks).

These subjective and objective ratings are all associated with the processes that regulate sleep: the circadian rhythm and the sleep homeostatic process. The circadian rhythm is an approximately 24-h cycle in sleepiness/fatigue/tiredness that is endogenously generated. The sleep homeostatic process is the process that results in increased sleepiness/fatigue/tiredness the longer one is awake. The biological basis for this process is presently thought to be the accumulation of sleep-regulatory substances in the brain, including adenosine, nitric oxide, and some cytokines. Since cytokines are immune signaling molecules, we now know that the tiredness associated with illness or chronic inflammation probably involves the same biochemical pathways as those involved in generating sleepiness under healthy conditions.

Muscular fatigue (e.g., due to exercise) is a different phenomenon, involving different biochemical pathways from those regulating sleep and wake. I don't know enough about muscular fatigue to comment on it in depth.
The latter. The purpose of the renin-angiotensin-aldosterone system is essentially to control blood pressure, blood volume, and sodium balance. Like you mentioned, reductions in sodium stimulates renin release. However, that is just one of multiple renin-release stimuli. The more correct way to see it is: (1) decreased circulating volume (low BP)- >  (2) renin release- >  (3) increased sodium reabsorption/decreased sodium excretion- >  (4)BP correction

The issue here is high blood pressure (aka hypertension; HTN). Water follows salt, so high sodium diets will have lots of salt in the blood vessels, which promotes water movement into blood vessels, thereby increases wall tension and inhibiting renin release. *For those w HTN, a reduction in sodium is unlikely to increase renin as the HTN is often due to high blood sodium and HTN inhibits renin release*
Not an expert here, but I do know that women have more estrogen, whichncauses more fat to be stored at the surface of the skin.  I am uncertain how this would affect thermo regulation, but I think it has to do with the skin not coming into contact with enough blood. 

Hope that helps
You can think of your vocal cords as 2 "batwings" that are normally tucked away on either side of your trachea. When you want to talk, you unfold them so that they stick out from the sides a bit, but with enough space in between so that as air rushes past them they vibrate and generate a tone. You can stretch them tighter to make a higher pitch or loosen them to make a lower pitch, and you can change the shape of your mouth and lips in order to form words.

In the case of a single cord being neatly removed by a surgeon, the remaining cord is still functioning normally so it participates in making sound as usual. However, the other cord is missing, which means that you're only generating 1/2 as much power (volume). You're also left with a rather large space for the air to escape through now (the space where the cord used to be) which gives the voice a breathy, airy, or whispery quality.
Basically it creates two different levels of drag on the ball. The shiny side is low drag and the rough side (that they haven't polished) is high drag, relatively speaking.

This means that when they bowl the ball (I'm going to go with a fastball, it's easier to visualise) the seam will be running parallel to the pitch lengthways, with the two sides facing perpendicular, the rough side will have high drag, and low airflow, and the shiny side will have low drag and high airflow.

high airflow means simply 'low pressure' low airflow means 'high pressure' so the ball is effectively pushed to the shiny side. Meaning if the shiny side is on the right the ball hooks right

However, when the ball is newer and the seam is quite prominent that is the main tool used to create swing so this is less of a factor
Finding associations between genes and traits/diseases is very difficult. Having amazing genetic information is only part of the battle. 

Detecting gene/trait associations with SNPs is done with a process known as [Genome Wide Association Studies](_URL_0_) (GWAS). Typically in a GWAS you genotype individuals in two groups: those who have the trait/disease you are studying, and those who do not (controls). 

If researchers had access to all human genotypes, this would only potentially help increase the pool of control individuals. Even then, the results of a GWAS can be influenced by the ethnicity and environment of the subjects, so at minimum some demographic information would also have to be collected. 

Some DNA-testing sites, like 23andMe, have user-surveys where they can provide trait and disease information. Scientists at 23andMe has been able to use this to find genes associated with some traits that would not normally be studied by the medical community, such as the a gene that [apparently causes some people to taste soap when they eat cilantro](_URL_1_).

For more medically relevant traits, most researchers doing GWAS would probably prefer to collect their own data, rather than adding Promethease results to their existing data, for quality control purposes. Besides, collecting the trait data for complex diseases should involve more than a survey conducted over the internet!

Sites like Promethease (and 23andMe itself before its current battle with the FDA about reporting disease associations) are useful for knowing how your own genotype lines up with GWAS results from peer-reviewed studies.

I'll give the final word to Promethease itself, which makes you agree to the following terms when signing up:

 >  I realize that most published reports about DNA variations explain only a small part of the heritability of a trait, and they also don't take into account how different variants might interact. In addition, published reports typically ignore environmental, dietary, microbial, medical history and lifestyle factors, any or all of which may well affect my true risk for any trait or disease.
Recycling some materials does take more effort than other materials, but overall the energy you expend recycling something requires less energy than producing it from raw materials.  [Here's](_URL_0_) a good article from the Economist that discusses the vice and virtue of recycling.
Gravitational time dilation happens near any object, no matter how big or small or how heavy or light. However, the strength of the effect depends on the strength of the force of gravity. Outside a star, the effect is relatively small. But when you turn that star into a black hole, it becomes much more dense and compact and you can get much closer to the center of mass without actually entering the object (or in the case of the black hole, it's Schwarzschild radius).

If you would stay at the same distance from the center of mass, the gravity, and therefore the effect of time dilation, of a black hole would not be any different than that of a star with the same mass. It is only because a black hole is so much smaller than an equal mass star that you're able to get close enough to experience the effects of strong gravitational forces.
Do you mean do animal's joints pop or do animals deliberately pop their joints like people do?

It has only just recently been demonstrated that joints crack when a gas bubble suddenly forms inside the joint.  As the joint is pulled apart it causes rapid depressurisation of the synovial fluid inside the joint, at a critical tipping point this low pressure "suction" suddenly cause gases in the synovial fluid to dissolve out of solution and form a bubble. This near instantaneous bubble formation is accompanied by the cracking sound.

_URL_0_

Broadly speaking equivalent mammalian joints have the same architecture from animal to animal. So we would expect in most cases that any joints we can crack will also be able to crack in animals, especially simililarly sized animals. I'm not aware of any actual research pulling animal's appendages until they crack, I guess it would regarded as rather cruel and unpleasant so I doubt it's been done. 

With regards the latter, I'm not aware of any research around whether other animals deliberately pop their joints. I assume most apes are capable of doing this but whether they've worked out how do it and whether they also enjoy it remains to be seen.
Everyone has blue eyes. For a lot of people the blue is covered with a layer of melanin makes them brown. 

The consequences haven't been fully evaluated yet to my knowledge, but I member concern expressed at the time that small particles of melanin would be adrift in the eye ball and could obscure or cloud vision at a later date.

Edit: there is [another article about it here](_URL_1_) , and the company offering it is called [Stroma Medical](_URL_0_).

They state:

 > "The Strōma procedure has only undergone limited study in humans, and no adverse events have been reported to date.  Before the procedure can be declared safe, however, it will have to undergo extensive additional testing and satisfy the requirements of multiple regulatory bodies.  In our next study phase, we plan to treat about 20 patients in our initial pilot clinical study.  Following the successful completion of that pilot study, we will treat about 100 patients in multiple countries and follow them for a predetermined length of time.  We will not release the Strōma laser unless and until we and the governing regulatory bodies are satisfied that the procedure is safe and effective."
No; averaging over random motion of particles in a solution, a magnetic field produces no net force on ions. If you took the entire container and moved it quickly enough through the magnetic field, you'd be able to separate the ions, but in the container's frame, you're just inducing an electric field in it.
Yes, via damage to the suprachiasmatic nucleus (SCN), which receives input from a special subset of light-sensitive cells in the eye and uses that input (among other cues) to keep its internal "clock" aligned with the day/night cycle. 

Animals with lesions to the SCN will still engage in sleep and other cyclic behaviors, but they will not do so in a regular, organized way. [Diagram!](_URL_0_)
Around 7000 -- 10,000 years ago, there was a mutation in people who lived in northern Europe.  The mutation changed two DNA bases in the LPH gene on chromosome 2.  In people with the unmutated gene, an enzyme called lactase is produced in infants, but the stops being produced in adults.  In people with the mutated gene, the enzyme is produced throughout their lifetime.

The enzyme lactase is responsible for breaking down lactose, the sugar found in milk.  So adults without the mutated gene (including all of our ancestors from more than 10,000 years ago) are lactose intolerant and cannot process milk.

The mutation became widespread in humans right around the time that people started domesticating animals, and only in regions where dairy animals were common.  We think the people with the mutated gene had an evolutionary advantage over the people without it, as they had access to an additional food source.

I don't know whether you'd be willing to call lactose tolerance the "next step in evolution", but it's a fun example of how human behavior (domesticating animals) directed evolutionary change.

Sources:

[_URL_1_](_URL_1_)

[_URL_0_](_URL_0_)
Curiosity is significantly heavier than the others.

 > "With a payload this size, the rockets could kick up enough dust to compromise the rover and its instruments," explains Sell. "And the rockets could excavate craters Curiosity would have to avoid as it drives away. Add to that the risk of a big, heavy vehicle driving down off the lander via an exit ramp to reach the surface."
Pathfinder, Spirit, and Opportunity used airbags to eliminate these concerns. But Curiosity is too large for airbags.
"Bags big enough to soften its landing would be too heavy or too costly to launch. Besides, you'd have to drop the payload so slowly for the bags to survive the load, you may as well place the rover right on its wheels."

_URL_0_
This is the [coupon collector's problem](_URL_0_). The expected number of draws to collect N cards is approximately N\*ln(N).
As far as i understand it, no creatures have both as it is simply not required. One of the basic functions of a skeleton (exo or endo) is simply to provide structural support for the creature, so it can move. If it has a exoskeleton it has no need of an endoskeleton, and vice versa. 

However i'm not sure if you would classify a creature which has an endoskeleton, but also has some kind of bonelike armour (not connected to the endoskeleton) under this category.
There isn't any fusion that's occurring in a neutron star - unlike in a star (where the star doesn't collapse because of the fusion occurring at the core), it doesn't collapse due to the degeneracy pressure from Pauli Exclusion.

If it *does* collect more material (this happens quite frequently, such as in binary systems), the material will be pulled in and crushed into neutrons. This is because the gravitation pulls everything together so tightly that the electrons and protons in atoms are forced to fuse together; thus, fusion won't restart.

If enough material *is* accreted by the neutron star so that the gravity overcomes the degeneracy pressure, then it will just collapse into a black hole.
Just to add to what Criticalist has said, this is a statistical phenomenon, and not something that left-handers should worry about.  However, while I'm certainly not an epidemiologist and really can't speak to the methods of measurement of this statistic, I think there are several explanations that could account for this difference.  In *some* cases  the reason for the switch in hand dominance is because something led the brain to reorganize itself (i.e., meaning that during gestation or early development something happened to the brain that caused certain networks of neurons to form differently than they would have otherwise).  The reason this is relevant is that the cause (i.e., physical trauma, teratrogenic medications, birth defect, etc) of the reorganization could have led to other neurological or physical problems that might otherwise shorten a person's lifespan or predispose them to accidents.  I haven't read through the articles provided by Criticalist, but I would argue that an accurate study on this topic would need to control for medical/neurological factors that could skew the statistics and that without controlling for those things you can't say that the simple state of "being left-handed" is associated with a shortened life span.
Those are just maps of the whole sky. You could put it in any [map projection](_URL_0_) you wanted, but astronomers tend to prefer equal-area projections like the Mollweide.

You can also [find them](_URL_1_) in rectangular maps.
All parts of your body undergo mechanical and chemical degradation over time. Regular upkeep is important and occurs not only in muscles but in all other tissues.
It's a super complicated question that doesn't really have an answer.  [This video](_URL_0_) discusses a lot of the factors involved (resources, space, life expectancy, etc.).  But essentially your guess is as good as mine.  We anticipate at least another 3 billion people, and estimates I've seen (in google) have ranged from 10 billion to 1 trillion (though these have been called extremely political, and not scientific).
The heat from re-entry is the heat of all the kinetic energy from orbit being expended, you're slowing down from well over 10,000km/h to around 300. In other words, it's the energy you originally imbued the spacecraft with by accelerating it to those speeds in the first place. There is no net gain to be had, only a loss due to inefficiency.
Based on the GOCE satellite, gravity appears weakest in Southern India.

_URL_0_

That axis is mislabeled though, the units should be milligal (1 gal=1 cm/s/s).
Mostly improved lymphatic drainage. But also mechanoreceptor activation that leads to lowered muscle tone/spasm and reduced sensitivity to pain.
Life has been widespread across the planet since shortly after it developed. (Shortly in geologic terms - a few tens of millions of years.)

Unfortunately it would be really tough to link to a good source, because it's sort of a known truth in geology.

Before the Cambrian explosion of animal life, which brought forth the various hard-shelled animals that are common in the fossil record, I don't know if we'd be able to answer your question with certainty. There's not much of a fossil record before that - we have some trace fossils (fossils of activity) from the Edicarian period immediatyeely preceding the Cambrian, as well as a few rare direct fossils of soft-bodied proto-animals. Before that it gets even more sparse. If there were some brief period ~800 million years ago when the equator were not inhabited by life, I don't know if we'd be able to figure that out, given modern techniques. 

After the Cambrian explosion, we have uninterrupted direct evidence of life in all equatorial biomes.
The prokaryotic respiration mechanism is glycolysis and it is the first step in eukaryotic respiration mechanism (which is oxidative phosphorylation). In both cases, glycolysis occurs in cytosol; in eukaryotes, the byproducts of glycolysis then go on to further react in mitochondria.
Modelling the rate of sea level rise beyond the present century is somewhat tricky as there's still a lot we don't understand about the mechanics of the melting ice sheets. 

We can ignore glaciers - in terms of large sea level rise we're talking about Greenland (equivalent to 7m sea level rise), West Antarctica (6m) and East Antarctica (50m). 

[Looking at the past](_URL_0_) about 2/3 of ice retreat events went to maximum extent in 400 years and 95% in 1100 years. Events that started with a similar amount of ice to what we have now reached maximum rate of sea level rise of between 1 and 2m per century. Continued extreme warming could potentially increase this - the fastest observed in the geological record is about 5.5m per century.
This is an excellent question. I am not an arachnologist, but I will apply knowledge of molecular biology, ecology, and toxicology to your question with the caveat that I may stand corrected by someone with more specialized knowledge than myself. 

First, we should consider how probable this interaction would be in nature.  Generally, immunity to toxins evolves under selective pressure, meaning organisms that frequently encounter certain toxins are the ones most likely to evolve immunity.  I'm not sure as to the overlap in range of these two species, but let's assume they do overlap.  Given the eponymous reclusiveness of the brown recluse and shy nature of black widows, it's my assumption that these spiders would rather avoid each other than risk a confrontation.  

Probability of the interaction aside, let's consider biochemical feasibility.  Most venoms are a complex mixtures of peptide (protein) toxins that act collectively to aid in the demise of prey.  While each venom cocktail has certain components that are most damaging, each specific component usually has some, however small, role to play. (search "curare" for an example of an ingenious anthropogenic "venom" where each component contributes to the overall effect.)

Let's discuss two consequences of venoms being proteins.  Firstly, they must be injected into circulation, as their molecular structures are too large to simply diffuse into an organism from its surface or diffuse between cells within an organism.  This is also why venomous creatures keep their venom cordoned off from the rest of the body in a special venom gland which means they don't necessarily need immunity to their own venom (contrasted with "poisonous" creatures, which are systemically toxic and therefore must be altogether immune to their poison).  Therefore, there is little selective pressure for a given species to be immune to its own venom unless there is a high occurrence of intraspecific envenomation (fighting within a species). 

Widow spiders are Latrodectus spp., and the primary component of their venom is latrotoxin, which is a neurotoxin that causes neurons to leak acetylcholine, GABA, and other neurotransmitters.  This increases the frequency with which certain neurons "fire", resulting in deleterious effects.  In humans, the syndrome caused by widow envenomation is called latrodectism.

The brown recluse spider is Loxosceles reclusa.  Less is known about its venom, but the most active component seems to be sphingomyelinase D, an enzyme that degrades certain elements of cellular structure.  While there is still some debate over whether or not it's necrotic (kills/dissolves tissue), we know that it causes hemolytic anemia (it bursts erythrocytes (red blood cells) causing a lack of oxygen-carrying capacity).  In humans, the syndrome caused by brown recluse envenomation is called  loxoscelism.

So given:

There is little selective pressure for a venomous species to be immune to even its OWN venom,

and,

The differing modes of action (neurotoxic vs. hemotoxic) of the venoms of the two species, 

my supposition is that both species would be susceptible to the venom of the other, given that their respective behavioral and geographic characteristics coincide to produce such an interaction.

TL;DR: yes, probably, but MANY factors are at play.
Assuming you want to keep the moon at it's current distance, rather than bring it closer so it's in a natural geostationary orbit, you'd need to accelerate it so that it performs one orbit every 24 hours, rather than every 27 days.

Obviously, the Earth's gravity won't be nearly strong enough to hold such a super-fast moon in circular orbit, so our tether will need to have enormous tensile strength to stop the moon from flying away. But how strong?

Acceleration due to circular motion is given by a=v^2 /r. Currently, the acceleration of the moon is

(1.02 km/s)^2 / 385000 km = 0.00271 m/s^2

This is equal to the acceleration due to the Earth's gravity. The force that the Earth's gravity exerts on the moon is therefore the moon's mass times this acceleration (F=ma):

F = 0.00271 m/s^2 * 7.34*10^22 kg = 1.99*10^20 N

If we want v to increase by a factor of 27, the acceleration (and therefore the force) will increase by a factor of 27^2 = 729. So the tensile strength of the cable will need to be 728 times stronger than the current weight of the moon.

That's 1.45*10^23 N.

That's huge.

If we wanted to make our tether out of steel, which has a tensile strength of about [250 MN/m^2 ](_URL_0_), it would need a cross section of 6 billion square kilometres, which would make it about 6 times wider than the Earth.
There isn't going to be a way to parameterize a general space. In fact, there won't be global coordinates on any space with nontrivial topology. However, for a Riemannian manifold (a setting where you might do geometry), there is a concept of [normal coordinates](_URL_0_) which apply locally in a neighborhood of any point. This is actually what you have found for the sphere. (These coordinates aren't global because there is a singularity on the point opposite your origin. This may or may not bother you, but it means that point doesn't have a unique representation.) A hyperbolic space has the topology of R^(n) so it does have global coordinates. I'm not sure off the top of my head whether the normal coordinates (which agree with the geometry) are global.
Well it doesn't necessarily start at zero. Only gradients of potentials are observable, so potentials are defined up to a constant. A point of reference if you will. However, your potential starting at zero is just a very useful choice!

Now why does it become infinity negative? Well let's say we have a very small mass point particle at infinity, not moving (so its energy is zero), and some other very massive point object that the first will be attracted to. As it starts moving, its kinetic energy will increase, but its total energy has to remain zero because energy is conserved. So the gravitational has to be negative for it to balance out. As the particle gets close to the large mass, its kinetic energy growth without bound (becomes infinitely positive), so its gravitational potential energy must become infinitely negative.
I think there's two ways of looking at this:

1. In both methods, you're already right or wrong despite *when* you wrote down what you expected to happen because the laws of classical physics are sternly defined and are never deviated from so, even though you haven't rolled the die yet as in method 1, it's already predetermined how you'll roll it, where it'll land, what number it'll land on, etc, because you exist within this physical world therefore your actions cannot deviate from these laws. With this kind of thinking, it's already been determined what number the die will fall on, it just hasn't happened yet.

2. I don't know much about quantum mechanics but from what I do know from a few courses in college and late-night wikipedia perusing, there is a distinct difference in methods 1 and 2. Quantum mechanics says that, until an outcome from a set of potential outcomes has been picked, particles, the universe, whatever, exists in a super-position whereby all possibilities exist and only when the time comes is a particular possibility collapsed upon for you to see. With this way of thinking, I really can't give you a good answer for your question. I fully see what you're getting at as I've pondered similar thoughts myself - and the result, for me personally, has always been to stick with thinking method 1.

I know I'm not qualified to answer this but I thought I'd give some sort of response since this question hits close to home for me.
So many variables. Do the kittens between the litters have the same father? Do the kittens in the same litter have the same father?

Cats are capable of superfecundation and superfetation, which means that kittens in the same litter can have different dads:

"A litter of kittens born to a single mother can have multiple fathers (superfecundation) and cats can become pregnant while already pregnant (superfetation). In such cases, the second litter may be born prematurely along with the first litter (which can be fatal for them), or they may hang on and be born at the normal time, which means that the mom will have extra kittens to deal with while still nursing the first litter, though feline moms usually do a good job of caring for all their kittens."

_URL_0_

If they've all got the same father between litters or within a litter, they're like full-blood siblings. Different fathers, and they're half-siblings. In a litter, it's also possible to have identical twins (or triplets etc), which could be demonstrated with a DNA test.
Wetness depends on two things - the liquid *and* the surface you're trying to wet. 

You can get an idea of how wet a liquid is by just considering its own surface energy alone. In this case the wettest liquids are those that have very low surface energies i.e. comparably they more readily spread across surfaces increasing their own surface area. Common low surface energy liquids are typically carbon based solvents e.g. diethyl ether = 17 mN/m. Compared this to water = 73 mN/m. The wettest liquid, however, would be given to liquid helium which at -273 K has a negligible surface free energy. This, combined with it's negligible viscosity, produces some [extraordinary super fluid phenomenon](_URL_0_).

However, for more common situations consideration of the surface energy of the surface you are wetting is as important. In this case matching of the types of intermolecular interaction so as to decrease the interfacial tension between the surface and the liquid define the wettest liquids. For example, water being polar readily wets the polar surface of glass. On the other hand, diethyl ether having its intermolecular forces almost entirely comprised of dispersion interations doesn't wet the polar glass surface as well.
Here's the thing: it doesn't matter what is going to happen outside the Local Group. We're stuck here, forever. Assuming the Big Rip is bunk (which is a good thing to assume), we will drop out of causal contact with the redshifted universe, with Coma, Fornax, Hercules, even Virgo (clusters, not constellations). It's going to be us, Andromeda, and Triangulum; well, the things that were those three but have now collided. Luckily, we're well enough Virialized that we're in it together, but the universe has conspired to rip the rest of the universe away from us.

So, we have the Local Group. That's all the universe we can see. Scary thought, isn't it? Well, if we weren't all long since dead, I suppose. In the next 50 - 100 billion years (~5-10x the age of the universe), star formation will come to a halt as all the Hydrogen gets used up. Small stars will continue burning until about a trillion years into the universe, at which they, too, will shut down. We will be left with black holes, neutron stars, white dwarfs, brown dwarfs, planets, comets, and asteroids. Asteroids are fairly iron-y, so we won't count them. Comets are dirty snowballs, but they're not massive, so we can ignore them. Black holes aren't iron, but they aren't really anything else. Same with neutron stars. White dwarfs and brown dwarfs, however, aren't iron, and they should escape the eventual turn-off of star formation. Same with planets.

Mass-wise (which is how astronomy is conducted, really), the universe will not be iron at all; rather, it will be black holes and neutrons. Of the elemental baryonic matter, you'll get mainly oxygen and carbon from where stars weren't able to get enough energy to go all the way.

And, yes, after the stars turn off, the universe will go black. The only light will come from collisions.
When placed in a stressful situation, e.g. the cutting open of one's hand, your body has a fight-or-flight response. This is the term for a combination of several effects including increased heart rate, slowing of digestion, etc. These effects are the result of an increased presence of a group of neurotransmitters which include epinephrine (adrenaline), norepinephrine, and dopamine. This fight-or-flight response is evolutionary advantageous in several ways, one of them being a dulled pain response. 

As to the reason for the decreased sensation of pain, I believe it has to do with the initiation of the response as a whole. First there is a stressful stimulus. Then your body begins to take action. 

Increased oxygen intake is necessary for greater muscle action? Increase heart rate, increase respiratory rate, dilate muscular blood vessels, and divert blood from non-essential systems (digestion). Epinephrine accomplishes many of these things; however, to form epinephrine, norepinephrine must first be formed. Norepinephrine is formed from dopamine. Dopamine is thus a precursor to epinephrine (cool/related graphic below!). Dopamine is also shown to deal with pain response and thus the production of epinephrine may contribute to the decreased sensation of pain.

The main reason for this phenomenon, however, most likely lies with the release of endorphins. Endorphins, like the neurotransmitter above, are released during stressful situations (it's also responsible for orgasms feeling, well, like orgasms). Their primary effect is the overproduction of dopamine which as mentioned results in a decreased perception of pain and a general 'good feeling'. 

tl;dr - dopamine

As promised: _URL_0_
Yes, and they make this motion regularly. It is called *torsion*, and is triggered by head rolls that move the ear towards the shoulder. The vestibular organ, located in the middle ear, sends commands that reach the superior and inferior oblique muscles of the eye.

The origin of torsional eye movement traces back to fish, who needed to keep a constant alignment of the visual scene while diving or rising. It persists in us to this day, though it serves little visual purpose, as we have been shown to be relatively insensitive to torsional rotation of visual input.
Toyota engineers found that they extend life of batteries by only charging to 40-60% of max capacity.  (This is also to act as a buffer for their brake generated power, so one can assume don't charge to more than 70%-ish)

_URL_0_
Due to tidal friction of the moon, the day was about 8 seconds shorter back then than it is now. The Earth was also in a different part of its Milankovich cycle (the various orbital perturbations mostly caused by Jupiter), so the seasons were positioned differently relative to the apse points (now, for example, the Earth is closest to the sun near the Northern winter solstice), and that affects the strength of the seasons in a given hemisphere. [The effect is much stronger on Pluto](_URL_0_).
In what context? Does the person still need to be alive afterwards? Simply incinerating the person would certainly remove all contamination.

If the bacteria/viruses are only on the skin, then they should be relatively easy to remove with soap and water (or bleach if needed).

If the person is already infected, then antibiotics/antivirals are probably the best option. However, these will not completely remove all of the microbes.
According to the paper and the new article, it is the distinctiveness of Australia's amphibian life in particular that cause this new biodiversity classification.  When researchers looked at just birds and mammals, Australia wasn't as distinctive.

So, in a way, it isn't New Guinea that is unique, it is Australia.  And if I had to guess at the cause for that, it would be the very large size, and diverse geography of Australia compared to new Guinea.  Adaptive radiation and speciation (apparently, particularly in amphibians) in unique and geographically distant regions of Australia likely drive this *statistical* difference.  So, while the biotic can be similar, they are different enough to be considered statistically distinct.
When particles are entangled, they share a state until measured. A practical example of this is that if you measure the spin of one of an entangled pair, the other will then always be found to be spinning in the opposite direction. It's not the case that changes made to one propagate to the other, so you can't send information this way.

Additionally, even the fact that the "knowledge" of a particle's spin (due to measuring the other in the pair) travels faster than light doesn't actually violate causality - you have no control over which spin you measure at your end of any hypothetical communication device. Put simply, you have a device capable of interacting with far-flung parts of the Universe instantly, but you have no control over what message you send with it.
**tl;dr atoms/molecules moving past each other**

#Metals
Atoms are arranged in crystal lattices, with each atom (for the most part) being positioned identically compared to its neighbours.  Atoms are held together in metallic bonds. Past a certain stress, relative motion (slip) will occur between atoms in the lattice along certain planes—that is, one plane of atoms will move relative to another.  This slip is not recoverable and therefore is retained after the stress is removed, resulting in plastic deformation.  

All metal lattices will feature *dislocations*, where atoms are missing or extra atoms are added where they shouldn't be.  These cause localised stresses that (usually) make it harder for slip to occur, as the energy required to move these dislocations (and their associated stress fields) is higher than for a uniform lattice.  Also, as dislocations move, they can tangle, which further limits motion.  Similar effects can be seen with solid solution strengthening, which uses different atoms (of a different size) to introduce similar stress fields. These limits increase the yield stress of a metal, at the expense of decreasing its ductility. 

#Polymers

Polymers are typically very long chain molecules—imagine spaghetti but each noodle is tens of metres long, then scale it down to the atomic level.  These are all tangled up and intermolecular forces prevent plastic deformation.  Past a certain stress these forces will be overcome and relative motion occurs—chains slide past each other.  This motion is, again, non-recoverable.

#Ceramics and glasses
Ceramics form regular crystal lattices (like metals) where as glasses have no regular structure (they are _amorphous_). In ceramics and glasses, atoms are covalently (or sometimes ionically) bonded to their nearest neighbours.  Stresses are elastic until these bonds break, at which point fast fracture almost invariably occurs.  This causes them to have very high (theoretical) yield stresses, as stresses must overcome these strong bonds.  

Typically however, flaws and micro-cracks in the surface of such materials cause them to fail at tensile stresses far below this maximum.  Failure stresses depend on the size of these flaws, and as such are very hard to predict.  In compression, stresses work to close these cracks and failure stress is much higher (though still far below the maximum).

Edit: spelling and grammar
No, it just adds more calories to your meal. The cola isn't actually acidic enough to have much of an effect. Your stomach is much more acidic due to hydrochloric acid, and contrary to popular belief, it doesn't break protein down into amino acids. It denatures them, meaning it makes them lose their shape so they can be broken down by enzymes in the small intestine.

Digestion is the breakdown of proteins into amino acids, carbohydrates into glucose, and fats into fatty acids. A calorie is just a measurement of how much energy can be harnessed from the breakdown of these units. Most of the calories come from the breakdown of glucose and fatty acids to make ATP, the molecule used as a direct energy source in all cells. Some amino acids can be turned into glucose and used to make ATP, but the body prefers to use them to make proteins. Any of these nutrients you do not use to make ATP (and the calories stored therein) become part of your body weight, as proteins, glycogen, or fat stored in fat cells.
When you see color, that is the wavelength off light (or color) that is not being absorbed by the paint. The more colors you combine, the more colors are absorbed, and fewer are reflected back at you. Eventually, if you combine all colors, all light is absorbed, and the paint looks black.
In a stationary environment I'm quite certain there would be no problem whatsoever. Simply swim just like you would underwater at any other time, and you'll be pushed forwards. Changing direction would be slower, since we're accustomed to letting gravity help with that a bit, I think.

However, to give the film some chance (not that it deserves it) with the bubble and JL being bounced and spun around, I'm not convinced it would be easy to make progress in any one direction consistently.
Velocity is not additive at relativistic speeds.  Well, technically it's not additive at *any* speeds, but the closer you get to 0, the more accurate of an approximation you get by adding up the two velocities.   [Here's a page on Wolfram Alpha](_URL_0_) where you can plug in various numbers to see how it works.  In your specific case, each person moving at 3/4 c would see the other party as moving at 0.96 c.
To be clear, sexual dimorphism refers to the morphological differences between males and females of a species.  Typically the term is used to describe obvious phenotypic differences, like larger or more colorful males (and sometimes females).  In the case of elephant seals, it's being argued that mating competition leads to larger males.  It's a common example because the dimorphism is so pronounced in this species (males 5 to 6 times larger).

Now, you're question is about what happens to the loser males?

_URL_0_

There are a few things to think about here.  First, it's important to point out that the loser males don't just accept defeat; the winner has to stay and protect his harem all the time.  Losers will try, and sometimes succeed, to mate with the females.

Sometimes, though, a male will decide to retreat.  This might not seem optimal from a fitness standpoint, but it could be for a few reasons.  That male could be endangering itself by risking a fight, preventing any future opportunities to mate.  Even if he's not in danger, he'd be expending significant resources (calories and time) on a likely futile endeavor.  Instead, he could be feeding and getting larger, or looking for other groups of females without such a large male.
The pressure on the inside of their bodies is the same as the pressure on the outside of their bodies. They do not have any cavities containing gasses which must be at surface temperatures.
A follow-up question stemming from [this thread](_URL_0_) on the front page: 

This link (_URL_1_) mentions the various volcanic gases and their effects, but would BBQing over lava present some kind of health danger?
Well, as the front page has made obvious, Earthquakes are impossibly hard to predict. If we could predict them, it would take a tremendous amount of explosives to set them off prematurely. We're talking on the order of hundreds of kiloton sized nukes. Even if we wanted to do that, the earthquake in your example wouldn't be a 3, it would more likely be a 6.8 instead of a 7. Still a significant difference, but these things are measured on a log scale. Setting one off a few months early wouldn't save us much at all in terms of energy buildup.
The effectiveness cannot be compared directly, since antibodies and small molecules have different properties, and are used for targeting different kinds of things. One important point is that antibodies can only target things on the surface of cells, or outside cells. Also, they can't be dosed orally. For some targets they are better than small molecules. For others, they aren't.
Unlikely.  People were far away from that test.  They were not allowed back for a long time.  If you are worried, you can buy a Geiger counter.  An easier way would be to buy or make a small cloud chamber and see if the paper causes an increase in tracks.  Otherwise you would have to send the letter to someone who can measure it.  Once again, I highly doubt it is contaminated.  You should keep that because that is an awesome letter.
I am a chemist. It makes sense that Furan has a higher melting point than THF because furan is aromatic and can pi stack.
At higher temperatures it seems that polarity of the C-O bonds become more important. Note that THF is shaped like an envelope with the O above the plane of the 4 carbons. Additionally the carbons in THF have sp3 hybridization and are more electropositive than the sp2 carbons in furan. This leads to a more polar C-O bond in THF.

That's about all I can do for you.
The expansion of space is not the same as movement through space.
If you're talking about fruit jelly, it's a gel, which is a network of solid molecules (in this case, pectin) surrounding and supporting a liquid phase.
Newtonian gravity is not consistent with relativity, because the field must propagate with instantaneous velocity. Thus fully relativistic theory of gravity is general relativity, which relates the energy density in a region to the paths that objects take through the region, and reduces to Newtonian gravity for low energy systems.

Relativistic mass isn't really a meaningful concept, it's mostly a relic from the past that doesn't  make much physical sense. What increases is the momentum, the expression you quoted times velocity. 

Just multiplying the mass by gamma and plugging it into Newton's equation won't give a real description of high velocity gravitational interactions. If anything, faster moving objects have a weaker effect, because they spend less time attracting. An exception is two objects in a very close orbit, where the total angular momentum contributes to the energy density.
This is actually very much an open area of inquiry.  

For your first question, fat stores do exist in a state of dynamic equilibrium.  Release of fat into the blood is hormonally triggered (there is typically very little release, and in fact mostly storage, when blood sugar and insulin are high), but as with most metabolic processes, it's more of a sliding scale than an on/off switch.  The two main culprits for release of fat are glucagon and epinephrine.  

When your adipocytes receive a signal that energy levels are low, they will release fatty acids into the blood.  However, for reasons that aren't well-understood, regardless of how much fat your cells are putting out into the blood, most of it circulates through the blood and liver and comes back to be stored as fat again, so your fat stores are constantly in flux in this fashion.  If you are particularly in need of nutrients, flux through this cycle will be higher, but the percentage of the pool that comes right back around to be stored again remains roughly the same at around 75%.

In terms of which fat stores are accessed and added to, there's a lot of active research in the area.  Generally speaking, we know that insulin resistance and visceral fat tend to travel together, whereas exercise and subcutaneous and leg fat also tend to travel together.  It is not entirely clear what the causal agent is in either case.  Some studies have found that low sugar diets which improve insulin resistance also decrease visceral fat storage, while others have found that as long as a deficit is being maintained, fat will be lost, but macronutrient ratios do not have a strong effect on the ratio of visceral and subcutaneous fat.  In some studies, women lose visceral fat better than men during calorie restriction.  In some studies most forms of exercise are observed to hit visceral fat harder than subcutaneous fat, and in others only aerobic exercise was observed to do this, and in fact could shift fat from one depot to the other even if there is not overall weight loss.  Overall, where fat is placed and where it is removed from seems to depend on everything from sex, genetics, and age to diet and exercise.

To complicate things further, some fat cells can undergo a process called "browning", wherein they stay fat cells but switch from being solely a storage depot to actually being basically a cellular furnace, burning fat for heat.  This happens upon cold exposure, tends to be more frequent in women than men, and probably has a genetic component, as some people simply do not have identifiable fat tissue that can do this. 

We are still learning about different fat depots, what triggers fat to be stored there, what kinds of signals the fat in these depots sends out, and what risk factors are associated with them.

Here are some sources you might check out; you'll note that some are quite new, since this is a really open field right now.  (Unfortunately this also means a lot of the interesting papers are behind paywalls, though.)  If you want to do some searching of your own, "triacylglycerol cycle" is a good search term for the first part of your question (and adding "glucagon" and "insulin" will get you information about the regulation of the cycle), and "adipose depot", "adipose browning", and "brown" and "beige" adipose tissue will get you newer research on the latter half of your question.  

_URL_3_    
_URL_0_  
_URL_1_  
_URL_2_
It depends on your age and health. There is good evidence that in people with cardiovascular disease, taking an aspirin a day does reduce the risk of heart attacks and strokes. However, if you don't have cardiovascular disease, then [this analysis]( _URL_0_) suggests that the risks of bleeding outweigh any benefits.

The other note of caution would be your age. I doubt any long term follow up studies have been done on people in their 20's or 30's who start taking daily aspirin. Thats a very long time of drug exposure, in people who are at low risk for developing these sorts of conditions.
The thing about ebola is that the disease lasts for a relatively short period of time (2-21 days but usually 8-10 [for symptoms to appear](_URL_6_) and most patients die [5-9 days after clinical onset of the disease](_URL_4_)) and for most of the time when people are infectious, they are obviously very sick (Humans are not infectious until they develop symptoms says the [WHO](_URL_2_)-scroll down- and symptoms include vomiting and bleeding from the gums.  People very quickly become very sick--[NPR interview with Doctors without Borders specialist](_URL_1_).  It _is_ suspected that the virus could be sexually transmitted for a few months after the disease because viral particles were found in semen for quite a while after infection, but I can't find any documented cases of transmission actually occurring via that route).  

People with HIV can walk around for years spreading the disease through sex without ever even knowing they are sick ([WHO says](_URL_0_) the time to develop HIV related illness is 5-10 years, for an AIDS diagnosis is 15-20).  People with ebola can walk around for a few days at most (see above-usually only 5-9 days of showing symptoms and being highly infectious-and based on the rapid onset of severe symptoms, most of that time is typically spent severely ill-_much_ shorter than the years to decades seen in HIV).  And after they get sufficiently sick they _will_ wind up in a hospital-pretty much everyone who is that sick goes to the emergency room and worries about costs later-or a the very worst a morgue.  Then someone will notice they had ebola and will track down their contacts...which will probably be small in number due to the fact that they would have been very sick and not out and about.  And since you don't start spreading the disease until you start showing symptoms, if you can get to the people and isolate them in time you can reduce the chance of spread significantly (now obviously some of this is my interpretation, but I want to point out that both [Nigeria](_URL_5_) and [Senegal](_URL_3_) were able to stop single point introductions from getting out of control using exactly these methods, despite the fact that it took them days and days in Senegal to get the patient quarantined, and note that of 900 contacts followed in Nigeria, only 20 developed the disease, and of 67 followed in Senegal, none developed the disease).

EDIT: Added some sources and additional information in parentheses.  I wasn't expecting this to become my most-upvoted comment.
> I understand that this principle explains you can not know where the object is and what it is doing at the same time, (If this is wrong please correct me.)

It's close enough for nontechnical purposes.

 > Does this hold true for dual/multiple observers? In other words, does the number of observers change the accuracy of the detection?

It holds true in general. No detection system can determine all properties precisely, even if that detection system is "several different computers".
Palm trees can survive at very low temperatures like most trees, especially when they are old and fully grown, the 'huge' palm trees that you are referring to likely do not even notice a few days below 40F.

That being said, the reason you see palm trees on islands and tropical areas is because of tropical storms, most trees are unable to grow in that kind of condition. For example, a red wood or a tree you would find in say Oregon or Washington would not survive a tropical storm, but they are better suited the absorb frequent moisture and their environment.

It's not exactly about temperature, but it is about environment, particularly other aspects of weather, animals, insects and other plants.

Also, there are palm trees almost everywhere, even in NYC and NJ. They may not be native but they survive.
It's possible to escape from a planet because the force of gravity weakens as you move away. You calculate escape velocity by finding out how much energy is needed to move from the surface of the planet to infinitely far. On the other hand, escaping from a constant 9.8m/s^2 acceleration would take infinite amount of energy.

Also, equivalence principle only applies locally, in a small region of space. It's completely possible to differentiate between gravity and acceleration if you can move around. For example, on the surface of earth, you could travel to the opposite side, and you'd either fall off or not.
Their main function is to add mass to the system, and thus inertia.

Cribbed from the wiki: The combined weight of the counterbalance and camera means that the armature bears a relatively high inertial mass which is not easily moved by small body movements from the operator (much as it is difficult to quickly shake a bowling ball). 

In professional systems like Steadicam, a low friction gimbal is added and the freely pivoting armature adds additional stabilization to the photographed image. Also, a body harness makes the weight of the heavy high-inerta camera-sled assembly more acceptable.
The value for the speed of light is not a fundamentally meaningful physical thing. By this I mean the fact that it has a value is a result of humans giving it a value, and there's no real sense in which it *needs* to have a value as far as the universe is concerned.

The reason the speed of light has a value is because humans decided a long time ago that we would measure distances and times with different units. This means that ratios of distances and times (like speeds) also have to have units, and therefore speeds need to have dimensionful values. However, when you understand special relativity you realize that it's not natural to measure times and distances with different units. Since four-vectors mix time and space components freely when you boost to different frames it seems silly to treat these two kinds of quantities differently.

If you give up the notion of measuring time with a different unit than space, the value of the speed of light just becomes 1. This is a very simple and satisfying number, indicating that things that move at the maximum speed possible are things that move along spatial directions the same amount they move along the time direction. Things that don't travel at the speed of light travel at speeds between 0 and 1.

Furthermore, if you understand this you realize why questions like "What would the universe look like if the speed of light were slower (or faster)?" don't make any sense. We couldn't possibly detect a difference in the speed of light, because the speed of light is just the maximum speed, and all meaningful notions of speed are just fractions of the maximum.
I'm far from an expert, but I took a course on linguistic simulation while pursuing a masters. The programs would try to replicate morphological and syntactic systems using iterative models of language development. The presiding assumptions were that language began with random utterances being paired with external events or internal states. Literally, cavemen pointing and grunting, then returning to the same grunts for the same events/states. This was far from anthropology and may have little bearing on what you're asking. But even from these assumptions, the models could return results very similar to natural language.
If it's average speed is 1 m/s, and it's accelerating uniformly, it has to be going twice that fast at the end for its average speed to be that. If it reaches 2 m/s in one second, that's 2 m/s/s.
It's generally believed that there was a single origin of life for two reasons, all life that has been discovered on Earth exclusively utilises left handed amino acids to build its proteins. If life were discovered that utilised right handed amino acids it would point to a distinct origin for that organism. Similarly all life on Earth seems to share a similar coding scheme for transcribing the codons of DNA into the amino acid sequences that build proteins. There is no known reason for one specific coding scheme to be prefered over another so the consistency across life on Earth is consistent with a universal common ancestor for all life remaining on Earth, if life emerged on the primordial Earth with an alternative coding scheme it has left no progeny.

The most prevalent explanation for why life no longer seems to emerge spontaneously is that the available chemicals for life tend to be taken up by organisms that are already there, so whatever improbable chemistry took place to give rise to life (or some self replicating "pre-life" molecule) would be more unlikely now. It's also worth considering that life has now evolved to exist in an environment with a significant amount of free oxygen, this is because photosynthesizing organisms have released oxygen over the millions of years life is known to have existed. The first living things would have come about in a world that had quite different chemistry with very little free oxygen.

Edit: replaced a word for clarity and a bracket closure
Food contains energy in the chemical sense. The feeling of having energy is not nearly the same thing. Both concepts involve the same name so it's most likely you just accidentally applied troll science in asking this question.
[Pagophagia](_URL_4_) is the term specifically to reference ice eating, as opposed to [pica](_URL_2_) which is the ingestion of non-food substances of any kind. Long story short, no one really knows exactly why people eat ice when they're iron deficient, but one theory is because [it could improve non hematologic symptoms of iron deficiency such as stomatitis and glossitis](_URL_1_). A quick literature search and review of the UpToDate article didn't really give anything more solid than that case report.

Pagophagia isn't very sensitive or specific for IDA, but it certainly supports the diagnosis. Most often, however, patients will present with generalized fatigue/weakness and malaise. On physical exam they will appear pale, and often have very white sclera with lack of the normal palmar color. Severe IDA can also have more profound findings like esophageal webbing, [koilonychia](_URL_3_), [glossitis](_URL_5_), [stomatitis](_URL_0_), and gastric atrophy.
Nothing absorbs 100% of the light that is shined on it, except maybe a black hole.  You will be able to make out shapes, but everything will look dark and green.
It's hard to judge as oysters have no central nervous system.  It should be alive when you eat it (tested by ensuring they can close tightly and aren't clackers).  I'd say either when you chew it up or if you gulp it down then after a short time in your stomach acid.
It really depends on the specific drug.

One common schizophrenia drug is Abilify which is a partial dopamine receptor agonist, meaning it binds to the dopamine receptor 2 (activation of this receptor is inhibitory, not excitory) without fully activating it, thus lowering the ability of endogenous dopamine to activate its receptor. It can cause parkinson's like symptoms (muscle ridgity) in people without parkinson's because the full effect of dopamine in the central nervous system will be inhibited. Note that this would occur in both healthy people on the drug and those with schizophrenia taking the drug. Schizophrenia may be caused in part by hyperactive Dopamine Receptor 2, so thats why drugs that lower its activity are commonly used.
Thousands of years ahead? Try billions of years.

A solar system capable of supporting life needs to be formed from the material from a dead star. Most stars last for several billion years, but really massive stars can have lifespans of only a few million years. The universe had 9 billion years to develop before our sun came along. Our sun has been here for a bit over 4.5 billion years (just a little older than the Earth). Our sun is believed to have formed partly from the remnants of a nearby supernova. This may have been due to the explosion of a short-lived massive star. The oldest star we know of was born when the universe was around half a billion years old, and the Milky Way galaxy has been around for about the same amount of time. It is reasonable to assume that it is possible for a new solar system to form out of the residue within a similar time frame. That leaves us with 8 billion years for a really massive star to form, go through its entire life, and end with a supernova. That's entirely possible, because as I said, really massive stars can have lifespans as short as a few million years. The bigger the star, the shorter it's life.

It took our solar system 4.5 billion years to form intelligent life, but even there, there's some leeway. It took Earth 800 million years, but it's conceivable that some other planet, with slightly different conditions could develop life a hundred million years or so quicker. Once the first cells formed, it took about 2 and a half billion to 3 billion years before multicellular organisms appeared. I don't know much about the biology, or the probabilities of it happening, but is it not possible that this could have happened on another planet maybe half a billion, or even a billion years quicker? Mammals lived under the dinosaurs for almost 150 million years. Imagine if the dinosaurs went extinct 100 million years sooner? A 3 billion year old planet could possibly reach our level.

There's so much leeway here. There's no reason a species of our technology level could not have existed 5 billion years ago, more than 5 billion light years away, and we just can't see it yet.
[Atom traps](_URL_0_) allow us to put very few atoms inside a potential energy field, partially isolated from everything else. If we know the form of the potential field, then we can infer the atom's mass from its dynamics. 

I have only done this theoretically, but maybe an experimentalist can have a more nuanced opinion on whether this procedure makes sense.
Clearly visible, at brighter than magnitude -12 from the distance of Triton orbit. Human eyes can see down to magnitude 6 or so. It would look like [this](_URL_0_), based on simulation.

Edit: It would be clearly visible in daytime on earth, from a similar distance*. This means: like if we had a neptune at the same distance as from Triton orbiting us, but only receiving neptune's usual light from the sun, while our usual daylight washes it out. It would look like [this](_URL_1_)
You might be interested in this [previous thread](_URL_0_).

But no, your body doesn't expend a large amount of energy for thermoregulation in usual scenarios, especially since your body's natural energy expenditure (for movement, etc.) already produces heat. However, that said, your body does take measures to regulate your temperature when it's too hot or cold (e.g. shivering, vasoconstriction, etc.), which definitely requires extra energy expenditure. That's why a day's worth of work in Antartica can require [up to 11,000 calories](_URL_1_).
1. best possible human [visual acuity] (_URL_0_) is around 20/10 (or 6/3); with this acuity you can discriminate between details or spots that are just one half arcminute of visual angle across (and separated by the same distance, so the limit of resolution is 1 discrimination per arcminute). you're going to need two good eyes to get such good resolution (basically for binocular noise correction).

2. the closest distance to your face you can converge both eyes and keep focused images on your retinas (assuming you're young and can still accommodate and have normal binocular vision) is (this is an educated guess) about 5±1cm.

3. with a little trigonometry (tangent of the angle of minimum resolution, times the speck distance, or 5cm*tan{1arcmin}), the tinest speck you can discriminate from other specks is 0.0015 cm wide, or about 15±3 microns. this is the size of a big [cell] (_URL_1_) (e.g. human eggs are much bigger than this), and a little bigger than your typical bacterium.

**this is lots of atoms, i don't know how many**.

(this might sound amazing, and it would be dependent on perfect lighting and other sorts of conditions, but if you change the viewing distance, it's a little more believable. for example, under these perfect conditions, someone with 20/10 vision could see, at 1 meter, the spaces between the millimeter marks on a ruler. impressive, but not unbelievable..)
It's important to be careful about definitions and terminology.  For example, we don't really say that objects (like a bullet) "**have**" force or no force, because it's not clear what you mean.  A better phrasing is to ask whether the bullet *experiences* any force, or alternatively, whether the bullet *imparts* any force (on the wall).

-

In Newton's law (*F*=*ma*), the force *F* represents the *net* force that the object (of mass *m*) is experiencing at a given time.  This net force is the (vector) sum of all individual forces acting on the object, and it is sometimes called the *resultant force*.  Under Newtonian physics, it is axiomatically true that if the velocity of an object is constant (*a*=0), then the net (resultant) force acting on the object must be zero (*F*=0).

-


Your hypothetical scenario is not very realistic:  even of the bullet velocity is constant before it impacts the wall, surely it will slow down (and even come to rest) when it actually hits the wall.  The change in velocity implies a deceleration (*a* < 0), so the net force experienced by the bullet is nonzero.  Furthermore, as a result of Newton's third law, we can conclude that the bullet therefore imparts a nonzero force on the wall.

-


A more interesting example would be if you consider a rocket flying into a wall, and assume that it is possible to program that rocket thrusters to increase thrust on impact, in a way that would keep the rocket velocity constant.  In this scenario, the rocket acceleration is zero on impact, which means that the resultant force *F* is also zero.  However, this does **not** imply that the rocket and wall feel no forces.  Because *F* represents a *net* force, the correct conclusion is that the sum of the impact force and the thruster force is zero, i.e., the thrust is equal and opposite to the impact force.  And again, due to Newton's third law, the impact force experienced by the rocket is equal and opposite to the impact force imparted onto the wall.
Such conversion from time to frequency domain, done using [Fourier transform](_URL_1_) has a very wide variety of uses. Here are some of those I am familiar with:


A lot of devices, including transistors, amplifiers, detectors, our ears and others respond differently, depending on the frequency. Some frequencies can be detected or retransmitted, while other will be blocked. If you have some signal in time domain, you need to transform it to frequency domain to calculate how some device will change it and afterwards you can use an inverse transform to see how your time domain will look after passing through device. 


Many audio compression algorithms delete the frequencies that we can't hear from the recording to reduce the file size. Sometimes the sources in your audio signal consist of different frequencies, so you can emphasise one of them by deleting other, like in [this Vuvuzela example](_URL_2_).


In spectroscopy, the use of Fourier transform allows to make spectrometers that don't sacrifice light intensity to get higher resolution ([FTIR](_URL_0_)). Using interferometer, the spectrum is recorded in length domain, which is related to time domain. Applying the FT to that signal gives the spectrum.


In laser optics and electronics, FT allows to calculate what will happen with your signal as it passes elements with different spectral responses.
Caffeine reduces vasopressin secretion from the posterior pituitary gland. 

Vasopressin, also known as antidiuretic hormone (ADH), is generally produced in response to increased osmolarity of plasma, meaning higher concentration of stuff in blood due to low overall blood volume. One of its main functions is to act on the kidney, modifying permeability of specific sections of nephrons, allowing for greater recapture of water from urine (which will hopefully be used to fix the osmolarity problem in the circulatory system). (Its other main function is to cause restriction of blood vessels, but that's not really relevant here.)

Since caffeine works to inhibit vasopressin production, it prevents this antidiuretic effect, resulting in me having to go to the bathroom many times during class on the mornings which I choose to drink coffee.
I'm not sure if there will be an expert to answer this. Here's a general answer based on PVC degradation and whether conditions enable it to degrade.

UV and thermal degradation mechanisms both exist for PVC. It will depend a lot on the specific site what the degradation rate will be, and even on the type of water flowing through the pipe while it is active. With heat and UV light, it would have trouble lasting to 500 years or even 10 years in usable condition. Trapped in the ground at ~15C, it could last far longer, particularly if it happens to be a low oxygen environment. This is why sites like bogs, deserts, and glaciers can tell archaeologists so much more than other sites where degradation is rapid. Also, if there is mineral buildup in the pipe, that might outlast the pipe itself. If a life form would happen to emerge that can digest PVC, than that would be a new degradation mechanism and would alter results.
A little knowledge is a dangerous thing.

There is no significant evidence for the brain having a clock, at least in any fashion that is comparable to clock in a microprocessor. There may be some neural behaviour that is based on rhythmic neuronal behaviour/oscillations, but that is not equivalent to the clock on a microprocessor. In order to understand why they are not analogous, you need to understand what they both are.

A clock in a microprocessor is obviously based around a piece of vibrating quartz, a frequency multipliers and so on... but we can abstract it to a square wave generator, that is a voltage switching between high and low. The logic gates inside a microprocessor a built up of transistors, and they do not need a clock unit to work: an AND gate will perform it's function at any rate, up until the transistors simply do not have enough time to respond. HOWEVER, in order for large numbers of logic gates to work in parallel, they must be synchronized, both on the input (both inputs to an AND gate arrive at about the same time) and on the output (the output from an AND gate and and OR gate might need to arrive at another gate at the same time). This is, in essence, achieved using something like a 'register', which stores a binary value, and spits it out when it is asked, which is the next clock cycle. That is, the gate performs it's function, the value is put into a register, and then the next clock cycle, the register supplies that value to the next set of gates.

The brain does not work in a comparable fashion. There is no central clock. There are no registers. Neural function does not happen in a synchronous fashion. However, if one was to FORCE the analogy, it would be a network of logic gates with no registers. When a complex pattern of inputs (sensation) arrives at the input nodes of the logic gates, then perform their functions which then cascade as fast as the logic gates will allow through the network, and produce an output (movement).

So then, this might make you wonder what these EEG oscillations are? I think the most useful analogy is to simply think of them like the sound of your cars engine. When you car performs it's task, that task necessitates the generation of sound. Those sounds reflect the underlying task (low rumble: idle. Load roar: acceleration. Ticking: engine cooling) but they are not fundamental to the task. There are some neuroscientists who disagree, but I have never seen any actual evidence that proves that this is not the case.
There's quite an elegant solution for this actually.  T cells are produced in the thymus, located in the chest, from precursors from the bone marrow.  T cells have [T-cell receptors](_URL_0_) that function almost like an antibody directly attached to the cell membrane - that is, its role is to recognize exogenous epitopes.  To do this, developing T cells must create a very diverse pool of receptors, such that they can recognize almost any peptide sequence.  This is done through a complex process called [VDJ recombination](_URL_3_).  When a developing T cell successfully produces a T cell receptor, it passes the first 'checkpoint' in its development.  The next checkpoint verifies that these T cells and their receptors are able to interact with the rest of the cells in the body, by checking that it can bind to [MHC proteins](_URL_2_).  If they don't bind with the right affinity, they would not be useful in the body and also are culled at this stage.

So now we have T cells that can bind to other cells, with tons of different recognition sites.  Obviously many of them will recognize 'self' protein sequences, which would be bad.  So the next step is to avoid auto-immunity.  Developing T cells migrate to a part of the thymus that expresses protein sequences from all over the body.  Any T-cells that bind here (signifying that its receptor recognizes a native epitope) would not make it into active circulation.  Thus, auto-immunity is prevented (ideally).  There's also some complexity involved with [regulatory T cells](_URL_1_) selected from the cells that have high affinity for self antigens.
As a person suffering from tinnitus: no, there is no biological way to reduce the level of tinnitus. You can however distract the brain from the tinnitus so you forget about it. Although, that depends on the intensity. If you suffer from tinnitus the level of a vacuum cleaner its impossible to ignore...Some people administer "counter sounds" to mask the tinnitus but that only works well for types of tinnitus in which the patient experiences a constant tone.
Long ago, the earth went through the process of [planetary differentiation.](_URL_0_)  Dense material (iron and other siderophile elements) preferentially moved to the core while lithophile elements concentrated in the mantle and crust.  

The part of the earth that we see, the crust, looks so variable because of magmatic differentiation processes.
From my response to a similar question: Primary deposits of native element minerals like gold are found in hydrothermal veins associated with volcanic plumbing systems.
As a magma starts to crystallizes, it will crystallize typical igneous minerals like olivine, amphibole, pyroxene, and feldspar. These minerals have crystal lattices that require ions of a particular charge and size (with some wiggle room). Some trace elements can replace the "usual" element in a lattice more easily than others.  Other trace elements like gold and silver just don't really fit anywhere, so they continue to be concentrated into the remaining liquid portion of the crystallizing magma. Eventually, they can escape with fluid in high enough concentration that they form ore deposits.  In a process called fractional crystallization, early forming minerals like olivine will sink to the bottom of the magma chamber, taking the elements compatible with their crystal structure out of the magma system.  This is why we see such variation in rocks at the crust.

The way an element behaves in magma is described by its partition coefficient, where Kd = (Concentration in Mineral) / (Concentration in Magma). A number less than 1 means that the element doesn't fit well in the mineral and will be concentrated into the magma. You can view different elements and what minerals they like at the [GERM](_URL_1_) database.
You're feeling your [flight-or-fight response](_URL_1_), which (among other things) winds up releasing [adrenaline](_URL_0_) into your bloodstream. The sharp change in heart beat and breathing rate, as well as the dilation of blood vessels, can make you hyper-aware of your heart beat at the same time that it is changing quickly. This can make it feel as though your heart as "skipped a beat."
Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.
The "don't do anything unless it's proven totally necessary and safe" philosophy is applied much more strongly to humans than to animals, which means that we're willing to do things to our pets that we wouldn't do to humans.

Also, different vaccines have different lifespans. Chicken pox is usually a once-in-a-lifetime shot, but tetanus is every few years as needed.
Mid 17th century. René Descartes published his *La Géométrie* in 1637, proposing the union of algebra and geometry, and a 1649 Latin translation expanded and clarified this to the two-axis system we know. We still call it *Cartesian* coordinates after Descartes.
That is because this is a good example of the difference between semantic memory and episodic memory. Episodic memory refers to memory about past events, whereas semantic memory includes our basic knowledge about the world.  Typically, in most types of amnestic disorders (including the most common types of dementias, like Alzheimer's), episodic memory is selectively affected more than semantic memory.  Examples of questions a counselor or doctor might use to evaluate the latter include "what is the capital of the US?" and "what do dogs and cats have in common?"  So, in your question, you could include in the semantic category the very idea of what "identity" means.

When speaking of forgetting self identity, it does happen in what is called dissociative amnesia.  You see it depicted in Hollywood tropes such as *The Bourne Identity* and *Memento*, but in real life, that degree of self identity loss is quite rare and usually psychogenic, rather than due to organic brain disease.  This is probably due to the fact that our personal identity is ingrained deeper than other forms of semantic memory in the temporal lobes.
If it is completelly inhibited, the cell would die, because it couldn't make any proteins and catalytic enzymatic RNA. When it would need new proteins, it wouldn't be able to make more, it also couldn't replicate, because it couldn't synthesize the proteins needed for binary fission.
The short answer is yes. In the first half of the 20th century, we came to understand all the non-gravitational forces in terms of *fields*, with the quintessential example being the electric and magnetic forces. In fact, it wasn't just the forces that came to be described this way, but the matter. Field theory came to underlie all of particle physics.

Not long after, people figured out that general relativity could be formulated in much the same way. In this picture, spacetime is flat, and gravity is carried by a *tensor field*. That just means that, much the way that the electromagnetic field is described by a vector, the gravitational field is a matrix. And while it's not a geometrical theory, this particle-physics view of gravity is also very elegant: it turns out that, as long as you demand a few reasonable conditions (like special relativity), the *only* consistent theory for a tensor field which describes gravity is general relativity!

Now this approach agrees with all the findings of GR for a very simple reason - it's the exact same theory, just in a different mathematical language. Instead of looking at things in terms of geometry, you look at fields and particles, just like in the rest of particle physics. Particles, in particular, come in when you introduce quantum mechanics - then each field comes with an associated particle, and the type of field (vector, tensor, etc.) turns out to correspond to how much spin that particle has. In natural units, vector fields have spin 1, tensors have spin 2, and so on. So the statement is that GR is the unique theory of a massless spin-2 field. The associated particle is called a graviton, analogous to the photon in electromagnetism.
If you can come up with an infinitely big blob of mass...

Oh well, to make the question realistic: Get a finite sized container and start increasing the gravity underneath it, it should be the same, right? Now, ok, at some point, the force will be so big that it will be no longer sensible to talk about water, since all the atoms will be completely crushed apart. If the pressure is very very very big, then at some point you'll have a neutron star.
I am not sure how to answer this in a strictly one dimensional space, but I know in 3D space that cancellations of the light wave amplitude in one region of space lead to amplifications of the amplitude in other regions so that the total energy is still conserved. 

For a typical example of this take the famous double slit experiment. There are regions on the wall that appear black, and there are regions on the wall where they are brighter than they would normally be (brighter than one of the light sources alone). The explanation is that the energy lost by the two light waves cancelling each other out in regions where it is black reappears in the extra energy for the bright fringes where the two waves add together. When you take into account the energy before and after you find that the total energy overall is still conserved. 

Now in the double slit experiment, if we were to just take our detector and place it on a dark fringe and only measure that portion of space then we wouldn't "see" any of the constructive interference going on elsewhere, we would just see the region we are measuring where there is no light and perhaps conclude that energy has been lost. 

However, even in this case energy isn't really lost; it is just appearing in regions where we aren't looking. If we moved around our detector we would observe the full interference pattern and detect both bright and dark fringes appearing and the extra energy would be in those bright fringes. 

In the interferometer it would work the same way, if you had it adjusted to a dark fringe, and then changed the settings to adjust the path length you would see the bright and dark fringes of the full interference pattern.
These ratings are called ["audience measurement"](_URL_1_) and are usually **extrapolated from a small panel of supervised viewers.**

At least in Germany, we are talking about [2,000 to 6,000 more or less randomly selected people](_URL_2_) who receive some equipment that logs their viewing habits and sends that data "back home".

Other techniques include diaries or simply asking people, or more recently [digital TV boxes / receivers that communicate with the "mothership".](_URL_0_)

---
Disclaimer: I'm not working in this field, which is why I link to Wikipedia. But I remember discussing this whole panel group thing when I was younger. Pretty much common knowledge in Germany.
One of the things to consider in order to address your question is how ecologies have rebounded from previous mass-extinction events. There is quite a bit of litterature on this topic, and the observed recovery times for biodiversity levels vary considerably form a few million years to about 30. I suggest you use the Permo-Triassic extinction as your benchmark. It has often been singled out as the singlemost severe extinction event in our history, which obliterated about 90%-plus of all extant species ... this gives you an upper limit, and an order of magnitude for the time scales involved.

In the case of the Permo-Triassic, the recovery of biodiversity to pre-extinction levels took 30 million years ([Sahney, S.,  &  Benton, M. J. (2008). Recovery from the most profound mass extinction of all time. Proceedings of the Royal Society of London B: Biological Sciences, 275(1636), 759-765.](_URL_1_)).  The causes and mechanisms responsible for the severity of that event are multiple and complex, without singling out a specific cause, it involved massive climate change and aridification, ocean acidification, loss of marine platform habitat (gee ... sounds familiar ...).

The Cretaceous-Tertiary event, which kicked Dinosaurs and Ammonites out and opened the door to the expansion of mammals, was followed by a relatively quicker 3 million years of recovery time ([D'hondt, S., Donaghay, P., Zachos, J. C., Luttenberg, D.,  &  Lindinger, M. (1998). Organic carbon fluxes and ecological recovery from the Cretaceous-Tertiary mass extinction. Science, 282(5387), 276-279.](_URL_2_)). 

Overall and in a general sense, recovery from mass extinctions is believed to operate in the order of the 10 million year range ([Kirchner, J. W.,  &  Weil, A. (2000). Delayed biological recovery from extinctions throughout the fossil record. Nature, 404(6774), 177-180.](_URL_0_)). The big five have involved a certain variety of causes ... climate change, meteorite impacts, habitat loss, glaciation, etc. However, none has ever involved the mechanism you propose: nuclear fallout, so we have to be conservative in assessing how this would affect hypothetical recovery times, hence my suggestion of using the Permo-Triassic as a benchmark. And you have to keep in mind that this nuclear event would piggyback of current environmental stresses resulting from climate change. Together, they might pack quite a wallop...  So, that raises the question of whether recovery would be possible at all, at least in terms of metazoans. In each mass extinction, post-recovery communities were quite different from the pre-extinction ones. There is no rule saying anywhere that all and any extinction event must necessarily be followed by a recovery. It is quite conceivable that some level of hardship is just terminal ... at least for some kinds of life. And therein might lie the difference between a world populated by metazoans, and a world of bacteria and archeans for instance.

One last word, to put things in perspective. The average life span for a species is in the order of 1-5 million years, and most of these species are marine invertebrates. Complex critters such as primates average shorter life-spans in general. The implication is that if we should go through an extinction level event within our lifetime, whether a nuclear holocaust, through climate change or some other mechanism, it is unlikely in the extreme that our species will make it to the other side and witness any hypothetical recovery which might follow.

other usefull references:

[Solé, R. V., Montoya, J. M.,  &  Erwin, D. H. (2002). Recovery after mass extinction: evolutionary assembly in large-scale biosphere dynamics. Philosophical Transactions of the Royal Society B: Biological Sciences, 357(1421), 697.](_URL_3_)

[Chen, Z. Q.,  &  Benton, M. J. (2012). The timing and pattern of biotic recovery following the end-Permian mass extinction. Nature Geoscience, 5(6), 375-383.](_URL_5_)

[Payne, J. L., Lehrmann, D. J., Wei, J., Orchard, M. J., Schrag, D. P.,  &  Knoll, A. H. (2004). Large perturbations of the carbon cycle during recovery from the end-Permian extinction. Science, 305(5683), 506-509.](_URL_4_)

EDIT: I can't spell...
The sperm is chemoattracted to the egg. The egg, in mammals, has a glycoprotein "mesh" known as the Zona pelucida.  When the sperm comes into contact with the Zona pelucida and binds, using a protein called Bindin. Bindin does exactly what it says on the tin, and binds to the zona pellucida, bringing the sperm and egg cell membranes very close together. Exocytosis of the acrosomal vesicle in the sperm- a vesicle containing digestive enzymes etc, occurs. These enzymes degrade a path to the egg cell membrane for the sperm. The membranes then fuse.

The fusing of the two cell membranes causes an influx of ions into the egg, and egg's cell membrane becomes polarised, this prevents other sperm from fusing with the cell membrane for about a minute. This is known as the "Fast block"

Also, after fusion of sperm and egg, Cortical granules, in response to Ca^2+ signalling in the egg fuse with the plasma membrane, releasing the protein degrading enzymes they contain to the outside of the cell. These enzymes degrade the protein that links the glycoprotein mesh to the egg, this causes the mesh to be removed from the egg. This is known as the "slow block"

I hope this helps.
If you are referring to that classic "WW2" divebomber sound, you might be thinking of the siren the German "Stuka" carried to cause panic and fear among those it was attacking. The siren was mounted to the landing gear spat and would be driven by a small propeller that intensified when the Stuka entered a dive.
Honey bees have the waggle dance. The direction of gravity inside their hive represents the direction of the sun outside. So if a bee does a waggle run straight upwards in the hive (into the force of gravity), then other bees know that, when they leave the hive, they should directly toward the sun to find food.
A punch is fundamentally a collision.  For collisions, it is more useful to view them in the context of conservation of energy and conservation of momentum than from a force perspective.  A punch before the collision has kinetic and rotational energy.  In the case of a hook, we could consider the torso and arm a fairly rigid body, and therefore find the moment of inertia.  In the case of the arm being extended, the moment of inertia changes, but the same concepts can be applied.  When a punch lands, there is kinetic energy transferred (ie the head snapping back), and there is also deformation and friction and heat loss.  

The dynamics of a something like a punch are fairly complex, as you are dealing with many levers, many forces/torques, and non-ideal materials.
Aside from the mechanical effects of stretching the stomach, satiety is also conveyed through fat-tissue-release of leptin. This lets the brain know that at least fat levels within the blood are adequate for the work output. More leptin in the blood = less desire to eat.
Actually, from what I understand (and somebody else will have to confirm) the actual physical mass does not increase - the *momentum* does. So we measure the energy required to get something up to very high speeds and calculate that there was an *apparent* increase in mass.

This works because of mass/energy equivalence.
Digestion, remember,  is really just a way to get your food small enough and broken up enough that your gut can efficently absorb what it needs. Your teeth breaks it down into physically tiny bits, your stomach acid (and proteases that specicially chop up proteins) break your food down chemically, into a nice soup of peptides, sugars and fats and other things. This is then absorbed by the very efficient small intestine.

Drinks are already a soup of sugars and chemicals, and don't need to be broken down. It can get along the tract a little quicker. I would NOT say it's immediate though-- you need it to get down to the small intestine to get caffeine properly absorbed.  This takes very roughly an hour depending on what your GI tract is actually doing. Then your blood needs to carry it up to those receptors caffeine so loves to block.

So! TL;DR: The effects aren't immediate! But the psychological impact of it probably inspires more alertness. The placebo effect is wonderful.

Edit: fixed my definition of protease.
Different vending machines work in different ways, but they essentially all want to do the same thing - sort different coins into different categories. 20p coins in one category, 50p coins in another and so on.

How do they do this? Different methods, but two of the most common are measuring electrical conductivity or resistance and measuring mass. There will be a narrow range of masses and resistances that are acceptable for each coin type. A narrow range is used to exclude fake coins.

If the coin is tarnished in some way there may be a fine layer pf some other compound on its surface British 20p coins [are made of 84% Cu, 16% Ni](_URL_2_). If the copper or Nickel has reacted with another compound there may be some fine layer that will effect its resistance. That is what I suspect happened here.

Edit: You can see the difference between [an old, slightly dirty coin here](_URL_1_), and a [brand new clean coin here.](_URL_0_)
AMTRAK trains run at ~100 gallons/hour, 79 MPH, so 79/100 or 1.2 MPG. Theres an average of 260 passengers per train, so if 2.5 people rode in each average car, it would take 104 cars to transport the same as 1 train.

104*30=3120 gallons/mile total for the cars, and .79 gallons/mile for the train.

Somebody please check if my math is correct. i suspect it isn't, but even so, i have gathered a lot of data for you.
First thing to understand, the BBB is, in essence, a thicker, more selective wall surrounding the blood capillaries that supply the brain.  These specialized capillaries are surrounded by astrocytes, which are a specialized type of cell that only exists in the brain.  Among its many functions, it sends out processes which surround the capillaries.  These capillaries, much like those in many other parts of the body, are extremely small, and can be small enough in diameter to allow the passage of blood cells only in single file.  These tight diameters mean that very little is needed to clog these areas, and large particles can become stuck.

As far as metastases go, they are cells which have broken from the primary cancer site and have begun circulating through the body.  The most likely way for them to enter the brain is through arrest and extravasation; they become lodged in the small capillaries of the brain, then begin to proliferate through the BBB.  

Does the BBB make brain metastasis more rare?  Yes.  In fact, cancers are most likely to metastasize into the lungs, liver, or bones, where capillaries are small and blood or lymph is filtered.  The most important issue, however, is how the BBB affects the treatment of cancers in the brain.  Because of its impermeability to large particles and most drugs, chemotherapy cannot be used to treat most, if not all, brain cancers regardless of their origin.  It's why cancers such as glioblastoma multiforme (GBM) is considered one of the most deadly.
Thermodynamics says that since the surface of the sun is roughly 6000 kelvin, that's the hottest you can make anything by focusing sunlight (if you could make something hotter you'd effectively be moving heat from a cold body to a hot one without doing work). Fused quartz (a misnomer since it's a glass, but we'll set that aside for now) which is the most temperature resistant silicate glass in common use will soften at roughly 1200-1300 K, but most glasses, such as the borosilicate often used in lenses, will soften at much lower temperatures.

The maximum concentration factor for sunlight (set by the dimensions of the sun in the sky and the principles of geometric optics) is roughly 46,000x. If I take your claim that you could focus the light from the fresnel lens (1850 sq. inches) down to to the size of a pencil lead (roughly 0.01 sq. inches) that gives a concentration factor larger than is theoretically possible. You will in fact find that you either have to throw away light to get such a small spot or work with a larger spot size.

A detail here is that the amount of radiation something emits is proportional to its temperature to the fourth power. Since the thing you heat up must be in radiative equilibium with its surroundings, it will have to be radiating as much energy as it receives. Thus, to get a rough idea of the max temperature you might reach you can use

6000^4 /46000 = T^4 /(your concentration factor).

The upshot of this is that if you manage to hit a solar concentration factor of 23,000x (half of the theoretical maximum) you'd heat a fully isolated object with a reflective back side to 5000K. If you work this out for a temperature of 1300K, you find that a concentration factor of 100 will be enough in an ideal system. Real systems have a lot of thermal losses and so operators of solar thermal power plants find that they need concentration factors of ~1000 to reach 1300 K (see [this source](_URL_0_)). If you manage to reach these concentration factors, any absorbing flaw in or on your lens may get hot enough to melt glass and destroy your optic. If it heats up fast locally, the optic could shatter due to thermal expansion shock.

I recommend taking extensive safety precautions and wearing a full set of personal protective equipment if you plan to work at high concentration factors. The risk of burns from concentrated light and/or (potentially molten) glass shrapnel is real.
It depends on what we're talking about. Motivation to accomplish actions depends on a lot of factors, but generally boils down to the effects of dopamine and serotonin. 
Dopamine is responsible for the reward response (along with a whole bunch of other things), but that means that we get a little hit of dopamine when we perform rewarding activities (increasing the likelihood that we'll perform said activities at a later date). 
Serotonin appears to play a role in activating behaviour - stimulation of the raphe nuclei (where most of the serotonin-producing neurons live) produces locomotion and cortical arousal, whereas drugs that inhibit the production of serotonin, such as PCPA, reduce cortical arousal. Also, the reason that a number of SSRIs have the (apparently counterintuitive) side-effect of increasing the risk of suicide in depressed patients is because they increase the amount of serotonin available to neurons (by inhibiting the reuptake of it by cells). So, people who were previously suicidal but didn't have the motivation to go through with it suddenly now have extra serotonin, and are therefore motivated enough to make an attempt.
(Note: Most of my source for this was an undergrad neuropsych text book)

Willpower, however, is a bit different. Currently, one of the major models concerning self-control is known as the "Strength model" of self-control. Basically, it posits that self-control acts like a muscle - it relies on some underlying resource, and as you use self-control it gets exhausted, such that subsequent acts of self-control become harder (and thus, are more likely to fail). Like a muscle, rest can replenish this resource. Now, they haven't really identified this mysterious resource, though there's some suggestion that it might be blood glucose in the brain (just the abstract, search it if you have access to PsychINFO: _URL_0_). Most of the research into this model has been driven by Roy Baumeister, Mark Muraven, and some others, so if you have access to a journal database, you can search for their papers. Personally, I disagree with their model, but that's just me. It is, however, a juggernaut in social psychology at the moment, and their findings are certainly robust and interesting. So it could, potentially, be that blood glucose is responsible for willpower.

TL;DR: Motivation accounted for by dopamine and serotonin. Willpower potentially accounted for by blood glucose available to the brain, look for stuff on the strength model of self-control for more information.

Source: I'm doing a PhD on self-control. 

Edit: If you're particularly interested in improving your own self-control, there's a good book called "The Willpower Instinct" by Kelly McGonigal. It's engaging and has lots of real-world examples of ways to improve your self-control (as opposed to dry academic stuff).
According to [this study](_URL_0_) memory B cells reside mostly in Germinal Centers (ie the lymphatic system) and not in the blood. Therefore while there are a small number circularing in your blood, I doubt donating blood will have any meaninful impact on wipeing out an entire clonal lineage of memory B cells.
Since any answer is better than no answer at all: Na is soluable in water, so it detaches from F and that leaves a bunch of F ions floating around in water. Now we know that F is very very reactive and so that amount of reactivity is harmful to our bodies.
Most of those were below ground, and the above ground tests would have produced a slight cooling effect due to kicking up particulates.
The energy is stored in the gravitational field itself. There is a quantity called "scalar potential" or just potential for short which is a measure of how much work a field is capable of doing to a unit "charge" (which in gravity's case would be mass).

It works like φ*m=U where φ is potential, m is mass, and U is potential energy. To find the work done, you just take the difference between potential energies at two different points (W=U2-U1).
it was scraped away, by stones embedded in a glacier, or else it'd be smooth and uniform. at least that is what the guide told us in upstate new york, the drag marks prove it was a movement, like an excavator.  just what i was told, hope it helps.
Everything, every kind of energy and momentum and stress and strain curves space-time in some way. Some of these space-time curvatures *result* in the effects we commonly call gravity. On top of this, gravity pulls mass closer together, so the initial variations in density of the very early universe became amplified over time to leave compact regions of mass like galaxies separated by vast voids between them. So in a way, gravity also "creates" everything in so far as it pulls mass together into the structures we're familiar with. 

So both.
It's possible in a binary star system for one of them to have planets, or for both to have individual planetary systems. Alpha Centauri is an example of this (if its recently discovered planet actually exists).
Inertia:  the resistance of any physical object to any change in its state of motion including changes to its speed and direction or the state of rest. 

Plus, low friction coefficient between the side of the mug and the liquid. 

Equals, the coffee tends to remain in the same place while the mug (and you) moves around it. 

If you spun really quickly for a while, eventually the coffee would start spinning too. Then, when you stopped, the coffee would keep spinning. 

Give it a try and report back!
The real answer is "the partial differential equation describing elastic deformation in solids has two types of solutions, corresponding to P and S waves". The derivation is too long for reddit but it can be found in any seismo textbook.

A more intuitive explanation is that the stress-strain equation (stress basically represents surface forces in a continuum; strain is how the material moves and deforms) has two terms for solids. The first term represents compression of the material (as in, squeezing it in a way that changes the volume). The second term represents shearing the material (this one is for solids only: liquids can shear freely).

P waves represent compressional strain (so ground motion is in the same direction as the wave's propagation direction). S waves (again, solids only) represent shear strain (so ground motion is perpendicular to the wave's propagation direction).

P waves are faster because for a given magnitude of strain, the stress is higher if it's compressive than if it's a shear strain. More rigorously, the elastic modulus used to calculate compressive strain is, for any material, higher than its shear modulus...and a wave's speed is the square root of the elastic modulus divided by density.

Surface waves are P and S waves coupled together in particular ways. I don't know any intuitive explanation of why they work out, just that they happen to be solutions of the seismic wave equation. They're bound to the surface because their slowness vector (the vector that points in the direction the wave is moving and whose magnitude is 1/wave speed) has an imaginary z component (big mindfuck, I know). This means that they decay exponentially with depth and propagate slower than either the P or S wave speed.

So, for a given seismic source, the resulting waves will be a combination of the various solutions to the seismic wave equation. As a result, most sources will produce a mix of P, S, and surface waves. (Explosions are an exception: because it's a purely volumetric, zero-shear source, a true explosion will produce P waves only. Ultimately, some of the P waves will convert to S waves at boundaries in the Earth, so in practice you'll still record S waves and surface waves.)
It depends what you mean by touch. My hand isn't really 'touching' the computer. What we perceive as touch is the electromagnetic force when my hand gets very close to the table. But on a smaller scale, when particles get very close to each other, other forces (the strong force) becomes dominate over the electromagnetic force.
Are you talking about passive diffusion down, for example, a simple concentration gradient? In that case, the behavior of the system is governed by statistical mechanics. Consider a system that consists of two closed cells separated by a membrane permeable to a solute, with the same solvent on both sides for simplicity's sake. If you pour, say, KCl on one side, the ions will begin bouncing off one another and off water molecules at random. In the absence of outside forces, they disperse evenly because that's just the most likely arrangement of molecules. You still get flow back and forth in a system that's reached equilibrium, but in this example, the concentrations are going to be equal on both sides because it's the most statistically likely arrangement. 

In the human body, things are a lot different. Depending on the type of cell you'll have sodium, potassium, chlorine, calcium and other ions in different concentrations on the inside and outside of the cell. The concentrations of these change rapidly in response to stimuli or voluntarily to initiate various processes in the body. In addition to concentration gradients, you have voltage gated transmembrane proteins, Na/K ATPases and a host of other things moving ions back and forth.
I'm not an expert in this specific question, but I can tell you that it is a very difficult prospect.  The problem with space is that it isn't just gamma rays and x-rays, but cosmic rays.  They have so much energy that if you put a shield between you and the ray, the ray will [*shatter*](_URL_0_) the nuclei in the shield and just turn them into more rays that will cause more damage than if you didn't have a shield.  The thickness needed to stop a beefy cosmic ray is prohibitive to space travel.

This could be why you are having trouble finding the work being done... there may not be any.

Keep in mind that the dose one receives is the dose rate times the time of exposure, so making spacecraft faster will decrease the radiation dose.
Assuming you mean friction between water and another surface, the answer is yes, but not in the heat based way you may expect.

Boiling is the vaporization of a liquid, which occurs when a liquid sees conditions below its vapor pressure.  This can be induced by increasing temperatures, lowering pressures, or a combination of the two.  

So, to answer your question, water can vaporize (boil) from high speed but it isn't due to the only to friction, and is not a result of heat caused by friction.  At high enough velocities, the friction between water and a surface causes the water to sort of stretch, causing a temporary, localized area of low pressure below the vapor pressure.  The result is a temporary pocket (bubble) which then quickly and violently collapses as the pressure recovers.  This is known as cavitation, and can wreck boat propellers and pump impellers if operated  in cavitation conditions.  [Here](_URL_0_) you can see the surface of the boat prop moving through the water fast enough to cause cavitation (the white spiral shaped bubbles).  Notice that the cavitation occurs at the leading edges where velocities and pressure differentials are the highest.  This cavitation is your boiling, just not in the normal sense.

edit: grammar
My vote is for heat. At colder and colder temperatures, the formation of ice crystals and lack of thermal energy for chemical reactions to occur generally spell doom for organisms. Some organisms can utilize different molecules (like natural antifreeze) to deter the formation of ice crystals, which can help preserve the organism, even if they aren't doing much 'living'.

At higher and higher temperatures, however, you begin to have so much thermal energy that water boils, chemical bonds break, and the basic molecules of live fall apart. High pressure can prevent the water from boiling at temperatures above 100 celsius, but I don't think they've found [thermophiles](_URL_0_) that survive past 150 celsius.
A few things.  

1.  Water born illness is no joke for humans but different animals have different risk factor from bacteria.  So it in part depends on what bacteria are swimming around, and how they effect species.  

2.  We actually develop immunity to our local environment.  Ever wonder why the locals can drink the water and not get sick but a traveler comes and they can't handle it.  Well we attenuate to our conditions.  That being said the world has gotten a lot more safe based on improvements to water quality over time in many areas.  So while this is true for some water and some disease there are certainly water contaminants that will make you sick regardless of how local you are.

3.  For non pathogen based water contamination the amount of body mass and surface area goes along way in determining how that contamination will effect different organisms.  We have a  different body mass than an elephant and different gastrointestinal sensitives as well....so that's a really long was of saying that some animals bigger, and there are a lot of biological differences between us and other animals.  The difference will depend on the contamination and the animal type.

4.  Assess how well you know whether an animal might get sick.  I think its fairly probable if you see a deer or elephant drink from a murky steam you're not following it around to see if gets sick.  If people get sick after drinking bad water they will tell you; you might hear about it in the news if it makes a lot of people sick.  Short of a mass kill off of another animal type its probably not making headlines when animals have bad reactions.
It would be more protected, yes, but we keep our seeing, hearing, smelling, and tasting organs in our heads. If our brains were in our torsos, our senses of sight, smell, and hearing would be much less efficient (because the nerve signals would have to travel farther along pathways), and this would make us less able to survive. It's better to see a predator and run away before being attacked than it is to be attacked at all, even if your brain is better protected.
Muscles are filled with actin and myosin filaments that do the actual work of contraction. After exercise, the muscle cell releases signals that promote the synthesis of new actin and myosin filaments. This may partially be stimulated by breakage of existing filaments. Either way, the muscle cell ends up with more filaments than before, and grows to accommodate them. This is known as hypertrophy.

  


In some cases you might also get hyperplasia, in which muscle cells divide to produce new cells. But most of the time, increased bulk is primarily due to hypertrophy.
You won't directly need quantum mechanics, but the electromagnetic fields and currents in the plasma, and fluid dynamics are significant to the behaviour of the plasma. You will need to cope with magnetohydrodynamics, which is absolutely not trivial, and you probably will have to invest an enormous amount of work into learning how to solve MHD problems. These things are usually calculated on massive clusters. Maybe look into an introduction course about numerical MHD, there you'll also see easier toy problems.

You will not be able to do a fluid calculation as a pure N body simulation, the particle counts are way too high. You need to use continuum mechanics to get rid of individual particles. From there you could develop a lagrangian kernel approximation, like SPH, to get back to "particles". This is often done in astrophysics calculations that span a wide range of densities. For plasma in a reactor, you'll want a finite volume scheme though.

Edit:
You can take a look at [this overview](_URL_0_) about MHD SPH. It also comes with an example code. As I've said, I'm not sure that this is usually done for a confined plasma, but that's essentially the n body sim you are looking for. It's certainly doable, but it took me quite some time during my master's to program something like this.
A completely silent room is not really silent, and the recording system will introduce noise anyway. But let's assume you want an artificially generated file with complete silence.
Then it depends on how the data is encoded. For an uncompressed PCM format you get the same file size regardless of content. So 30 minutes of silence will be the same as 30 minutes of music as long as all other factors (metadata) are the same.
With a compressed file the situation changes. It will be smaller. How much smaller depends on a lot of factors such as what codec you use, what settings you use such as bitrate.

In the end you'll never get a 0 sized file because every file except raw PCM has a header detailing what format it is and how to read it.
Increasing access to abortion and birth control - eliminating this access increased fertility among low-economic status women in Romania in the 1990s ([see study](_URL_0_))

Sex ed, free condom distribution, low cost or free birth control and morning after availability - those are the keys to limiting limitless reproduction among any economic class, but specifically the lower class that doesn't have access otherwise
> Two elements that glow or fluoresce are proteins and the alkaline salts.

 > Why do some components glow or fluoresce and not others? This has to due with bonds between carbon atoms and other atoms in the molecule. When there is a molecule with alternating single and double bonds, called a conjugated bond, some of the electrons are able to move between shells or energy levels. They absorb energy in the form of UV light which raise the electron to a higher energy level. When the electron falls back to the original energy level the energy is emitted as visible light.

[source](_URL_0_)
There's an entire field of medicine devoted to trauma management. Probably without treatment a bullet to the liver or stomach or bowel would have been fatal. Today a lot of things can be fixed if you have time. Stop bleeding before they bleed to death, prevent infection and multi-organ failure, keep intracranial pressure under control. 

It's true that bullets cause some extra damage due to their velocity but it depends on the bullet and it exists on a continuum. Knives and crossbow bolts generally do less damage, high explosive shrapnel can do significantly more. Your chances of survival depend on the projectile, where it hits, as well as your own physiology and luck.
I'm not happy with the other answers here. I think even without math and a "deep understanding of physics" you can still understand how magnets and magnetism work generally. I'll try to conceptually explain some of magnetism for you without any equations.

Let's first discuss magnetic fields. Like electric field and gravitational fields, some things in our universe produce magnetic fields. This alters the space around them in such a way that certain other objects can under certain circumstances will "feel" the magnetic field of the first object and react to it.

OK, so what produces a magnetic field? 

In classical (pre quantum mechanics) physics, only *moving electric charges* produce magnetic fields. This means if you have something like a ball with some static charge on it or an electron (which as you know is negatively charged), or anything else that is charged and it is moving, then a magnetic field will be produced. 

Quantum mechanically some subatomic particles have a property called "spin". This spin, as well as the angular momentum of electrons around the nucleus of atoms gives rise to some atoms having a magnetic moment. Picture a magnetic moment as a small arrow that is a source of magnetic fields and also reacts to magnetic fields around it.

Ok so how do objects react to magnetic fields?

The Lorenz force law tells us that moving electrical charges will feel a force from magnetic fields. I won't say much more about that because you need math and the cross product to explain the direction of that force.

The magnetic moments that I mentioned earlier (which I described as small arrows) also react to magnetic fields by trying to align themselves along the direction of the field. A compass needle is a good example, the material it is made out of is ferromagnetic, which means the whole needle has a magnetic moment. A compass works because this large magnetic moment feels the magnetic field being generated by the earth (or a larger magnet nearby) and turns to point it's arrow along the magnetic field. Note that magnet moments attract one another because it is energetically favorable for them to be aligned along a strong magnetic field and since magnetic fields are stronger closer to their source, magnetic moments will be in a "better" and "better" state the closer they get to one another if their alignment is correct.

Since I mentioned ferromagnets I'll talk about them here. Some materials like Iron, Nickle and Cobalt are ferromagnetic. This means that all the little magnetic moments of their atoms like to spontaneously align themselves in the same direction as their neighbors. In some cases this gives rise to a macroscopic object (like the permanent magnets you are familiar with) having a magnetic moment. This will cause it to attract or repel other ferromagnetics depending on how they are aligned. 

There's lots more to learn about what I've said and beyond what I've said. If you are interested feel free to ask me more specific questions.
One of the direct ways of observing the vacuum is via the so-called ['Casimir Effect'](_URL_2_).  When two metal plates are placed close together they prevent long-wavelength (low-energy) virtual particles from being created in the space between them, so there are more virtual particles outside the plates and this creates a force pushing the plates together.  (The shorter wavelength, higher energy particles balance on both sides and don't contribute to the force, like air pressure pushing on both sides of an object.)

There are some processes in nature that can turn virtual particles from the quantum vacuum into real particles.  One such process is known as [Hawking radiation](_URL_0_) and results when one particle of a virtual particle-antiparticle pair falls into a black hole, leaving the other member of the pair flying off as a real particle.  While conjectured, this has not yet been observed.

Another such process is called the dynamical Casimir effect; it is an analogue of the Casimir effect described above and involves shaking a mirror at relativistic speeds to convert vacuum photons into real photons.  Observation of this effect was [reported recently](_URL_1_) in an experiment that used a superconducting cavity whose electrical length was changed rapidly, rather than shaking a mirror.
Through hormone signaling. You can look up [phyllotaxis](_URL_1_) for more information. Although phyllotaxis usually refers to how leaves are arranged on a stem, the principle for branch formation is more or less the same. Basically there is a hormone signal at each branch point. The farther away you get from the branch, the weaker the signal. Once the cells stop sensing the signal they start branching. It should be noted that this "signal" can be either the presence or absence of the hormone (my cursory look at the literature seems to suggest that it might work differently in different plants, but the basic principle is the same).

If you want to watch a cutesy video explaining this with highlighters and plants, check out [this video](_URL_0_).
Doesn't looking in all directions and seeing the light that bounces off things mean the same thing?
If you want to know why 0! is defined as 1 you must understand that the factorial function is originally created for combinatoric purposes. n! is initially defined as the number of permutations of n objects, not in the multiplication form we know now. Considering that the number of permutations of 0 object is 1, 0! is conventionally defined as 1.

This definition is convenient for a number of combinatorial calculations, as the factorial function is used in the definition of a number of combinatorial operations, like nCr and nPr.

Here's an example: Consider that nCn is the number of combinations of choosing n objects from n objects, which obviously equals to 1. 

but using the definition of nCr, we have

nCn

= (n!)/(n!(n-n)!)

= (n!)/(n!0!)

= 1/0!

Note that we must have nCn = 1, defining 0! = 1 fits the bill.

TL; DR **0! is defined as 1 for convenience in combinatorial calculations**
Control rods are designed to absorb neutrons without emitting any as a result. By inserting the control rods further, more neutrons are absorbed, so there is less of a chance that neutrons from one fission event will hit another fuel rod and induce another fission event.

Neutrons are the most important factor in the criticality of a reactor, which is described by k effective, the number of fission events directly caused by the neutrons from one fission event. You ramp up the power output by having a k_eff greater than 1 (without going prompt supercritical). This means that for every fission event, there is more than 1 fission event caused as a result, so the reaction rate and therefore the power output increase exponentially. You shut down the power by having a k_eff less than 1, meaning that there are less fission events being caused than are occurring, so the reaction rate decays exponentially. k_eff of 1 is critical, and will maintain your current power level.

In normal operation, the control rods are adjusted to maintain criticality. During shutdown, k_eff is reduced to some value that is less than 1 by the shutdown margin. For example, if the shutdown margin is 2%, the k_eff of the reactor when the control rods are fully inserted is less than or equal to 0.98.

It's worth noting that you don't increase the power by pulling out the control rods and leaving them there. You pull out the control rods enough to go supercritical until you reach the desired power level, and then insert them again to the point where the reactor is critical. You also have to be careful not to pull the control rods out too far. You want a k_eff greater than 1, but the k for prompt neutrons needs to stay subcritical. Delayed neutrons will add to the k_eff to allow the power to increase exponentially on the timescale of several seconds, but if prompt neutrons are supercritical the power will increase exponentially on the timescale of nanoseconds, which causes catastrophic incidents like SL-1 or Chernobyl.
Yes, mechanical strain affects the resistance of a conductor three ways.  
The resistance of a wire is (rho)L/A, where rho is the conductivity of the metal, L is length and A is cross-sectional area. If you stretch the wire, L goes up, A goes down, and rho goes up because the molecular bonds are being strained. The effect of rho changing is small in metals, but much larger in semiconductors.  
All three of these effects, taken together, are why we can measure stress and strain on objects with strain gages -- a subject well worth googling.
You got an answer [yesterday](_URL_0_), and indicated in a comment that it was the answer you were looking for. Are you looking for clarification or something else?
It is a result of one, they don't understand what the red dot is and believe it to be a prey animal. Two, their belief that this is a creature [kicks on their predatory instinct and they want to catch it](_URL_1_). In Florida, where I live, my dog and cat love to go out into to our back pool patio. Occasionally they will catch or chase these tiny lizards, called anoles, around. They chase he lizards around for he same reason they chase the lights.

Edit: so I did a lot more [research](_URL_0_) and found a better explanation to this. Whilst trying to find more good sites, I ran into something. I was unaware that laser pointers may have a negative affect on your pets health. The first link will give a good explanation.
Peak oil is a very simplistic view.  Ever since we've been extracting oil, we have been stopping extraction when the yields drop below what is ecomically viable. There might still be millions of barrels in a reserve, but if the daily yield drops below the cost of operating the rig, the operation shuts down and moves on.  Now, when the oil price rises it can be econimcally viable to go back and extract that lower yield material (or at elast some of it).  This same is true for pretty much every natural resource in the world (the same has happened recently with the tin mining industry in Cornwall, UK, which has been uneconomical for almost a century, but which is now feasible again due to rising commodity prices).

So yes, we are running out of oil. However, as prices go up, it caps usage, and also enables extraction of otherwise abandoned resource. Now, we didn't used to use oil, we are now using oil at rate X, and at some point we will run out and usage will drop back to 0. So there *will* at some point be a maximum usage. however, that curve is going to be a lot broader than the 'we'll run out in 50 years' crowd were concerned about 20 years ago.

As far as abiogenic sources - it's crank nonsense. There are billions of dollars expended every year in developing our petroleum exploration skills and there has not been a case which could not be explained by biogenic fossil fuel development. The abiogenic system has so many holes in it I struggle to know where to begin in pulling it apart.  For starters petrochemical products break down at high temperatures, yet they are suggesting it is generated in the mantle (1200 degrees+). However, we know if source rocks get too hot, they no longer produce hydrocarbons.  And that's before we start getting to the idea of how and why this material is rising into high level crustal storage, rather than getting trapped in lower strata.  Or why the release of this material from the mantle is not directly coupled to magmatism (where's my oil volcano?).
As it would turn out, the observable universe is actually 92 billion light years across, 46 billion out in all directions. This is because space itself is expanding faster than light. Things can not move through space faster than light, but space itself can expand fast enough that a 13.5 billion old universe is much wider than 13.5 billion light years.

Since the expansion of space appears to be accelerating that means galaxies are moving away faster than light can propagate thus reducing the total amount of matter we can see. That however doesn't mean there isn't more matter outside of what we can observe, but rather it is just outside of the *observable* universe.
Withdrawal and addiction are to completely separate phenomena. Many drugs cause significant withdrawal and are not addictive (corticosteroids), and many addictive drugs only cause minor withdrawal symptoms (nicotine).

Those two lectures take quite a different view on addiction. I'm much more in the camp of Marc Lewis, but the Kurzgesagt video has some important ideas in it too. However, I believe the Kurzgesagt video simplifies matters quite a bit. In general, rats in "rat park" like conditions, i.e. "environmental and social enrichment" do self administer drugs less, but the certainly still self administer them, and sometimes in amounts very close to isolated animals. It depends somewhat on the drug, but a lot on the manner of self administration. Intravenous self administration is much more potent.

I believe the evidence is that certain drugs drugs "hijack the reward system" (some people hate that metaphor, but I think it's okay). The reward system is a natural set of brain structures and neutransmitters designed to allow you to solve a particular set of problems which can be summed up as "how do I predict what behaviour will get me the biggest reward?". Specifically, there are two related but separate problems. The first is, imagine you are exploring a new environment, and are poking and prodding tree trunks and flowers, and all of a sudden a tree unfurls its leaves, and deposits a fruit at your feet. Did you do that with your poking and prodding? It is a difficult problem because your action preceded the reward. i.e. when you were performing your action, you had no idea what effect was coming. The reward system seems somehow to strengthen synapses that were previously active prior to a reward. And by doing so, increase the chance of performing that behaviour again.

The other related problem is about reward expectation. If you have a button that will give you 1 pound of food, but only every 100 presses, or another button that will give you an ounce of food, but every 10th press, which button should you push? You can do math to solve this, but most animals can't. The reward system seems to be about making sure that your expectation of reward is correct, and allows you to intrinsically solve problems like this.

Addiction seems to mess with this system. It activates the reward system, and makes you more likely to use the addictive drug, or perform the addicting behaviour. That is to say, it leads to drug taking becoming compulsive.

Withdrawal on the other hand is simply what happens when you body adapts to certain drugs, and then behaves poorly when those drugs are taken away. High dose corticosteroids, which are anti-inflammatories, can be lethal when rapidly stopped suddenly. But no one breaks into cars to fund the corticosteroid addiction. On the other hand, nicotine, while it does have withdrawal symptoms, they are so mild that people can hold down a job, and no one can tell. Sure, it is possible that in certain cases, withdrawal symptomes are the "straw that breaks the camel's back" in terms of people relapsing. But it isn't the root cause of addiction.
In men, LH/FSH stimulate androgen production in the testes. Androgens then affect the kidneys and the formation of erythropoietin. They also affect the bone marrow's response to erythropoietin (it makes signals to produce more RBCs and then makes sure the receiver is efficient and effective in receiving that signal).
It's actually pretty complicated, and depends on the type of formula you want. There are literally multiple books put out by international agencies about exactly how to name compounds.

For simple compounds such as CO2 or H2O or NaCl, they're ordered in (roughly) order of electronegativity within each column of the periodic table. Electronegativity is how much the atom attracts electrons, and is based on the size of the atom, the positive charge in the nucleus, and a few other factors. Electronegativity generally increases left-- > right and down-- > up on the periodic table. I say "roughly" because there are some oddities, like hydrogen being shoved in between nitrogen and oxygen so we can keep "H2O" and "NH3", since those formulas were well-known before everything was standardized.

For more complex compounds it starts to be even more complicated. C3H8 is the formula for [propane](_URL_1_), but you could write it as CH3CH2CH3, which gives you a better idea of how the molecule actually looks. When you get to much more complicated molecules like [sucrose](_URL_0_), you can say it's C12H22O11, but that's not particularly helpful if you want to know the structure. At this point, simple formulae break down and you have to start using organic molecule nomenclature if you want to be specific about it, which leads to ungodly names like (2R,3R,4S,5S,6R)-2-[(2S,3S,4S,5R)-3,4-dihydroxy-2,5-bis(hydroxymethyl)oxolan-2-yl]oxy-6-(hydroxymethyl)oxane-3,4,5-triol. There are a bunch of really complicated rules for this kind of nomenclature, which every organic chemistry student can tell you are a huge, huge pain to learn.

Of course, no chemist would ever call it that except under extremely specific circumstances. Most of the time, even in technical work, they'd refer to it as sucrose.
New organism, as in a new species?  As for sexually reproducing species, I know one rule of thumb is that a member of one species cannot produce viable offspring with a member of a different species.  There are some exceptions, such as hybrids (Horse + Donkey = Mule).  However, normally the offspring of hybrid species cannot reproduce, meaning the parents of the hybrids have to have been different species.  
So as for sexually reproducing organisms go, if there have been enough mutations in the genome to prevent an organism from being able to produce offspring with what seems like a member of the same species, then they are now in actuality different species from each other.  Not sure about asexual/single-celled organisms though.
This is an eyes-open condition, but it may help your understanding. In aviation medicine / human factors, we're taught to avoid  “empty-field myopia". When you're looking out the cockpit to a low-contrast scene, in the absence of something to focus on, the eyes tend to relax to a near-sighted focus point that lies anywhere from 80cm to a few metres ahead. The short focus can delay recognition of traffic or hazards and has been speculated as being a contributory factor in a number of incidents.

_URL_0_
No, Earth's gravitational field is basically a [monopole](_URL_0_), pointing inwards everywhere, whereas the magnetic field is largely a [dipole](_URL_1_), sort of the shape of the surface of an apple running from pole to pole.
To understand what you see in these kinds of pictures first you need a rough idea how the actual detector looks like and how your picture relates to the detector.
Take a look at this ATLAS schematic (the detector that your event display comes from): _URL_0_
The accelerated proton beams enter the detector from the left and right and collide in the very center. All new particles creates in the collision move outwards and are detected in the various detector systems of ATLAS.


Each detector subsystem has a different task. The parts labelled "Inner detector" are designed to interfere the least possible with the created particles and just record their paths as exact as possible.
For this basically huge amount of digital camera sensors are placed around the collision point. Everytime a charged particle passes through the sensor plane, the corresponding pixel lights up and can be read out like a photo afterwards. (To be less incorrect: only rather small parts of the inner detector are silicon pixel sensors, but the analogy stays more or less accurate)

The next outer part, the "Calorimeters" are designed to stop particles and measure their energy (which makes more sense than measuring their speed, as everything moves ~at the speed of light anyway) from the radiation emitted during the stopping process. Most particles are stopped in the calorimeter system, which consists mostly of large amounts of heavy materials (copper and steel for ATLAS).

Even further outwards the muon system is placed. Muons are the "heavy brothers" of electrons, and as they are 200x heavier they are relatively unaffected by material they fly through. Thus muons are (usually) not stopped by the calorimeters and just fly through them. This is why some layers of detection material are usually place around the outer parts of a detector to identify muons as such. In the ATLAS schematic this muon identification system is labelled "RPCs" and "MDTs" (resistive plate chambers and monitored drift tubes for the technologies used in them).


Now that you understand the rough structure of the experiment, let's apply this to your event display picture. Your picture shows the detector in the direction of the beam. Also most of your picture actually shows the inner detector part, as this carries most of the information (as in most of the data points).

Each of the white/red/yellow dots in your picture is one pixel stating "a particle went through here". Now after the event raw data is recorded, powerful algorithms have to reconstruct most the likely particle paths from these thousands of fired pixels. These reconstructed paths (usually called "tracks" in particle physics) are shown as orange/red lines in your picture.

The yellow bars around the inner detector are the calorimeter energy measurements for the corresponding positions. Longer bar means more energy deposited. As the calorimeter system is not nearly as granular (divided into separate cells) as the inner detector, the positional information is much rougher. As you can see high energy calorimeter depositions usually have lots of reconstructed tracks leading to them. These are very common occurences called "jets".

The muon system is not even shown in your picture, but the information from it is included. You can see, that the red lines go through the calorimeters and out of the picture, so they are detected in the muon system. Practically only muons show this behaviour, so such tracks are identified as muons and colored red. From the rather low amount of bending in the muon tracks you can tell, that the muons have very high momentum (they carry lots of energy).


In conclusion: you are looking at an event in which at least four high energetic muons ("hard muons") + 2-4 jets (that's hard to tell from this one picture) are created.
The 1859 x-ray event was exceptional but it isn't peerless even in modern times. 

Rather than mangle the summarization you should refer to Leif Svalgaard and E. W. Cliver's paper, "[The 1859 Solar-Terrestrial Disturbance and the Current Limits of Extreme Space Weather Activity](_URL_0_)" published in Solar Physics in 2004. It'll give the gist of how the 1859 event compares to modern events and their recorded effects.

  > Abstract: It is generally appreciated that the September 1859 solar–terrestrial disturbance, the first recognized space weather event, was  exceptionally large. How large and how exceptional? To answer these questions, we compiled rank order lists of the various measures of solar-induced disturbance for events from 1859 to the present. The parameters considered included: magnetic crochet amplitude, solar energetic proton fluence (McCracken et al., 2001a), Sun–Earth disturbance transit time, geomagnetic storm intensity, and low-latitude auroral extent. While the 1859 event has close rivals or superiors
in each of the above categories of space weather activity, it is the only documented event of the last ∼150 years that appears at or near the top of all of the lists. Taken together, the top-ranking events in each of the disturbance categories comprise a set of benchmarks for extreme space weather activity.
All digital watches contain a tiny metal can with a special quartz crystal tuning fork inside, that crystal is calibrated by lasers during manufacture to vibrate at a precise frequency when you pass a current through it, usually 32,768 Hertz. The circuitry of the watch senses and counts those vibrations of the crystal in order to keep track of time, every 32,768th "tick" of the crystal is one second. [You can watch a video explaining this more in depth here](_URL_0_).
Heterochromic individuals don't even maintain perfectly consistent colour patterns through life. I cannot imagine the chemical factors that give identical twins varied patterns of melanin in the iris would be eliminated by cloning.
> So my question is, if the ship is still moving at a great velocity through space (although enclosed in a bubble), why doesn't time dilation apply?

It's not moving at great velocity through space. Within the context of relativity, there are two ways for the distance between objects to change. One is for the positions of those objects (i.e., the set of numbers that specify their location in spacetime) to change. The other is for the distance to *just change*. This seems weird, and I give a fuller discussion on the matter [here](_URL_0_).

The statement that nothing can move faster than the speed of light is related to the first sort of distance change. Specifically, the rate at which an object's position changes relative to you cannot exceed the speed of light. The Alcubierre drive manipulates the *second* method of changing distances, and so is not constrained in that way. It's only at the very end of the "trip" that you have to make a short trip through space (change position), and that can be done at very low speeds provided you get yourself close with the drive.
Ok, you're basically asking two questions, and not one.  

To answer the first, basically its because that isn't how entropy works.

 > In any natural process there exists an inherent tendency towards the dissipation of useful energy.

That's what entropy is basically.  Energy spreading out from its source to lower energy destinations. It's not a rule about how things always become chaos or less complex.  If it was, the universe as we know it wouldn't exist.

To answer your second question, the reason the universe isn't filled with randomly spread out particles *anymore* is because of gravity.  In any *random* system, gravity will eventually pull them into clumps.  Only in a non-random system would this not happen.  Even the slightest missplaced atom would eventually ruin the layout because of gravity.  After the early days of the big bang, the universe was a random collection of energy and particles.  Gravity pulled this into clumps as things cooled down into matter.

Those clumps eventually became stars when their mass reached a certain point.  And then they would explode.  And later those particles became planets and other stars once they accumulated enough again.

Stephen Hawking has a really cool miniseries on netflix that explains this very well, a lot better than I'm able to anyway.
We currently do not know. And unless someone develops the necessary technology for time travel, we most likely never will. The furthest back we have reconstructed human languages is only about 10 000 years, and even then we're in rather speculative territory. Anything going further back than that is mostly just guesses based on wishful thinking.
You have learned that there are three phases of matter: solid, liquid, and gas. However, these phases are not absolute. If so, how do puddles evaporate? The temperature of a puddle never reaches boiling temp, so how does the water turn into gas?

The answer is that whatever phase of matter your object is in, it is always an equilibrium. Individual water molecules at the surface of the liquid will gain enough energy to break away and into the gas phase. The same concept can be applied to pretty much every solid or liquid; small molecules of the object in question (or the more volatile parts) will be in the air, and that is what you smell.

So next time you're in the bathroom and you smell a little poo, that's a few poo molecules floating into your nose. Cheers! :)
In this case, it's better to think of color as a result of light absorption and emission, rather than reflection. When light hits a gas, it can be absorbed by various processes. Visible light just happens to be the right energy to excite the electrons bound to atomic nuclei in some molecules, such as those making up chlorine gas. These excited electrons, which have been given energy by a photon, then relax to their original energies, giving off new photons of a particular wavelength (and therefore color).

Electrons are unusual in that, due to quantum effects, they can have only certain discrete energies. This is determined by the structure and composition of the atom, and its interactions with other nearby atoms. Gasses that are not colored do not have electron excitation mechanisms of the correct energy to be excited by visible light, then give off light of a specific color.

If a gas were black, it would have to absorb most incoming photons, then give off accumulated energy as something other than visible light, such as photons of a wavelength that cannot be observed by the human eye.
There is a genetic role in some forms of mental illness like schizophrenia for example. There's also a lot of environmental factors that cause or worsen mental illness, like physical/emotional abuse/neglect, malnutrition, traumatic life events, etc. There are usually many factors at play and no case is exactly the same.
You might want to try this query at /r/asksocialscience
Perhaps I am misreading your question, but you seem to make it more complicated then necessary. 

Take two conductors which are isolated from each other. If one has more electric charge than the other, there is now a *potential* that the charge can move to the lower charged conductor, if they were to be connected by for instance a wire. That is really all that is meant by potential or potential difference.

So a potential is set up when one conductor has more charge than another. Note that both may have a total charge excess, the difference is really what matters. The 0V can be chosen freely. However, it is common that one part of the circuitry is grounded, meaning formally it is coupled to an infinite container of charges so that it will equilibrate with that container, and then that will be said to be at 0V. 

A common way to create a voltage difference is to put a battery between the grounded part and the other one. The other one now has an excess of charge that wants to flow to the grounded part, and this flow of charge can be used to drive a motor or heat things or whatever.

In your example, the conductor at +3.3volts will have a depletion of electrons with respect to 0V. It is not useful to talk about individual atoms in conductors; instead the outer electrons of atoms are rather free to move around the whole material. Under a force (like from the battery) they can even be extracted from the material.
4-5 hours. The half life is only in regards to the metabolic half life inside the body. A lot of enzymes break down caffeine into other compounds and this process takes time. Some is also expelled through the urine. 

Natural degrading of compounds does not happen in the same way. It is a totally different process and it usually takes way longer depending on the enviroment. A dry coffee bean will not lose its caffeine so easily. A wet one however would be subject to bacterial processes. Or one in a very hot place could destroy the caffeine gradually though heat. Or pure caffeine in sunlight could slowly break down the molecular bounds. Some compounds can resist enviromental hazards better than others and some break down quite quickly if they are not kept in an ideal enviroment. Caffeine is on that side of compounds that can take a beating.
The standard DT fusion reaction is 

^(2)H + ^(3)H - >  ^(4)He + n.

So yes, an alpha particle is in the final state. An alpha particle is a ^(4)He nucleus.
My guess would be [inductive charging,](_URL_0_) but I don't have one of those toothbrushes so I don't know.
Yes, learned language improves after sleep. It sounds like you're asking whether consolidation is improved if there is only a short delay between and sleep.

For some types of memory, particularly hippocampal dependent learning, consolidation is better when there's a short delay before sleep rather than a long delay. That line of research can be complicated, however, by the possibility of memory interference during a longer period of wakefulness. In other words, you are still experiencing/learning new things during the time between learning and sleep, which could have a detrimental effect on the memories you are wanting to preserve/improve. Studies of motor-learning tasks, which are not likely to be affected by interference, have shown no significant difference between short and long delays between learning and sleep.

Now language learning is not really hippocampal dependent, nor is it motor-learning. While studies have shown improvement across sleep, they have not directly tested whether the length of delay between learning and sleep makes a difference. A study in which participants learned pseudo-words showed that participants who slept between learning and testing improved more than participants who remained awake. However, after  the group that had initially remained awake was allowed to sleep, their performance was the same as those in the other group ([source](_URL_0_)). So this suggests sleep will benefit your memory, regardless of the delay between learning and sleep. However, the caveat is that participants in this study were just tested to see if they recognized the words they had learned. They didn't learn any meaning to the words, grammar, etc.

So all of that leads to the following underwhelming answer to your question: maybe.

We definitely know that, independent of when you study, sleeping will help, so getting regular and sufficient sleep is a good idea. So is spending more time studying.
The invariant ("true") mass (_URL_0_) of an object does not increase with velocity - what changes is the relation of momentum and energy to the rest mass.  This is actually a very important distinction to make, as in your scenario, confusing the invariant mass with the apparent mass would result in contradictory results depending on what frame you are in, which cannot happen.  I would also suggest reading _URL_1_ for a good explanation of this distinction.
Your hypothesis is correct. Alcohol and water bend light differently, and having an incomplete mixture of the two causes certain areas to refract light one way and other areas in another. The result is the borders between areas become very apparent.
Okay, it works like this:

The two primary gonadotropins involved are follicle-stimulating hormone (FSH) and luteinizing hormone (LH). These come from the pituitary gland. FSH causes a follicle to mature and produce an ovum. During this maturation process, the follicle produces estradiol, which at low levels suppresses the release of LH. When the follicle is mature, high levels of estradiol instead turn on a big rush of LH, and it stimulates ovulation. The ovum makes its way into the fallopian tube, and all is groovy.

Meanwhile, the follicle it left behind turns into the corpus luteum. That body starts pumping out progesterone, which suppresses the production of LH and FSH. Without these two, the corpus luteum starts to fall apart, *unless* fertilization occurs. The fertilized egg releases human chorionic gonadotropin (hCG, this is what pregnancy tests detect), which stabilizes the corpus luteum so that it continues to produce progesterone, thickening the wall of the uterus - and also, incidentally, suppressing further ovulation, since FSH and LH will be suppressed.

Now, what happens with the pill? Well, the body is flooded with progesterone, which suppresses the production of FSH and LH, thus preventing ovulation entirely. No corpus luteum forms, because no ovulation occurs. The uterine lining is thickened as if you were pregnant.

I'm not sure which hormones are responsible for the effects you've associated with ovulation (other people might not be so fond of the effects it has on their breasts), but my guess would be LH - the timing is right and the effect is the most profound. It seems pretty difficult to imagine, though, that your ovaries are going to be too happy if they're receiving the signal to ovulate well before the follicle is mature. Seems like a bad idea to me.
No.  Not even the most detailed pathology work-up would be able to turn up anything.
Emotions such as sadness and happiness are not triggered by separate neurons, but rather, areas distributed around the brain cooperate in networks to produce and process emotions. You could call these [neural circuits](_URL_0_), or pathways. First, sensory networks relay information about the world further down the stream, much of which goes to the thalamus, in the limbic system. The limbic system is involved in the middle stages of the circuits, with the thalamus tagging sensory cues for emotional content ("Oh f***, something fast and hard is flying towards my head"), the hippocampus, which uses emotions to mark what is important to remember and in turn providing memories as context for emotions; the amygdala and hypothalamus are more directly involved in producing emotional states, while the prefrontal cortex deals with regulating emotion and interpreting them.

A specific emotion need not be characterized by the number of neurons firing, or their firing frequency, or what brain regions are active at the given moment. Exactly how emotions are encoded in the brain is not crystal-clear at this point, but it[ looks like](_URL_1_) saying that different regions of the brain are responsible for producing emotions, depending on whether they were positive or negative, is too simple. Closer to the true picture is something like an ensemble: different sections play different roles, but the concert is given by the whole orchestra together.
A massive ball of photons doesn't really make sense, but for individual photons it is called the [photon sphere](_URL_0_).
**Short answer:** A little over 2 hours. 

**Long answer:** This just boils down to conservation of energy, so a fan won't really matter. You're a heat source which is dumping heat into the air in the room, like a lightbulb or an oven.

Since people eat about 2,000 Calories per day, that tells me we use about 8.4 million Joules each day. Since all that energy eventually goes to heat, I'll call the human power output over a day (86400 seconds): 

    8.4e6 Joules / 8.64e4 s = 97 W

which is very nearly 100 Watts, and very close to a lightbulb!

I won't bore you with the thermal physics in extreme detail, but I know the specific heat capacity of air is about [1 Joule/(gram Kelvin).](_URL_1_) The thermal energy needed to heat the room from 20 C to 30 C is about [800,000 Joules.](_URL_0_)

Dividing by the human power output we find:

    800,000 Joules / 100 Watts = 8000 seconds

That's a little more than 2 hours. 

Since the human body temperature is about 36 C I doubt you could heat the room much higher, and I fully suspect that the heat would kill you pretty quickly as the room comes into thermal equilibrium with your core temperature. 

Don't try this at home.
It's difficult to answer this question. Naturally, the ultimate answer is "we don't know. We build QFTs under the assumption that they are causal since every QFT that we know to be realized in nature turns out to be causal."

If these are force fields, then consistency automatically forces the propagation speed to be 1, otherwise you have negative energy modes that make no sense. If they are bosonic matter fields, then you can try to construct backgrounds where the speed of propagation is big. Some people claim that they can write consistent theories with superluminal propagation, as long as there is a maximum speed, but there is no way that you can write a theory with instantaneous propagation that can be consistently quantized.  

Notice that the collapse of the wave-function is a completely different problem, since there is no propagation of information in the collapse.
A couple of things, the wireless access points should be on different channels, which puts them on different frequencies, so they won't interfere with each other. That said, there is a concept called "Adjacent Channel Interference" so they would actually interfere with each other, if they're side-by-side, but otherwise if they're at opposite ends of the house, it's not goign to cause it.

Anyhow, the point is that they shouldn't be on the same set of frequencies. The challenge with 802.11 on the 2.4GHz band, though, is that there are only 3 channels that don't overlap (Channels 1,6, and 11), so in a large deployment it's tricky to lay out. 5.8GHz has a lot more non-overlapping channels, so it's quite a bit easier.
> All the videos say is that things will go a lot faster since you can compute everything at the same time. Sure, that might be true, but the computation will be worthless if you can’t present the result in an understandable way, and it will be even more worthless if it isn’t even the answer to the question you asked.

You've correctly identified the reason why the "do everything at once" explanation is misleading to the point of being useless. If you actually tried to do an operation on all possibilities at once, you would not be able to read the answers because what you'd get out is a random answer, not all answers. The way to make a useful algorithm is to make the correct answer have a very high probability while the incorrect answers have low probability, and to do this you need a specific way of mixing the amplitudes so that the correct ones interfere constructively and the incorrect ones interfere destructively. This means your problem has to have some kind of special structure that allows you to manipulate the correct and incorrect answers in this special way, and this isn't always possible or easy to do. 

But you don't *have* to start out with a superposition. If all you want is to add 1+2, then you'd proceed just like a classical computer and have 1 and 2 encoded in bits, and then apply basic logic gates to implement the "add" operation. The only difference is that quantum computers only have reversible logic gates, which means the input has to be in one-to-one correspondence with the output, e.g. 3+4 has to have a different answer than 2+5. The way to do this is to keep around extra bits that let you reconstruct what the input was, which you then ignore when you read the answer. For addition this means you'd set it up so that 3+4=(7,3) while 2+5=(7,2). The answer you want is 7, but you also remember the first input number so that you can still figure out the inputs if you wanted. In doing this you wouldn't have really taken advantage of the strengths of quantum computing, but at least it means that anything you can do with a classical computer can also be done with a quantum computer.

The way you program the computer is you list the series of quantum logic gates that get applied to your initial qubits. The basic logic gates are things like AND, OR, and XOR but modified to be reversible, and there's also new gates like Hadamard which take a qubit in a 1 or 0 state and turn it into a superposition state, and gates that entangle or disentangle several qubits or change the phases of their amplitudes. A step of quantum computing is always a simple unitary transformation, which basically means that it conserves probabilities, respects superposition, and is reversible. 

[This wiki page](_URL_0_) will give you some idea what it looks like.
Yes, there has been a lot of research into the interaction of nicotine and alcohol.  I don't have time for a detailed review, but some of the research has shown that nicotine may counteract the sedating effects of alcohol (e.g., making you sleepy) and increase cognitive abilities (e.g., focus, energy, etc) while compounding the pleasurable/euphoric effects of both substances.
As this question is about you, personally and your physiological experiences, this is medical advice and against /r/askscience policy. Please go see a doctor.
It's a way of representing a bounded function in terms of the trigonometric polynomials of order less than or equal to n. If you choose a finite n, then what you have is the nth Fourier approximation. 

Bear with me as I step back a little to flesh out my analogy. Now, say you want to find the best possible approximation of a line L onto a plane P in 3D. What you have to find is the *projection* of L onto P. But also say you don't just want to project lines on planes in R^3. Could you generalize this concept  to functions? The answer is yes, but you'll need many of the same constructs you used to project lines onto planes.

Namely, the idea of an orthonormal basis and an inner product. If you think of a function space as sort of analogous to 3D space, then each orthonormal basis function can be considered analogous to each of the 3D coordinate axis vectors that have been shortened to be of length 1. Just as you define every other 3D vector in terms of some 3D basis, you can define every trig polynomial in terms of its orthonormal basis functions. Furthermore, that inner product that you've implicitly used all along to define the concepts of length, angle, and orthogonality must be generalized to your chosen function space as well. 

Now you've got all these things to project functions onto other function spaces, you're probably asking yourself...why? Perhaps because the function space you're projecting onto has *special properties*. This is the entire reason the space of trigonometric polynomials ( T^n ) was chosen for the Fourier series. They've got a  bunch of easily found orthonormal basis functions, *and* each of these orthonormal basis functions corresponds to a different frequency. Project a function *f* from C[-pi, pi] onto T^n (which happens to be a subspace of C[-pi, pi]) and you get the best approximation of *f* in T^n. But as a direct consequence, you've also split out *f* into its constituent frequencies.
They do have tongues, and they do have tastebuds, but not on their tongues.

[Source](_URL_0_)
Homo sapiens is a single species because members of any two populations can breed without difficulty (and typically don't need much encouragement). That's relevant to a species definition, physical characteristics are not. This is also why dogs (Canis familiaris) are a single species despite having **vastly** more individual differences than humans.

With the squirrel examples you gave, they're separate species because they're either physically incapable of making fertile hybrid offspring, or they would never meet naturally to do so.
No, Cu2+ has fewer electrons than Cu. Whenever electrons are removed from an atom its radius gets smaller. This happens because electrons push against each other so as electrons are removed the outer ones can move closer. If you want to think about orbitals, Cu has an electron in the 4s1 orbital and Cu2+ does not.
That depends on your definition of species... In your theoretical example, if two different organisms were genetically compatible enough to produce (edit: fertile) off-spring, it's debatable whether or not they were the same species to begin with.
Yes, though the more complex the thing you're burning is, the harder (I suspect) it would be to calculate. The color of a flame comes either from blackbody radiation, or from emission spectra of the molecules themselves, if the flame is hot enough to ionize them, and the absorption spectra. These can all be calculated, and I'm sure there's tables and tables of this stuff (I.e. emission and absorption bands) for most common materials. Probably someone who's in physical chemistry could give more specifics on the models they'd use, but that's the principle. 


For instance,  calculating the spectrum for hydrogen is something a second year undergraduate could do, though I think actually figuring out the likelihood of transitions would be a bit harder. But it can definitely be done.
Ethanol EtOH possesses a polar group in the hydroxyl and a non-polar group in the CH3. This amphipathic nature allows it to dissolve the polar phosphate group and the hydrophobic carbon chain on a phospholipid bilayer. Sugars, Glucose for example have several hydroxyl side chains which create a steric hinderance that block the hydrophobic interactions.   

70% EtOH is ideal for killing microbes, lower and it's not effective and higher and it can create lipid bubbles from dissolved membranes which protect other microbes. 

Sugars display a preservative effect because at high concentrations they create an osmotic pressure that kills bacteria, not because of a direct molecular interaction.
Certainly not.  In fact, there are debates about whether neutral drift is more important than natural selection as the main driver of evolution(I suspect most genetic changes are neutral, but quite a lot of _phenotypic_ changes are under selection one way or another).  Anyway, you can't really argue that neutral drift doesn't play a big role.

Aside from that, you could quibble about whether sexual selection should be included as a subset of natural selection or not.  Regardless, sexual selection plays a big role, and often favors the evolution of traits that decrease survival.  

Anyway, I don't really know of anyone who claims that natural selection is the _only_ driving force of evolution.  Even the hard-core selectionists would have to admit that neutral drift occurs, especially on a molecular level.
There isn't a specific threshold of the type you're asking about because there are too many free parameters.  The power of the signal measured on earth depends on the power and frequency distribution of the output at the source, the size, shape, and orientation of the detector on Earth, the sensitivity and selectivity of the amplification scheme on Earth, and the integration time (to name the major factors involved with whether a signal can be detected or not.)  Short of getting into a discussion about event horizons due to expansion of the universe (which are too large to be a useful point in your argument anyways) there isn't a way you can assign some fixed barrier in space beyond which we cannot detect radio signals.
Mathematically this comes from the fact that the fields (namely electric and magnetic) behave as a wave. 
So you can split the circular light in to linear waves, which are shifted, so when you add those two waves again and watch the field change over time it looks like the electric (or magnetic) field vector is spinning.
Here are some good illustrations:
_URL_3_

To create that sort of light you can take something called a waveplate (_URL_1_).
What actually happens is that linear light which enters the waveplate is split in two waves and one of those waves is shifted, therefore when they leave the waveplate they act like circular light.

Here some more graphics:
_URL_0_

_URL_2_;

Sorry, I didn't have much time. I hope it helps a little bit :)
To follow on from /u/AsAChemicalEngineer:

There are two factors to consider. In an electron collider (or other lepton collider, like the proposed muon collider) the collision is very simple and happens at one specific energy. If you collide two electrons through a range around the mass of some particle, you'll see an "enhancement" of that particle's production near the mass. For instance if you wanted a "Higgs factory" you'd build an electron collider with an energy of 126 GeV.

But LHC is a "hadron" collider. It collides nuclei together. Principally protons, but it can also do things like lead-lead collisions. The neat thing in hadronic collisions is that protons are "little bags of stuff" (nuclei are bags of bags of stuff). So the collisions are far more complicated. But it also means that you can scan an even wider energy range. Because some of the "stuff" within the bags will be moving with a sizeable amount of energy themselves. So your collisions can have a range of possible outcomes.

These are good for scanning for new particles (like finding the Higgs) because there are just so many *opportunities* to produce one. So as ChemEng mentioned, the LHC excels in luminosity, more so than the SSC would have. Which means our collisions of bags of stuff have many many more opportunities to create very rare reactions, and allow us to probe very rare processes.

We don't really have any interesting things at 40 TeV yet. Maybe we'd find some signal of supersymmetry or dark matter at higher energies. Having found the Higgs already, the standard model is fairly set at even the 7 TeV of the initial LHC run.
well this is quite a broad question, let me give you the answer for some viruses

for herpes we can control it and know how it's spread, can't currently stop it, but we're working towards it!

for chicken pox we've got a vaccine and can eradicate the virus in the next 50 years, so very very good

For influenza, we will probably never eradicate it. it's extremly good at mutating and sometimes can jump species, but only in the rarest of cases when conditions are perfect. so even if we eradicated it, the niche would be left open and we could get an interesting new bird flu. also the flu is constantly in humans, when the northern hemisphere has summer its infecting in the south and when it's the summer in the south it infects the north. vaccines for it are a combination of a geuss, statistics and the last known mutation of the virus, which is why they aren't 100% effective

HIV we're doing pretty good, havn't quite clamped it down, but we've got new treatments that can help those with AIDS


As for other viruses we're working towards them, viruses are just protein and code. they are very simple and don't need alot of working parts. so if they chop out half there code, they won't self destruct like our cells, they'll probably just infect better. But that means the field is always looking for new scientists!
We choose an arbitrary value of 'prepared' and go from there. For meat products especially, there's a certain amount of eyeballing that goes on as the proportion of lean and fat can vary (although not too much) between packages. For most people, the general nutrition facts panel will be correct - seriously, measure the weight of the grease you collect and you'll see it doesn't vary as much as you think - but if you have a preference for the extreme ends of the bacon doneness spectrum, and it's really important that you have accurate nutrition information, you'll have to look in specialized databases. So let's look at the [USDA Nutrient Database](_URL_0_) just for laughs. 

100g of raw pork bacon gives us 458kcal and 45g of fat. There are 40.2g of water in 100g of raw bacon, and only 12.2g in pan-fried bacon, so let's look at 72g of pan-fried bacon - 384kcal and 29.02g of fat, so you've only lost about 30% of the fat in the bacon. Changed to baked bacon (which supposedly reduces the fat) gives us 395kcal and 31.15g fat. Oops, scratch that folk wisdom!
No these are certainly not the only options, but the movie focused on these two. probably to make it more dramatic. 

One huge problem of modern physics is that we do not know why the Higgs is so light (hierarchy problem). Naively one would expect that quantum correctons would lead to a very high mass if there is no mechanism to stop this. The most popular one is supersymmetry. The radiative contribution of one particle is canceled by the contribution of the super-partner. The problem is that in supersymmetry thee Higgs must be relatively light ( <  130 GeV). The exclusion bound by LEP was around 115 GeV. So it could not be any lighter than this. The found 125 GeV still fits into many SUSY models, but it is already much harder to get. At 140 Susy would not be able to explain these things anymore.

One other ansatz to explain the lightness of the higgs mass is the anthropological principle. If there is a large (maybe even infinite) amount of universes, we would be in one of these where the Higgs is light, because life could have not evolved in a universe with a Planck mass higgs boson.
because it's such a weird concept to understand:
sure you can have one, two or even three apples...but to say one may have zero apples?! no, they don't have *any* apples. the idea of placing value on "nothing" is very abstract. if you have nothing, why bother naming it something other than nothing? but for arithmetic purposes, we must place a marker or name on that value so as to manipulate it. this development came once we started manipulating numbers which, i assume, came after we started counting.
Pyrogens act through a few possible endogenous and exogenous pathways, for example induction via interleukin-1/6 or tumor necrosis factor-α, but ultimately a prostaglandin (E2) is released into the blood stream. You can check out "Review: infection, fever, and exogenous and endogenous pyrogens" by Dinarello, C. A. (2004) if you're interested about the pathways.  

 It also interesting to note that NSAIDs such as ibuprofen and aspirin inhibit the cyclization of arachidonate into PGH2 (the basic prostaglandin from which others such as E2 are made) and later other inflammatory agents such as thromboxanes, which is why they reduce the effects of fever.  

Anyway, these prostaglandins travel to the hypothalamus and interact with specific receptors. By doing this, they decrease the activity of warm-sensitive neurons of the hypothalamus, the ones that are sensitive to warmth. It can also increase the activity of cold-sensitive neurons.   

The result of this is that the hypothalamus believes the body is colder than it actually is, and acts to restore the thermal homeostasis. The hypothalamus thus increases thermogenesis (production of heat) and also through the insulation of heat (constriction of peripheral vessels via norepinephrine, for example). Thermogenesis can occur through shivering, which is your muscles twitching which releases heat as a byproduct of this process, which is why you sometimes shiver when you have a fever. Heat is also produced via stimulation of brown adipose tissue.  

Thermogenesis via brown adipose tissue occurs in response to norepinephrine from the sympathetic nervous system (ultimately originating from the hypothalamus). Essentially the brown adipose tissue uses the energy (Proton motive force) it would originally use to make ATP in the mitochondria to make heat via an uncoupling protein-1, AKA thermogenin. If you're further interested in this, I would suggest reading (or skimming over if you have access) "Brown Adipose Tissue: Function and Physiological Significance" by Cannon and Nedergaard (2004).

Edit: grammar and a few changes.
If your sample is random, then yes. It would be quite easily to sample in such a way that does not preserve those properties. e.g. Imagine you have a normal distribution of mile times, and you end up sampling overweight people from that population. Your new sample will not have the same mean mile time.
In general, yes.

The higher altitudes have thinner air, and so less air resistance, allowing the plane to fly faster.  This can also save fuel.

Note that there is a limit though - above a certain altitude the engines don't work.  In the case of commercial air travel, the planes don't have any choice because they are flying in controlled space, doing what the controllers tell them to do.

Edit:  Just read the geometry part of the question.  No, higher altitudes are slightly further, because of the climb and, I suppose, the curvature of the earth.
The simplest answer is that those animals are physiologically set up for it (have evolved to ram into each other at speed as a mating display over millions and millions of years), while humans are not.

The actual adaptations generally include things like much thicker skulls, heavier neck musculature, all the things you'd expect, but the reason is really just that those animals have evolved to do that and have a physical setup that allows for it, while humans have not and do not.
Not scientific at all, but somewhat relevant personal experience.

I was in a tornado 4 or 5 years ago where water was seeping in sealed emergency exits. I know the pressure is worse in a tornado, but I'm guessing that if you had windows cracked you would have massive amounts of water in your house. Board up your windows.
Take a look at [this](_URL_0_) from Cambridge University; it is the curvature, not speed, that causes lift.

Edit: Link fixed.
You bet! 

The thermal energy can be converted into something other than mechanical energy (a turbine) in order to generate electricity. My first impression is that the thermoelectric effect will play a significant role, and I have had experience using Peltier coolers (using electricity to create a hot and cold surface) for refrigeration purposes. The effect you're looking for is likely the opposite of what I've had experience with.

Deep space probes will often have RTG's (Radioisotope thermoelectric generators) in order to provide power since the solar panels at great distances will provide negligible energy. 

I don't think the efficiency is perfect as of yet, but for low power situations it is not necessarily a bad choice.

I don't think people have had much experience 'catching' radioactive particles. If you think about it - alpha sources tend to get hot and stay hot due to the self shielding which is essentially like 'catching' it's own emitted radiation. Other forms of radiation (gamma, beta) do not deposit enough energy per unit distance ie: stopping power to make them worthwhile.

More info: 

[Thermoelectric Effect](_URL_1_)

[RTG's](_URL_0_)
Sexual reproduction randomly shuffles the genes of the mother and father, leading to overall greater diversity at the cost of slower reproduction speed. When some huge selection even comes along, organisms with greater diversity are more likely to survive than largely identical asexual organisms.

Early sexual organisms have a sort of half-assed mix between sexual and asexual reproduction. Yeast, for example, are haploid single-celled organisms that primarily reproduce by budding - just growing another individual off themselves. However, when stressed they form diploid spores used for sexual reproduction in order to rapidly adapt to whatever is stressing them. There is no concept of gender for them.

The obvious follow-up question is "why aren't we all hermaphrodites?" Splitting into two distinct sexes allows the sexes to be more specialized. In the case of humans, men are bigger, stronger, and more inclined to fight off sabertooth tigers. In the case of spiders, men are tiny useless little things that don't waste any valuable food. For most organisms with distinct genders and no asexual option, extremely rapid reproduction isn't that big a deal - there's not that much difference between doubling every 2 hours and every 24 for complex organisms.
First, you have to understand that the separate sexes are defined by the type of gamete they produce, megagametes (eggs) versus microgametes (sperm).

The null hypothesis is that sex would just involve gametes that are equivalent to one another. This would provide the advantages of sexual reproduction that others have pointed out, but would not involve different sexes. It's possible that that's how sexual reproduction first evolved, but the thing is there's a trade off between provisioning your gametes with enough resources that the fertilized zygote will have a decent chance at survival and simply producing lots and lots of gametes. Both of those are evolutionarily advantageous, but organisms can't optimize both of them simultaneously. You either have to sacrifice on resource provisioning per gamete or else you have to sacrifice on the number of gametes you produce.

So one possible explanation for two sexes with different types of gametes is that from a starting point of equivalent gametes and a sexual system that involved simply releasing gametes into the environment, which seems to be the common way it was done (and still is in lots of organisms), some individuals began to evolve a sort of cheating mechanism where they skimped on resource provisioning in order to produce more gametes. This was an advantage for them because more gametes produced meant they stood a better chance of reproducing. However, it would only work if their gametes encountered other gametes that were well provisioned. If they encountered the gametes of another individual who was using the same strategy, the fertilized zygote would be under-provisioned and probably die. So recognition systems evolved that allowed these smaller, more numerous gametes to only fertilize larger, more well-provisioned gametes. Once that mechanism evolves, it's a race to the bottom as the under-provisioned gamete strategy will solely optimize on producing more and more gametes that contribute fewer and fewer resources to the zygote. So we get microgametes. These tiny, under-provisioned gametes flooding the environment will in turn force the remaining individuals that don't optimize on producing large numbers to optimize instead on provisioning their gametes with sufficient resources to succeed even if they're fertilized by a microgamete, so they become megagametes.

There's still a question of why a mechanism to recognize and exclude microgametes from fertilizing the "normal" gametes didn't evolve. In fact, being fertilized by a microgamete might be beneficial since they may have dispersed further just because they're smaller and produced in larger numbers. That means a microgamete might contribute more genetic diversity than a more local "normal" gamete and that could help the fertilized zygote to survive. So there's an advantage to both sides of this and naytural selection will push them to extremes.

This is only one potential explanation, however. It's not a sure thing, but it is well supported by evolutionary models.
Luckily, someone has done a CTD (conductivity-temperature-depth) cast into the Marianas Trench down to 7 meters above the sea floor.  

_URL_0_

There is a paywall for this article but the abstract tells you what you'd like to know. Looks like salinity does increase with depth but there are some details...
Yes you would. Floating in water you are buoyant, but your inner ear and 'guts' still register gravity. In free fall, surrounded by water or not, you body will still detect weightlessness. Fascinating thought experiment tho.
See _URL_0_

We're basically a Type I civilization, and most likely looking at several thousand years until we get to Type II, and the ability to construct a Dyson sphere. At that point, cost will probably (hopefully) be a concept of the past.
Didn't this stuff give the people who applied it to watches all kinds of bad diseases?
There's a hard limit on the amount of kinetic energy a wind turbine can extract based on the frontal area and wind speed called the [Betz limit.](_URL_0_) Intuitively you can imagine that to extract 100% of the wind energy that wind would have to fully stop, which paradoxically means there is no wind, ergo there is some limit far below 100%. With some light math you arrive at the Betz limit of 59.3%. 

It turns out that modern wind turbines are extremely efficient, managing to reach 80%+ of the Betz limit. **Practically speaking this means that adding more or larger blades will actually choke the oncoming flow and lower the total efficiency instead of improve it.**

As far as the aerodynamics of the blades themselves go, as they rotate around they leave a wake. If you have another blade following too close to the one in front of it, the wake of the first blade will negatively affect the efficiency of the one behind it.
It's certainly a distinct possibility. And for that matter if we do detect signs of other lifeforms it's fairly likely that the source will have died out by the time we have a chance to respond. If the source is 1000 light years away (which is right next door when we're talking about galaxies) one round of communications will take 2000 years. 

Do you believe *humanity* will survive another 2000 years?

 Check out the [Drake equation](_URL_0_) for a simple and well thought out exercise on the likelihood of finding alien life in general. This question is closely related to the "L" term of the Drake equation.
Viruses rely on receptors on the outside of cells to recognize cells (and gain entry into them). Different cells have different receptors, and so different viruses recognize different types of cells. 

The same way that a lock pick will not get you into a door with a digital keypad, and a circuit analyzer will not get you into a door with a regular key lock.
No.

Being in space *irradiates* you, it doesn't make you *radioactive*.
Gravity, acting from the center of mass, pulls them into spheres as it is the most stable (all the 'farthest from the middle' points are equally far away - the surface).
If another life form were to be "looking at us", they would not see us in the future, they'd see us in the past. If they are, say, 25 light years away and had the ability to resolve our planet, they would see it as it was 25 years ago, just the same as when we look at them. This is due to the time it takes for light to reach them from us. 

Furthermore, time is just as relative as movement in every other dimension, and is not a constant that is universal.
The Planck area is a quantum mechanical description of space, which general relativistic frameworks, like the ones which allow for black holes, are notoriously incompatible with. This is the basis for why physicists are trying so hard to come up with a "unified theory" -- one that would reconcile the quirks between QM and GR.
Heat is a measurable thing (energy). Cold is the absence of that measurable thing. Dark is the absence of light and dry is the absence of water/liquid for the same reason.

A room is empty because there are no people in it. A room is not full because of the absence of emptiness.
depends on the type. The ones I used were based in an acid dissolution and redox chemical processes. Plug it the right way , it stored energy in acid reduction under a voltage differential that gave electrons to the solution and brought it to a higher energy state. They were viven back up through oxidation when the voltage went back down and the solution wanted to get back to his steady state.  If you are on the steady state already and you create the reverse polarity, you are forcing oxidation. Like most combustions this creates gases which, when forced in enclosed containers, tend to explode.
They were named after substances containing those elements - [potash](_URL_1_) and [soda](_URL_0_). Those terms were in use in alchemy before the individual elements were discovered.
#Short answer: Yes.

---

Psychology owes a lot to Freud, and it's kind of a shame how far the pendulum has swung. In his day, he was *the* man to talk to, the face of the still-very-young field of psychiatry. Nowadays, he's almost vilified because most of his theories turned out to be incorrect. What actually happened is Freud identified some key facts about the way the mind works, and then built his (incorrect, but fascinating) theories on those facts. 

One of those facts is that people react in very recognizable ways when presented with stress. Freud called them "defense mechanisms," and he was certain they were the person's attempt to repress memories of sexual abuse. Much of what we know about "repressed memories" has changed, but defense mechanisms have passed the test of time.

* Defense mechanisms are conceptualized as "experiential avoidance" in Acceptance and Commitment Therapy (ACT), a common practice (Hayes and Wilson, 1994). 

* Defense mechanisms are now considered active and conscious, rather than unconscious or subconscious as Freud conceptualized (Ellis et al., 2009).

* Cramer (2015) conducted a review of research into defense mechanisms associated with coping with stress, and found that:
 1. Utilization of defense mechanisms increases with increasing experimentally-induced stress
 2. Research supports the hypothesis that defense mechanisms assist in reducing psychological distress
 3. Use of age-inappropriate defense mechanisms (e.g., regression) is correlated with diagnosed psychopathology.

So, as with so many things Freud postulated, his basic assumption was correct, but his full theory has been shown to be incorrect: We do utilize defense mechanisms (including projection), but not due to unconscious ego-id-superego conflicts.

It is important to note, however, that "projection" is not a diagnosis of any kind, nor is it a recognized symptom of any disorder. Additionally, the use of "projection" as an insult or put down is not to be taken as psychological fact. Rather, projection can be identified in a therapeutic setting and used to conceptualize how a person is reacting to stressors, which can lead to more effective therapy.
I think this can be better worded : What is the maximum rate of  production of blood volume for a human given ideal conditions?
We would certainly like to study a black hole up close, so creating one in a lab would be highly useful. One example, we could directly observe Hawking Radiation, which would be really cool.

Keep in mind, a black hole is not a mystical thing with infinite mass. Any lab created black hole will have the same gravitational effect as the equivalent amount of matter. It would not disrupt the solar system any more than any other thing with the same mass. 

For example, if Mars was instantaneously replaced with a black hole of the same mass, it would not change anything abou the solar system dynamics.
The problem is that your brain wants to assume there's a "correct" frame of reference.

That is, there's a way to step outside of your frame of reference and see things as they "actually are". This is not possible if relativity is true.  Reality depends on your frame of reference.

If you are moving at close to the speed of light, it looks like light is moving away from you in concentric circles, and everything else in the Universe is moving past you at the speed of light. To you, all light from items you are moving toward is blue shifted.

To an observer in a different frame of reference, the unshifted light that you emit is blue shifted as you move toward them and the light that you see the observer emit (which is blue shifted to you) is normal to them.

That's the "relative" in relativity.

In the same way, people talk about earth centered versus sun centered views of the world.  Either one could be correct.

I could choose a frame of reference where the earth is the stationary center of the universe and everything rotates around it.  It's a great frame of reference for doing basic physics experiments.

I could choose a frame of reference where the sun is the center of the universe and everything rotates around it. That's a great frame of reference for planetary physics.

I could choose a frame of reference where the center of the Milky Way is the center of the universe and everything rotates around it.  That's going to be useful frame of reference for astrophysics.
No. Gravity is always attractive.

Gravity is mediated by a spin-2 particle.  For forces mediated by even-spin particles, the force is attractive if the product of the two interacting particles "charges" is positive, and the force is repulsive if the product is negative.  The gravitational "charge" is mass, and mass is always positive - thus, the product is always positive, and the force is always attractive.

If a particle with negative mass were to be discovered, however, this could change, and gravity could be repulsive.  There is no evidence of such particles as of now.
Ice ages take tens of thousands of years to reach their full stride, and it is, generally, a gradual process. And ice would not 'consume the planet', that has *probably* only happened once in Earth's history, last I checked, about 600 million years ago. Modern(ish) ice ages have ice cap extend maybe reaching partway down the continental U.S. (I don't know about other continents too much). But there is still plenty of land left.

That being said, there is very strong evidence that temperatures have dropped by ~10 degrees Celsius worldwide on the order of a decade in the past. Perhaps linked to the thermohaline circulation current. But those are best recorded in the middle of ice ages, I don't think there is strong evidence for them occurring in inter-glacial times (where we are now).

Seasons would be similar, but colder in general of course. Weather patterns would be much different though, so you may get increased rain in some areas and decreased rain in others. In general, you should get a decrease in rain and rain intensity. But again, the actual impacts will vary regionally in this regard. Snow would be more common as a percentage of rain than it is now, but the biggest impact is that it just wouldn't melt as easily during summer months.
The problem is that there is no obvious trend in the actual number of these events; patterns have shifted, clearly in part due to global warming, but it is far from obvious that the trend is unambiguously upward. 

Instead, the reporting of these tragedies had become more common, and the dollar damage has increased due to more building on the coasts and in other vulnerable areas.
Human hair is a pretty good source of DNA ([source](_URL_0_)).  Moreover, it looks like hair samples degrade the least when they are frozen at at least -20C in a non-freeze/thaw freezer within 6 months of obtaining the sample ([source](_URL_1_)).
Let's get some more concrete definitions in here:

A **structure** of N lego bricks is defined to be N lego bricks in any combination, connected or not, touching or not.  However, the bricks are all oriented flat, and at right angles to the plane. The connector holes are always on the top and bottom as expected, and the bricks are rotated at any angle.  Hence there is only 1 way to place the 2x2 brick and 2 ways to place the 2x4 brick, when not accounting for rotation (we are not, here).

Two lego bricks are said to be **connected** if either of the following hold:
1. They are directly above/below the other with at least 1 connector hole overlapping, or
2. There is a chain of blocks connected as described in (1) which go between the selected lego bricks.

A **building combination** of N bricks is defined to be a **structure** of N lego bricks where all N lego bricks are connected.

Two **building combinations** are said to be equivalent if they contain the same number of bricks and if the entire structure of one can be rotated so that the two structures could overlap perfectly - the same size bricks in the same exact places.

-----------------

So, the question has become, are there more distinct building combinations of 50 2x4 legos or of 100 2x2 legos.

And I say we can ignore the "distinct" restriction right off the bat.  Why?  It is only going to affect the result by at most a factor of 4, since there are 4 rotations, and worst case every determined structure when we count can be rotated into another.  But with numbers as big as they are, a factor of 4 is likely not to make a difference.  We can account for it at the end just to be sure though.

Next... it is actually very very hard to try counting and not count the same thing twice, or to not miscount by counting impossible structures where bricks overlap.  To the point of, I'm not certain you'll be able to get an exact answer here.  Or even a reasonably close answer.

For instance, let's suppose we restrict the problem even further, and are okay with only adding legos on top of the last lego you added - we'll make a wobbly tower.  With 2x2 legos, there are 9 ways you can add a new one on top.  And with 2x4 legos, there are (5 x 5) + (3 x 7) = 46 ways you can add a new one on top.  With that, we can get a pretty solid estimate of 46^50 versus 9^100.  But these are pretty close - only different by a factor of 10^12.  With this particular restriction, it is clear that there are more for 2x2, but we can't just extrapolate and say it is clear without the restriction.

Another way to think about it is to try comparing how many ways with one 2x4 versus two 2x2s, two 2x4 versus four 2x2s, etc.  But even that is very difficult to count without creating a computer program.

My last heuristic is to compare how you can add a 2x4 to another 2x4, and see if you can mimic that with 2x2s.  If you can mimic the connection, that it means we can construct a connected 2x2 structure that looks exactly the same as a 2x4 structure, just with 2x2s instead.  And this heuristic is what makes me think that there are far far more 2x2 building combinations than 2x4 building combinations.  If you iterate over the different ways of connecting a 2x4 brick with another 2x4 brick, you can either perfectly mimic it with 2x2 pieces instead, or else, you're left with disconnected 2x2 bricks - which mean that they can be "freely applied anywhere".  Those disconnected bricks are a multiplier, giving rise to a lot more combinations than would otherwise be possible when restricted to the form of a 2x4 brick.

Sorry I can't give a better answer - but this problem really has the feel of a problem from a computational algorithm contest, so if you really want a solid answer, try getting one of the experts there to work on it.
In the question you asked, computer architecture refers to the logical structure of, and the interoperability between processors and memory. Take XBOX 360 and PS3 as an example. While both have processors derived from the PowerPC ISA (instruction set architecture), they are very different in their computer architecture:

The X360 has a tricore Xenon processor and a uniform memory architecture. The three cores are the same general purpose processors, they see the memory as the same piece - and what makes it even better, the GPU shares the same memory, too!

The PS3, on the other hand is quite a different beast. Its Cell processor has one general purpose PowerPC core, along with 7 weird ass Synergistic Processing Elements (SPEs). The SPEs are highly dedicated in floating point processing, which is good for games, and that makes PS3's FPP several times more powerful than X360 in the book. However, these SPEs are very nasty to deal with. They have their own instruction set, their own memory, and they are unable to access the system memory directly, but with clumsy asynchronous DMA operations.

So now you should see why people say it's much harder to make games for PS3 than X360. Of course, if you are making a sketch level game and you don't need much processing power, both architectures could be almost the same to you; but if you are making a 3A game and want to get the most out of these boxes, it'll be much more challenging to deal with PS3's heterogeneous architecture - and much more rewarding if you managed to release all its horsepowers.
The soft and hard palate are both formed during foetal development (vs embryonic). There is a high likelihood that there is a minor genetic defect since both abnormalities to the palate, with similar point in gestation, have manifested themselves in siblings. Here is an excellent website explaining the development of the palate including abnormalities:  _URL_0_.
I have to contend that the best CGI looks very much like reality; but often times we will believe it to look different because we have no reference to how it should look exactly, or are filmed from impossible camera angles at a resolution we're not use to seeing otherwise. Here's some SFX reels to showcase CGI in big budget hollywood films:

Wolf of Wall Street [_URL_1_](_URL_1_)
Gravity [_URL_2_](_URL_2_)
Ironman [_URL_0_](_URL_0_)

If instead the question is why do some CGI look unnatural (as opposed to perhaps different from reality), there are several factors:

**Material**
When I say materials I mean things like skin, metal, wood. While computer have been very good at getting things close, many materials are extremely complex. Human skin for example is riddled with micro-abrasions, fractures, and variations in tone; not to mention our skin is partially translucent and refracts light. Furthermore light can still be reflected from our skin as well as through our skin at the same time (like holding a flashlight and shining it through your pinky). The overhead needed to model this sort of light behavior is simulated but it can't be 100% perfect since CGI generally deals with surface modeling instead of solid modeling (an infinitely thin surface simulates the properties of an otherwise complex solid surface)

**Animation**
Animation is hard, even with motion capture, especially with objects that are not completely solid, like muscle. Everything moves in relation to physics; and muscles for example move a lot (just watch any slow motion video of sports). Again, this kind of animation requires special simulations of mass, elasticity, solidity, and viscosity that are approximate but not perfect. Outside of that sometimes the animations are just not very good. Even for solid objects, there is some level of motion in every movement that is difficult to simulate perfect; there is inherently some framerate when rendering CGI that will be connected to the animation (say, 30 frames per second of a truck bobbing up and down on a dirt road). Reality has no such framerate so its calculations need not be approximated on a frame by frame basis.

**Virtual Camera and Causticity**
Sometimes the reason CGI doesn't look right is just because of the camera. It can have an unnatural focal length, or a depth of field that makes a scene look unnatural. Another factor is the causticity of the air and whether or not it is being simulated. Although we do not think about it, air has a not insignificant caustic effect on what you see. You can observe this by looking at distance mountain ranges or just distant lands. They will often appear lightly more blue, and dulled in color. That's because light from those sources has to travel through the air, not a vacuum, so it has an effect on light. Fluctuations in heat also affect that light (easily simulated by lighting a fire and trying to see something on the other side of the fire). If you don't simulate those in CGI, scenes become too clear and look unnatural.

**TL;DR: CGI doesn't look different from reality, but when it does, it'll generally be limits in the time and computer power it takes to get Materials, Animation, and Environment right**
There's at least a few problems. Normally when boiling water, warm water rises to the top. So the water that's in contact with the heating element gets warmer than the rest of the water and rises to the top. Colder water then replaces it and comes in contact with the heating element. So all of the water warms up more or less equally.

But in zero gravity this will not happen. The water that's in contact with the heating element just stays there and comes to a boil quickly while the rest of the water can still be relatively cold.

Similarly bubbles do not rise in zero gravity but just float where they are. So when the water in contact with the heating element boils, it forms a bubble, which just stays there. Since steam has much lower thermal conductivity than water, this will probably further hinder heating up rest of the water.

See [this video](_URL_1_). And [here's](_URL_0_) an article discussing it.
Top level question for an appropriate expert: Are Freudian theories still regarded as science?
So the DNA is floating in a solution of water, ions and proteins called nucleoplasm. These form a structured shell around the DNA strands known as a solvation shell. Water and ions are too large to penetrate into spaces between the base pairs. They do, however, occupy space in the major and minor groove of the helix and stabilize the shape of the helix into the biologically relevant confirmation known as z-dna.
_URL_0_

Post-February: 256,000 bits per second; (1 gigabyte) / (256 000 bits) = 33 554.432; 33 554.432 seconds = 9.32067556 hours = 0.388361481 days

Pre-February: 128,000 bits per second; (1 gigabyte) / (128 000 bits) = 67 108.864; 67 108.864 seconds = 18.6413511 hours = 0.776722962 days

That is assuming that it's a constant transmission, and that the article is correct (or correct still).  There are probably a number of factors that limit the amount of transmission time, error correcting, and the like.
Yes and no.  

The long term effects are generally the result of some of the nasty compounds that are created when you burn tobacco.  There are a few carcinogens, for instance, that are just the perfect shape and size to slide in between the ladder rungs of your DNA, in whatever lung cell or other cell they manage to find their way into.  Smoking now and then just adds more of these chemicals to your body, and they can take a LONG time to get out, because the cells they contaminate have to die and be removed for the chemicals to be removed from your body.  By the way, these particular carcinogens cause DNA damage in your cells because DNA replication can't read the bases well.  Other interesting carcinogens can actually tie the bases together (thiamine dimers), for instance, or do other things to your DNA.  Yes, your body can try to repair them, but generally, it takes a lot of time to get them out.

On the other hand, other chemicals in cigarette smoke are shorter lived - for instance acetyl choline receptor counts are most likely altered in response to the presence of the nicotine, and once the nicotine is cleared from the body, the acetylcholine receptors will slowly revert to their pre-nicotine levels.  That can take days to weeks.

Cigarettes produce an incredible variety of chemicals that get inhaled, so there are all sorts of results - from short term to long term.  It's nearly impossible to summarize all of it, but the general notion is that yes, you are generally resetting the clock - or at least taking a few steps back - but like anything, it's generally proportional to the amount of the chemical(s) you dump into your body.
You have synapses between neurons in your brain. cognition is neurons firing, many of them in concert. Memory is a particular circuit of neurons that, when stimulated, fires in the same way each time and affects other areas and circuits. The more neurons and synapses involved, the easier the “firing” is. When plaques gum up the synapses, memory begins to fail.
Maybe this gives you some hand-waving intuation:

Spatial Fourier transform takes position space into wavevector space. A lens focuses parallel bundles of beams into a single point. So everything that was described by the same wavevector beforehand shares the same spatial coordinate in the focal plain of the lense.

So the spatial coordinate after the lens contains the information that was encoded in the wavevector before the lens which in mathematical terms corresponds to a Fourier transformation.
There are more planets than stars. The number of rogue planets in our galaxy is about the same as the number of stars, as far as we can tell from microlensing surveys, and most stars appear to have a planetary system.

It's hard to say which types of planets are most common. Our ability to detect exoplanets is limited, so we only have a (more or less) complete picture of the planets in our own solar system. Our solar system has 4 rocky planets, 2 gas giants, and 2 (or maybe 3) ice giants, but we don't know if most systems have more giant planets or rocky planets.

If you include smaller bodies, it's likely that dwarf planets are more common than major planets, asteroids are even more common, and comets would have by far the largest number. Our solar system alone may have trillions of comets.
It's usually just a gross measure of activation. It can tell you how many monocytes or dendritic cells are present (they are some of the primary cells expressing the receptor for LPS; TLR4) but also how the cells are communicating. For example, LPS will stimulate IL-12 production which can promote T or NK cell expression of IFNg, if that doesn't happen it could tell you that T or NK cell function is screwed up. Also note that measuring some factors in whole blood is very very tough---specifically some of the proteins expressed downstream of LPS activation. IL-12 for instance is a heterodimer consisting of two parts---measuring the in tact cytokine as opposed to each individual part is very very hard.
Zika is a big problen mostly because it is new in Brazil. Even in SE Asia, there is few information on medical literature about zika (ex. it is not clear yet if zika is sexually transmited, there is only 2 cases of sexual transmition evidence on entire medical literature)

Brazilian doctors believe that zika came to South America during 2014 World Cup. The virus found a realy good ecosystem to spread, because has a very similar live cycle that others tropical disiases, as dengue (exatly the same) and yellow fever.

I am not sure if it is a mutated vírus or not, but I spoke to a doctor that do not believe in a mutation. The absence of research about zika is probably the cause of late discover of microcephaly-zika link.

I am a brazilian citizen that is watching the development of zika in Brazil since it's arrival.
Stimulation of mechanoreceptors in the skin/muscle/fascia tissue can also activate inhibitory interneurons in the dorsal horn of the spinal cord. This dampens the activity of A-delta and C pain fibers, resulting in short term reduction of pain. This obviously is only a small part of how a massage works physiologically, but its all that i can contribute
It's more that certain chemicals can hijack otherwise useful systems, usually by stimulating or repressing them far beyond their normal parameters. For instance, the dopamine-based reward system is in place to encourage us to perform things that make us more likely to survive and reproduce, such as figuring out patterns, having sex or eating calorie-rich foods.
> Temperature is a macroscopic quantity of matter which is related to the  > degrees of freedom of particles. In the case of a monatomic gas this is  > simply the translational motion (kinetic energy) of the particles. When a substance goes through a phase change, inter particle forces have to be broken (or formed). Therefore, when heating a liquid say, many of the inter particle forces have to be broken to vaporise the liquid, which takes energy. So when a liquid is heated and it reaches the vaporisation point all the heat going into the liquid is used to break these bonds instead of increasing the energy in the degrees of freedom (eg kinetic energy). This is called latent heat. Once the liquid has been fully vaporised energy any further heat applied will go into increasing the energy in the degrees of freedom, thus increasing the temperature. 
   
   
[Source Here](_URL_0_)
Gender is not a relevant criterium to judge possible organ donors. I only know about live organ donation, for example kidneys, but I assume that it is very similar with recently deceased donors. Your blood is tested for the amount of reactive antibodies, indicating how many of your white blood cells will recognize the foreign tissue as foreign and attack it. If your reactivitiy is under a certain limit the transplant will probably work. This reactivity is determined by how different HLA (human leukocyte antigen, a kind of molecular passport of your immune system) is between donor and recipient. The sex of the people involved isn't relevant. So to finally really answer your question: yes, absolutely, men and women can donate organs to each other, if they are a immunological fit.
K, let's set the record straight.

"Current" has been defined in the early 1800s as a form of "ether" that flows from the "positive" end to the "negative" end.

Later on, when we discovered the electron, we found out that the actual moving particles in an electric current flow from the negative end to the positive end.

The electrons gain kinetic energy when they travel from "negative" to "positive", otherwise they won't travel from "negative" to "positive" at all. You can say that they have *higher potential energy* at *lower voltages*.

So anytime you see a circuit with a point that's 5 volts and another point at 0 volts, then the electrons will be travelling from the 0-voltage point to the 5-voltage point, *but by convention, we say that the current is flowing from the 5-voltage point to the 0-voltage point*.

So on to your hypothetical. You have a 5 volt battery, a reversed 10 volt battery, and a resistor in series. The way that batteries work is that the positive terminal voltage is always equal to the negative terminal voltage plus the voltage of the battery.

So let's take the negative terminal of the battery and call that 0 Volts. We are labelling that portion of the circuit as ground. That means that the positive terminal of the 10 volt battery is +10 volts. 

Since the positive terminal of the 10 volts battery is directly connected to the positive terminal of the 5 volt battery through a wire with no resistance, that means that *both spots are the exact same voltage*. That means that the positive terminal of the 5-volt battery is 10 volts.

If the positive terminal of the 5-volt battery is 10 volts, then that means that the negative terminal of the 5-volt battery is 5 volts relative to where we declared our ground.

The negative terminal of the 5-volt battery is connected to one side of the resistor through a resistance-free wire. That means that one side of the resistor is 5 volts.

The other side of the resistor is connected to the negative side of the 10 volt battery through a resistance-free wire. *We already declared this as 0 volts*. So the voltage level on the other side of the resistor is 0 volts.

By convention, "current" flows from positive to negative. That means that the current goes from the 10V side of the 10V battery, to the 5V side of the 5V battery, across the resistor, to the ground.

And the electrons flow in the complete opposite direction.

Notice that the two batteries in series pretty much add up and act like a single 5V battery. Also notice that the voltage around the loop started at 0V, and ended at 0V (Kirchoff's voltage law).

Questions?
[Kolmogorov complexity](_URL_0_) is an important one. The Kolmogorov complexity of a string is the length of the shortest program that can generate that string.

Note that given any string, say "This is a sentence", you can write the following program:

    echo "This is a sentence";

Thus the Kolmogorov complexity of any string is bounded by the length of the string (plus some constant), so the question is whether there are any programs *shorter* than the one above.

If the answer is no - that is, if the only way to generate the string is to list out every character - then the string is said to be algorithmically random.
[L-Glucose](_URL_0_) is indistinguishable in taste from the naturally-occurring D-Glucose and cannot be metabolized by humans, but because manufacturing it is so expensive, the idea to use it as a low-calorie sweetener was scrapped.
Yes, in some cases, it just depends on whether or not the recipient has any antibodies against the donor blood.  
Best example of this is dog blood being transfused into cats. Cats can receive a one time transfusion from a dog in a emergency. 
Here is review article about this subject:
_URL_0_
No, think of it as similar to harvesting wind energy.

However, this is not a practical method of collecting energy. For example, it would take approximately six years of screaming to produce enough energy to heat a cup of coffee.
I'm not a nuclear engineer, but you can insert control rods to decrease the rate of fission reactions. But fission leaves behind a bunch of radioactive daughters which will decay with various half-lives.

Inserting the controls rods and slowing the rate of *production* of radioactive daughters doesn't stop the already created ones from decaying. Even if you were to instantaneously bring the fission reaction rate down to zero, you still have the residual activity of all of these radioactive daughters. The energy they give off as they decay is called the *decay heat*, and there isn't really a way to "turn off" decay heat.

Although I suppose if you needed to, you could purge the core of all of this radioactive material. But then you haven't gotten rid of the heat source, you've just moved it. But again, I'm not a nuclear engineer.
Modern computers are really many separate complex systems working together. Each system is deterministic, so *in theory* the entire computer is deterministic, but the interactions between components are so complex that it is not possible in practice to predict how long a program is going to take. Not just that, but program execution time can be highly variable depending on what else is happening in the system.

There are many sources of timing interference:

* Any shared resources may cause delays in executing a program if that resource is not available when needed. These can be actual shared resources like the hard drive, but they can also be conceptual structures like a software mutex. When this condition is encountered a program may *block* or it may *(spin)wait* for the resource to become available. Resource contention is extremely difficult to analyze because any program may potentially interfere with any other program on the system in unpredictable ways. Many seemingly unrelated programs will actually share resources at an operating system (OS) level.

* The OS scheduler will purposefully prevent a program from executing in order to run other programs. Individual processor cores can only execute one program at a time (modulo simultaneous multithreading techniques), but we want our modern computers to execute dozens or hundreds of programs simultaneously. This is done by switching the processor between a set of programs so rapidly that you cannot perceive.

* Programs themselves may run for non-deterministic amounts of time. For example, a program may request to be put to sleep for a random amount of time (such is a common practice in network protocols and is called random backoff or exponential random backoff).

* Programs themselves may employ randomization in order to execute, making it impossible to accurately predict their execution time. For example, the very popular *work-stealing scheduler* for parallel programming is inherently random. 

* Hardware interrupts may arrive from hardware peripherals (network cards, hard drives, keyboards, etc.) at arbitrary times. Each hardware interrupt causes a brief disruption in processing while it is handled. These are usually essential to the operation of the system as a whole, so they cannot be disabled and they cannot be predicted.

* Even if you try to disable hardware interrupts, there are *non-maskable interrupts* which are hardware interrupts that cannot be disabled under any circumstances. Some hardware vendors have motherboards that generate these interrupts (such as Machine Check interrupts) between the motherboard and the processor in such a way that is invisible to any higher level software (i.e. OS) and cannot be controlled. 

* Processors in multi-core systems will also have to deal with Inter-Processor Interrupts (IPIs). These are used for memory management, processor synchronization, and other system management operations that require the cooperation of some or all processors. 

* The memory hierarchy and memory caches in particular can create highly unpredictable delays in accessing memory. Sometimes access to a memory location will be slow, and sometimes it will be fast. The performance of caches will vary depending on many factors, such as what other programs are executing on the system, what parts of memory those other programs are using, how much memory those other programs are using, the order of program execution (from the OS scheduler), etc.

* Caches on multi-core processors are even less predictable. The cache coherence protocols designed to mediate different processors' access to memory may result in cache contents being stolen or invalidated at unpredictable times.

* Software may explicitly influence cache behavior as well. For a recent example in the news, the Linux Kernel Page Table Isolation patch will result in the explicit clearing of the translation lookaside buffer (TLB) on some systems.

* Other hardware resources that are seamlessly or silently shared may prevent a processor from execution. For example, contention on the main bus may cause a processor to stall unexpectedly.

Lastly, it's important to note that, even though computers are entirely deterministic in theory, there are many computer systems that respond to non-deterministic events. An easy example would be any computer that uses a network: those network communications come in at entirely unpredictable times, causing hardware interrupts, cache disturbances, etc. that completely destroy any deterministic model of your system. Beyond networking, there are many computer systems that are attached to sensors or other devices that generate a computational demand at entirely unpredictable times. An example of an unpredictable sensor that virtually all computers must manage is the keyboard.

Rather than trying to guarantee an actual constant time through analysis, system engineers who need to know how long programs run will employ a whole lot of benchmarking. This essentially means that we run our program hundreds or thousands or millions of times and record how long the program takes each time. The longest observed execution time is then assumed to be the *worst-case execution time (WCET)*, and that is used as the basis for any worst-case timing analysis. Since this method does not actually guarantee an accurate worst-case timing, it is not uncommon to add an "engineering factor" of an additional 10% execution time or somesuch. 

If you're not actually concerned about worst-case execution time, but average case behavior or similar then you can use your one million benchmark runs and generate a histogram or probability-density-function for how long your program takes to run. Like many other real-world data distributions, the absolute worst-case execution time may be tens to hundreds of times longer than the median or even 90th percentile execution time. Thus, you might design your system around a figure like the 90th percentile or 95th percentile execution time.
No, there is not. The question of proteins folding into a certain gemoetry, and the idea of chemical structure in general - that atoms hold more or less fixed locations relative each other - is itself proof that nuclei of atoms do _not_ behave very quantum mechanically and do _not_ have significant (in chemical contexts) uncertainties in things like their position on the scale of chemical bond lengths (10^-10 m). Note now that the same assumption is _not_ valid for the electrons. This distinction is so implicit in all chemistry it's easy to miss the fact that these two objects (nuclei and electrons) are being treated in an entirely different manner, and the reason for that is that electrons behave quantum-mechanically while nuclei behave almost-entirely classically. (and when not, only hydrogen ever 'tunnels' quantum-mechanically to a chemically significant extent)

Forget proteins and just consider a far smaller molecule such as [the enantiomers of tartaric acid](_URL_0_). The (+) and (-) forms are mirror images of each other. Physics dictates that the two have exactly the same energy, after all nothing changes about the electromagnetic forces when you mirror them. Because of that, according to quantum mechanics either molecule should when left alone, evolve into a superposition of being in both (+) and (-) states, and not assume one or the other until measured. (and then with a 50-50 probability)

In chemical terms it means the substance should spontaneously racemize, meaning it turns into a 50-50 mixture of enantiomers. This does in fact _not_ happen. And the reason we have the concept of enantiomers and other isomers is again because the structure of molecules is in fact stable in this respect. This seeming discrepancy is due to another silly 'paradox' like Levintha's (Hund's Paradox), and the resolution to it is the fact that when you do the calculation on _how long_ it takes to form a quantum superposition of states for an ordinary-sized molecule (a calculation that's harder to do than just _whether_ it forms a superposition), you get the result that it takes a truly astronomical, age-of-the-universe kind of time for this to happen. Then on top of that there's the phenomenon known as decoherence, that means that a protein, in the warm, wet, noisy environment its in, could not maintain a superposition for a chemically-significant amount of time (only something like ~10^-14 seconds). In other words, it's effectively being 'measured' constantly at that speed.

When we (quantum chemists) do actual quantum-mechanical calculations for chemistry, the nuclei are usually treated classically or semi-classically rather than as fully quantum-mechanical objects. We know of course that's not really true, but it's a very good approximation in most cases. (again, hydrogen's the exception. You'll have measurable errors in the kinetics of your hydrogen atoms if you don't take quantum tunneling into account)

The resolution to Levinthal's paradox is even more straightforward: Proteins simply do not 'test' every conformation when folding. And there seems to be increasing agreement that their natural folded conformation is not necessarily the one with the lowest energy (globally) either. (something many believed for a long time, for reasons that seem unclear at best, pure wishful thinking at worst)
They aren't actually rain, snow, or anything meteorological; what you're seeing is some type of [ground clutter](_URL_0_). "Ground clutter" is basically a catch-all term for any echo picked up by the radar that is not some form of precipitation. Since radars are "dumb", and will return a signal no matter what the radar beam is reflecting off, you will get echos from bugs, birds, dust, and even nearby buildings and landscape. While there are algorithms for removing this ground clutter from the displayed product, they are not perfect, so there will always be some ground clutter visible on radar. It will also be much more visible on warm, humid, calm evenings (when flying bug activity is high), which is why the circles are appearing mostly to the east of the thunderstorms in the central US in the image you posted: the thunderstorms mark the passage of a [cold front](_URL_1_), meaning colder, dryer air is to the west of this area, which reduces the appearance of this ground clutter.

When you see a "national radar" image like this, it's really just a whole bunch of local radars digitally stitched together. Therefore you can see these circles of ground clutter around each individual radar site.

I have seen people post screenshots from this and other apps before; the color schemes they use tend to make these rings appear, likely due to them incorporating [composite reflectivity](_URL_3_) instead of [base reflectivity](_URL_2_). Composite reflectivity is much more likely to show false precipitation echoes, so though it is useful for trained meteorologists who know how to interpret it, it's a poor choice to show the general public.

**tl;dr**: It's not rain.
Actually, no.  Silicon wafers are grown cylindrically.  Raw polycrystaline Silicon is melted down and grown into a [single crystal](_URL_1_) using the [Czochralski process](_URL_0_).  The result is a long cylindrical crystal that is pure Silicon (well, ~99.9999% pure, depending on how careful you are).  To then cut those into squares would be wasteful.  Instead the crystal is cut using thin wires and then polished/processed as necessary.
Well nothing's stopping you from using it except that few people know how to use it, and very little has been translated to it.

Also, people with different accents would _write_ differently. This is critically important in languages such as Chinese, where the differences would render every dialect mutually unintelligible, and somehwat important in languages like English, becuz eugeniks an da lik wud mak ritin litrl spekn had. Written language preserves etymology, whereas the IPA, which would produce different forms for the same word, does not.

Additionally, what is transcribed in the IPA is not entirely uniform, so representations would be ambiguous even among speakers of the same dialect.

Since people usually read by identifying words as a whole, direct transcription of what was said would be counterproductive and difficult to follow, and since that's what the IPA is for, it would be too.

Disclaimer: I can't speak AAVE, so my transliteration is probably shitty as fuck.
Once past the event horizon, all things move towards the singularity - even light.  The event horizon isn't just a one-way door.
Humans use their mouth (teeth, lips, tongue) and larynx (vocal cords) for speech.
Birds have a larynx, but it doesn't have sound producing structures (as ours do) and don't use their tongue. A bird's vocal apparatus is the syrinx an upside-down y-shaped structure located at the bottom of the trachea (where the windpipe meets the lungs). Each branch of the syrinx has an independently moving valve. This allows birds (primarily songbirds: Passeriformes) to produce complex series of sounds (such as human speech).
As far as I am aware, there is no clear answer.   The problem with answering your question is that we don't know how to exactly define consciousness as a neurocognitive process, and moreover, we have no idea what brain regions are therefore responsible for "consciousness".  Therefore, if we don't know what is going on in a "normal brain" it's tough to say what happens in a split brain patient.    

Some people would argue the left hemisphere is considered the "dominant" hemisphere, but usually that argument has to do with the fact that in most people language functions are assumed by the left hemisphere.  So all things being equal (no diseased brain tissue) then I suppose you could make an argument that the left hemisphere (or I guess whichever hemisphere is language dominant for that individual) would be more likely to maintain a persons "conscoiusness", though I don't really think that using language as a marker really answers the question.
Yes. Some species have low prevalence of tumors due to variation in biology or life history, but tumorigenesis has been observed in [insects](_URL_0_), [sharks](_URL_2_) (despite popular wisdom), [trees](_URL_1_) (though the lack of a circulatory system means these are not malignant) and as far as I can tell, every multicellular, organized species of life is at risk of developing a disorganized cell mass or tumor. 

The naked mole rat is famously described as [an animal that has never been observed to develop cancer](_URL_3_), however research would indicate that they are better at combating pre-cancerous lesions rather than never having cells go awry in the first place.
This is a great question, but answering it is extremely difficult. We still don't understand endocrinology and metabolism enough to be able to fully untangle how sex hormones and a host of other hormones, growth factors, receptor polymorphisms, etc. work together to affect body composition. 
These three papers may help start you off: 

[Overall Review](_URL_2_)

[Looking at Sex and Race in prepubertal children](_URL_0_)

[Looking at subjects in various stages of Puberty](_URL_1_)
For the example you used it isn't an oxidation, it's a reduction. I don't know if you're searching for a more quantical model explanation, but in terms of solution chemistry, it boils down to the exchange of electrons and mass (related to the ions and the matrix in which the reaction takes place) in an equilibrium.

There's at least two pair of semi-reactions happening simultaneously, a semi-reaction of oxidation (an increase in the oxidation number of a chemical species, loss of electrons) and a semi-reaction of reduction (a decrease in the oxidation number of a species, electron gain), summarized in a global redox (reduction/oxidation) reaction.

All of this is considering there are no side reactions involved in the equilibrium, but that's the case in most scenarios.
Taste researcher here: 

First, whatever you are eating needs to be in the oral cavity. From here, there are taste buds on the tongue, pharynx, larynx and epiglottis. Each taste bud contains 50-150 taste receptor cells that express different receptors for different stimuli. For example, T1R family transduces the signals for sweet and savory stimuli, whereas PKD1L receptors transduce sour, T2R family (which contains about 30 different types of receptors) are all for bitter, and ENaC channels transduce salt signals. I really want to stress here that if you have heard of a "tongue map", it is wrong. The entire tongue can detect different stimuli, not just a specific zone for specific taste qualities.

The tongue is innervated by the chorda tympani branch of the facial nerve (CN7), glossopharyngeal nerve (CN9) and vagus nerve (CN10). In humans, cranial nerve fibers project to the first relay in the brainstem, Nucleus of the Solitary Tract. It is important to note that in rodents, information relays in the parabrachial nucleus of the pons, but this is not so in humans. The reason for this is not yet currently known. 

From here, fibers ascend directly to the amygdala, accumbens, and other areas, but importantly the VPMpc of the thalamus. From the thalamus, information is relayed to the insular cortex (gustatory cortex) where things like flavor is thought to be integrated. The cortex also sends signals back to these other areas. 

It is also important to distinguish between taste and flavor. Taste is more basic in sensations in that it is salty, sweet, sour, bitter or savory. Flavor is the integration of taste and olfaction (smell). 

Fun fact, it was discovered by Pfaffman in 1951 that cats cannot transduce the signal for sweet stimuli when he was recording gustatory information from the chorda tympani nerve. (He is also the father of across-fiber pattern theory). 

As for neural coding of taste information, there are three schools of thought: Labeled-Line, across pattern theory, and temporal codes. 

Labeled line suggests that a taste cell (in the brain or periphery) responds best to one type of taste and this is enough to convey a perception. Across-fiber pattern suggests that the interaction between neurons is what gives rise to a perception, and temporal coding suggests that the pattern of a neuron firing is what conveys information. None are mutually exclusive as well. 

[Here](_URL_0_) you can see the figures from a really good review paper, but for some reason, Nature won't allow access to full text. 

[This ](_URL_1_) is also a decent review.
Do you mean in total weight of the animal (like 25 pounds of chicken breast equals one chicken) or as in eating various parts of the chicken to reach a full chicken.
As someone else mentioned, cockroach cells aren't dividing as frequently as ours, and cells that are dividing are the most sensitive to DNA damage from radiation.  Humans die from radiation because of our most sensitive cells that are dividing the most frequently, our gut lining, and our blood stem cells.  If you could stop those from dying, we would have a higher threshold of radiation resistance.

This property of radiation damage, that it wreaks the most havoc on dividing cells, has actually been exploited by anti-radiation medicines.  People are testing inhibitors of CDK4, which stop the cell cycle temporarily, as an anti-radiation medicine. These were originally made as anti-cancer drugs, to stop cells from dividing, but their side effects are pretty low, so they might turn out to be very useful radio-protective drugs.  They dramatically improve the survival of irradiated mice.

_URL_0_
Yes, although you won't recognize it as such. Since your brain isn't wired to see things in 4D, it will just see a 3D object. Much like how a hypothetical flatlander, when faced with your 2D drawing of a cube, will not see a cube—rather, just ~~six~~ eight points connected by lines in a specific pattern.
Quite probably due to redox half-reactions, although you'd have to describe the exact situation in more detail.

Battery terminals are usually made of some sort of metal, let's assume copper. This metal can oxidize, forming salts like copper oxide, CuO. Copper oxides separate ions are Cu^2+ and O^2-.

The negative terminal can release electrons, binding Cu^2+ + 2e^- back into Cu. The positive terminal draws electrons instead, turning Cu into Cu^2+ + 2e^-. As such oxidation and corrosion are suppressed at the negative terminal, while they are promoted at the positive terminal.

This phenomenon can be used to prevent corrosion of important things like ship hulls. Putting another block of metal into the water next to the hull and connecting them via a battery causes the sacrificial block to corrode instead of the more critical part.
There's a difference between oxygen molecules and oxygen atoms. Oxygen molecules consist of two oxygen atoms (hence the O2 abbreviation). But oxygen atoms can be found in many different compounds. Water has 2 hydrogen atoms and 1 oxygen atom per water molecule. Carbondioxide has (as the name already implies) 1 carbon atom and 2 oxygen atoms for each CO2 molecule. Sucrose, the primary ingredient in table sugar, has 12 carbon atoms, 22 hydron atoms and 11 oxygen atoms in each molecule.

And the list goes on and on. Each of these molecules has completely different chemical properties, despite having the same types of atoms.

So while a water molecule contains an oxygen atom, this oxygen atom can't be used to assist in combustion since it's already bound to the 2 hydrogen molecules. While it's possible to separate the oxygen from the hydrogen, and obtain molecular hydrogen (H2) and molecular oxygen (O2), this process takes a very large amount of energy.

In fact, when you burn hydrogen (H2) it uses the available oxygen (O2) to produce water (H2O). So water can be obtained as the product of fire.
It would depend on what you set as zero.
some interesting articles
_URL_0_
_URL_3_
also 4-manifolds are uniquely different beasts from other manifolds
_URL_1_
_URL_2_
Antibodies, also called Immunoglobulins, are proteins that are secreted in response to an antigen, which is most commonly a protein of the pathogen, but can also be against a toxin.
After an immune cell of the Innate Immune System (the first responders), a Macrophage for example, engulfs and destroys the invader, it then presents a piece of it on the cell-surface via a molecule called the Major Histocompatibility  Complex (MHC). 
In the case of Antibody production this is then presented to a B-Cell which turns into a Plasma Cell to produce antibodies specific to this antigen.  
Antibodies can then work in two ways; They can neutralize the target directly by blocking important proteins on the surface, or they can prime the target for easier attack by the immune cells - The latter is called Opsonization.
Bacteria does not become directly "resistant" to antibodies, but there are several ways to avoid it:
*Mycobacterium tuberculosis*, causative agent of tuberculosis, can survive and even replicate inside phagocytes like macrophages, by disrupting the fusion of the Phagosome with the Lysosome, which is a "digestive" mechanism of the cell. The Lysosome contains various hydrolytic enzymes to break down the pathogen.

Sometimes members of the same bacterial species have different serotypes, which means that 2 strains cause 2 different antibody responses and are not affected by the response to the other.
Other pathogens, like the parasites that cause Malaria, can change their surface proteins when they replicate to avoid circulating antibodies.
Not quite.

Global winds and ocean currents on planets are based off numerous factors.

- The tilt of the planet (not just seasons, but if influences the amount of sunlight at the equator and the poles)

- Positioning of continents (alters ocean currents)

- Positioning of continents (warm and cold air)

- Speed of the planet rotation (Coriolis effect)

- Size of Planet

- What 'substances' are being involved (we still don't know for sure what is in the Great Red Spot).

If earth contained only land at the poles, and was just ocean from 60S to 60N, you would see some systematic weather. Earth (with continents) has ocean currents surround our continents and impact our climates.

An example: Western Europe should be Taiga (as Taiga forests usually occur around 60N and 60S). However, the Gulf Stream which hits the Eastern United States Seaboard provides warm currents to Europe; Europe, as we know, contains deciduous forest.

_____

Now about Jupiter and other planets:

It is not uncommon for gas giants to have bizarre weather. There is little 'atmosphere', let alone friction from the surface of the planet. We still don't know what makes the Great Red Spot 'exist'. A theory is that hot gases rise to higher levels, eddies form, and ultimately fuel the Great Red Spot.


Anticyclonic storms, or storms which 'spin' in the wrong direction (like Great Red Spot), are not impacted by the Coriolis effect because they are so 'small' compared to the planet's size. We know this because tornados on Earth can sometimes spin backwards.
(See sidebar of /r/math to render equations.)

Powers of complex numbers are defined in terms of the exponential and logarithm functions. So we have

`[; c^i = e^{i\log(c)} ;]`

where "log" is a logarithm of *c*, and there are infinitely many of them. Let's write

`[; c = a+ib ;]`

with *a* and *b* real. Then

`[; \log(c) = \textrm{Log}(\sqrt{a^2+b^2})+i\theta+2\pi n i ;]`

where "Log" means the usual logarithm function defined on positive real numbers, `[; \theta ;]` is an angle such that `[; \tan(\theta) = b/a ;]`, and *n* is an integer. Substituting this into the definition for c^(i) gives

`[; c^i = e^{-\theta-2\pi n}\left[\cos\left(\textrm{Log}(|c|)\right)+i\sin\left(\textrm{Log}(|c|)\right)\right] ;]`

where `[; |c| = \sqrt{a^2+b^2} ;]`.


Interestingly, notice that if |c| = 1, then the sine term vanishes, and the final result is real. So any complex number on the unit circle to the power of *i* is actually a real number (and there are infinitely possible values).
From the point of reducing infection, yes.

From the point of bringing patients good food that they'll actually eat, keeping patients happy, helping them to the bathroom, and a whole range of other things that most hospital staff don't have time for... no it hurts overall health.

In most hospitals I've seen a limit on the number of simultaneous visitors, which doesn't help infection, but does help avoid crowding, noise, sitting on the room mate's bed, letting small children fiddle with IV, and a whole range of other things.
You are seeing thermal noise from the input amplifiers and attenuators etc of the analyser (around 290°k) the noise you see is related to kxTempxBandwidth. Which is why you see the noise floor go down as you reduce the measurement bandwidth or video bandwidth.

Background microwave radiation would require some very serious X band receiver and dish pointed at the "Cold Sky" to be observable.
Evolution is the very gradual change in genes over time. It isn't that one day an organism gave birth to a human. Rather, it's that over a several thousand year span, creatures stopped being a human ancestor and were instead human. There is no absolute point.
There's a few levels of detail I could go into here, but in my answer I'm addressing a more introductory audience. 


In molecules the electric charge is smeared out in a distribution around the molecule. Due to differences in the atoms that make up the molecule (mostly the nuclear charge), the distribution will be lopsided or otherwise irregularly shaped. In water, a majority of the electron density of a molecule is around the oxygen atom, making it more negatively charged than the hydrogens. Since the water molecule has its bent shape, the side with the oxygen is more negative than the side with the hydrogens, and you get a *dipole moment*. 


We can calculate the dipole moment directly using computational methods, but they require some more technical know-how to discuss. Now, there's a bit of an issue if you're trying to calculate the charges for each individual atom within a molecule. This is problematic since the electrons are smeared across the molecule and don't technically belong to an individual atom anymore. There's a few ways people have cooked up to try and calculate partial charges of individual atoms. The simplest way is to calculate the shape of the actual charge distribution and try to recreate this by changing the partial charges of each atom in the molecule. This  method will get you a decent idea of the charges for each atom. 


You can use these methods on any molecule in principle, and should be relatively straightforward* for the molecules you listed. I'm not aware of a method that's simple enough to do by hand, that would actually be useful (except for teaching purposes maybe), but perhaps somebody else can chime in in this area. 


*Straightforward if you use somebody else's software at least =)
The pressure constricts the nerves which eventually causes them to stop working correctly. This is called transient parasthesia. The pins and needles effect is basically the result of the nerve firing erratically while "waking up" and moving toward normal functioning again.
You asked about virtual particles before,and I [gave](_URL_0_) a brief explanation then. 

But I'll try again. Starting with standard quantum mechanics: Say you want to solve the Schrödinger equation for a Helium atom. You can't do that, because the electron-electron interaction makes it analytically unsolvable. (Here, we're just using an entirely classical coulomb potential. So the electrical force is infinitely fast and there are no virtual photons or anything)

But without the e-e interaction, you can solve, since that's just a hydrogenic atom. So what you can do then, is solve it that way, and use your hydrogenic 'non-interacting' states as a _basis_. A set of functions which you use to describe your real wave function. Using a mathematical method known as perturbation theory, you can then re-introduce the interaction and get an approximation of the real wave function, expressed in terms of your basis. So you're basically calculating a set of 'corrections' to your original function in terms of contributions from the other, excited states of the 'non-interacting' function. 

In theory, you'd need to calculate the contributions from _every_ state, of which there are of course an infinite number, but in practice, the series converges quickly, as long as the interaction is small. This is a common textbook example of how to apply perturbation theory. (usually they only go with the first-order correction) 

Now if you skip ahead to the advanced subject of quantum field theory, a lot of stuff is different; for instance you're describing your system with a Lagrangian instead of a Hamiltonian, and don't really have a wave function in the ordinary sense. The problem you're typically trying to solve is a scattering problem; say, two electrons scattering off each other. And now, you don't have a classical field, but a fully quantum-mechanical, relativistic one. So now you have a third quantum object involved, namely the EM field. In a manner analogous to the quantum harmonic oscillator, the EM field also has quantized states. Which is what photons are.

Just like with they Helium atom, the interactions are causing a problem. Because you have the electrons interacting with the field, and the field interacting back with the electrons. Again, you take the same perturbation approach and describe the interactions in terms of these kinds of non-interacting states. i.e. "bare particles" and the "vacuum field". Here you run into a problem - which is that some of the integral terms in the perturbation series diverge; they have infinite energy. 

This was solved by 'renormalization', which basically amounted to ignoring them. (which was controversial, but works) Richard Feynman came up with a set of 'rules', through which you could draw schematic diagrams (Feynman diagrams), which represented the perturbation terms that contributed to the result. The connections in the diagram tell you what the integral you needed to calculate looked like. And since that integral is expressed in terms of excited states of your field and particles, it can be _interpreted_ as interactions between these 'virtual' excited states, and in the case of the field a state equals a particles, so 'virtual particles' then. So you draw your Feynman diagrams to know which terms you want to include (to whatever accuracy you want your result), and you calculate those integrals, and you get your result. Again, you'd need an infinite number, in-principle, but seldom need more than a handful.

So in the jargon of QFT and particle physics, they often talk about interactions as occurring in terms of virtual particle interactions. Because that's what the mathematics of perturbation theory "looks like". 

But does that mean the perturbation series is _actually_ a causal description of how the interaction occurs? Not really; it's a way of visualizing it in simpler terms. There isn't actually such a thing as a 'non-interacting' particle. It's the _sum_ of the series, not the individual terms, that is measurable. There are also non-perturbative field theories which describe the same things without involving virtual particles in any sense.

On the other hand, 'virtual' particles and virtual states do represent _something_. Just as they do in the original helium example. Because they're not arbitrary, just idealized; they're determined by intrinsic properties of the field (or in the helium case, of how electrons act). Photon properties reflect how the EM field works, and so the interactions of virtual particles reflect the properties of EM interactions.

That's basically all people are saying when they say that EM interactions are 'mediated' by virtual particles, though. It's an interpretation, a way of visualizing the interaction. But if you follow what I just said, you'll realize it's not at all as 'deep' as it sounds. A particle (real or virtual) and its field are not actually distinct things.
Just concerning heat flux from human bodies: 40W/(m^2 Human)*7E9 Humans * 2m^2 (body surface area) = 560,000,000,000 Watts

But the earth's surface is constantly absorbing around 89PW of power from the sun. That is 89,000,000,000,000,000 Watts from the sun.

Therefore, the heat flux from human bodies is negligible compared to the net heat flux from the earth. This assumes that the temperature of the earth is relatively constant (i.e. there is no net heat flux).

However, the carbon emissions and other extraneous effects of human life 'will' ('may' if you are a global warming skeptic) have distinctly different results on the earth's temperature

Source: _URL_0_
_URL_1_

edit: formatting went crazy
One of the most important factors is water content of the food. Water is a necessary requirement for organisms to grow, if a food has no water in or on it no bacteria or mold can grow to spoil it.

Even very dry foods do have some amount of water content, dry legumes for example are still about 10% water. They will spoil eventually.

The other source of water is humidity in the air, certain foods are quite susceptible to this like bread. Water can easily get in the bread where mold can start to grow.

A food that is basically pure fat or oil, not only won't contain water, but it will actively repel the water. It is very difficult for anything to start growing in pure fat or oil because those organisms have very difficult access to water.

Although the fats can still become rancid but I don't think that is the type of spoiling you are asking about.

_URL_0_
These questions are always difficult for scientists to answer, because as you say, certain mutations in hox genes produce healthy surviving animals with different body plans in the lab. The best answer I can give is that these mutations are extremely rare. As hox genes are so important, there are probably only a couple of DNA sequence changes that can be tolerated. 
Another thing to mention is that in lobsters and shrimp, the duplications happen in small segments with few internal organs that are already quite numerous. It seems likely to me that adding an extra pair of swimmerets would be much less deleterious than adding an extra digestive system, or other crucial organs. Insects have really important organs in all 3 of their segments, so perhaps duplicating any of these creates a system that is too out of balance to survive (just a hypothesis!)
Another factor to take into account, is that we still estimate that there are millions of insect species still uncharacterised. Until we really know them all, the possibility of species with duplicated segments is still posssible :)
NK cells are an interesting part of microbiology. To put it simply:

Cytotoxic T cells recognize a major histocompatibility (MHC) I complex which contains foreign protein fragments on the surface of the cell using a TCR on a T-cell's surface. The foreign protein has been digested and broken down within the affected cell. When a foreign protein is detected, cytotoxic T-cells release granules of perforins that cause apoptosis (basically little packets of things that cause the cell to kill itself).

NK cells are an interesting cell, as they don't quite fit in innate (non-learned) or acquired (learned) immunity, but for all intensive purposes, we'll place them within innate for now. NK cells do NOT need to have prior exposure to an "antigen" (or foreign fragment), do not exhibit memory, and do not undergo clonal expansion. They do not possess a TCR, and recognize their "prey" using a MHC receptor, which recognizes "self" receptors, as opposed to foreign ones. They also have "stress receptors" that look for certain "stress proteins" on the surface of cells, indicating an issue. Should an issue be detected, granules are released.
Neither of those species you mentioned have any pKa value.  pKa is defined with respect to the equilibrium constant of an X-H bond dissociating in soluiton.  A halogen (e.g., Cl2) has no H, nor does an alkali metal.  Hydrohalide acids tend to have low pKa values because the halide ion itself is very stable in aqueous solution, which is partly due to their electronegativities.  However, there are no acidic H-M salts.  I'm not sure where that came from.
It depends on a lot of factors.  Assuming you're talking about Earth under standard conditions, the one that will vary the most is the size of the bubble.

[This paper](_URL_0_) gives a proposed data-fitting formula based on theory.  Equation (1), (6), and (8) are the most generally applicable equations, and Figure (1) shows you a plot of experimentally measured results along with the calculated results from a number of models.  

For example, the terminal velocity of a bubble of D = 0.1 cm is about 15 cm/s; for D = 1 cm, it's about 25 cm/s.
If Hawking is right, they'll eventually "evaporate" due to Hawking radiation, where they slowly lose mass due to quantum processes near the event horizon. This takes a really long time for really really massive black holes; if a supermassive black hole that was a billion times the mass of the sun never gained another once of mass again, it would still take 10^89 years to evaporate. 

If Hawking is wrong, then they'll probably just sit and chill and sweep up what's left of the mass of the universe, occasionally gobbling up another photon or electron. 

Of course, this question cannot be fully answered without a theory of quantum gravity, which we don't currently have.
Define "intuitive." There are several ways you can determine it, all of them require at least some physics knowledge. 

Shine white light through a prism, see that the violet light is bent more than the red light. This is because the refractive index of a material is dependent on wavelength. This is also why in a rainbow the violet is on the bottom. 

Notice that the daytime sky is blue, the setting sun is red. This is because of [Rayleigh Scattering](_URL_2_)- and Rayleigh Scattering is dependent upon 1/wavelength^4 so the shorter wavelength is the first to scatter. (The link I provided provides a discussion on "why the sky is blue.")

The most straight forward way I know is to heat something up. It will start to glow red first, then yellow all the way up to blue as it gets hotter and hotter. This is called [black body radiation](_URL_1_) and as things get hotter they release higher and higher energy photons, the higher the energy of a photon, the shorter its wavelength. Thus, it releases long wavelength photons first, after getting hotter it releases short wavelength ones. 

Finally, there is [Young's Double Slit Experiment](_URL_0_) in which the red interference patterns will be wider than the violet ones, since the size of the interference patterns are proportional to the wavelength. 

I don't know of any other easy ways off the top of my head.
What it's telling you is that we are all descended in an unbroken female line from the MRCA because everyone, male and female, gets their mitochondria from their mother. The MRCA's female contemporaries likely had children but if they only had male children, or at any point the contemporaries' descendents only had boys then they wouldn't have passed on their mitochondria only their nuclear DNA.

To answer your question about the MRCA's importance;

a) it was in and of itself an interesting scientific discovery that animals only inherit their mitochondria and their mitochondrial DNA from the female parent.

b) We can then use worldwide distribtions of mtDNA subtypes to map the divergence of and movements of humans

c) We can also use some of this information to estimate human evolutionary divergence times.

d) We can derive interesting haplotype information from distributions of mitochondria
There are many MANY different kinds of birth control.  The typical oral combination birth control actually works by stopping ovulation.  I don't know how familiar you are with birth control, but in the standard 28-day pack, there are 7 inactive pills that allow for menstruation.  The active pills have estrogen and progesterone.   The inactive pills have either iron or some other non-hormonal ingredient.  The estrogen (mainly) and progesterone in the pills promotes growth of the endometrium of the uterus.  When that is withdrawn (when you start taking the inactive pills) the endometrium is shed (AKA menstruation).  In general, by taking the estrogen and progesterone, they feedback on the brain preventing GnRH, LH and FSH.  These are needed to mature the eggs and then ovulate.  No ovulation = no pregnancy.  As far as PMS, some women can still get PMS on birth control, but usually birth control pills are used as a way to treat mild forms of PMS.
Now, when you say "continuous," if you are talking about women who take the "extended cycle" packs, then yes, by not withdrawing from the progesterone, they do not have a period.  This can result in "breakthrough bleeding."  But there would be no real period.  Also no ovulation, but there is no ovulation even when you take the inactive pills.
I don't know if that answers your question or not.  There is still a menstrual cycle, but no ovulation.  This explanation is specific to the oral combined birth control. There are many other types that work differently.
I can at least tell you the difference between summer and winter with authority.  The plant stores sugars.  (Which ends up being a balancing act for the trees because it wants to store sugars for itself, but also for any fruits/seeds that it may create.)  It doesn't continue to create leaves during winter because there isn't enough sunlight to make the leaf worthwhile.  Keeping a leaf requires sugars itself, so you need enough sunlight to satisfy the leaf and still have leftover sugar.  

I would imagine that if you removed all the leaves all at once, it would just grow them back with any stored sugars.
If it's a power cord that just transfers power (say, that to a lamp), then there is no current through it when the device is unplugged, as there is no circuit for it to go through.

If it is part of a power supply that changes the voltage, or one that rectifies as well as changes the voltage, there's either a transformer or electronics in it. Both of those will allow a small current to flow even when nothing is being powered, and dissipate some power as waste heat.

However, especially with modern chargers that waste heat is negligible. A good rule of thumb is: If the charger/power supply is plugged in, with nothing being powered by it, and it is not warm to the touch, the power wasted by it is not worth worrying about.
Humans can kind of survive on plants. You know, like vegetarians? If you took the plants away from the system, though, problems arise once we start eating each other (prion disorders).
I am assuming by that term that you mean when one person essentially transfers their "bouncing energy" to another. 
  
The key is for one person to come down on the trampoline surface slightly before the other. Their momentum stretches the trampoline down just ahead of the other person's arrival. When the 2nd person comes down and makes contact, they continue to stretch the trampoline even further, which resists the process of the trampoline recoil pushing person #1 back up. Person #1 will not rebound very much (since the trampoline cannot recoil), and #2 will rebound much more than normal, since they will have displaced the trampoline downward much further than normal (with the help of #1).  The effect is for much of #1's momentum to be transferred to #2.  Of course, momentum and kinetic energy are conserved.  
  
The process works best if #1 and #2 land very close to each other, and if #2 arrives at the trampoline surface just as #1 has reached his maximum downward travel and is just about to start going back up.   
  
Source: Spent a lot of time on the neighbor's trampoline as a kid.
It's a surprisingly complicated question as there's more than one way to get a calico/tortie male cat, including mutant coat patterns that look like calico but aren't produced by the same genes that produce a calico pattern in females. As such, there's no simple answer for "how rare".

More in-depth information can be found here: 
_URL_0_
Nothing visible. 

Fluorescent lamps excite mercury vapor, that excitation releases ultraviolet light. The paint on the inside is phosporous which emits light in the visible spectrum when it's struck by the ultraviolet light.

If you took out the paint you wouldn't see anything and would be bombarded with radiation.

**edit
As others have pointed out below it seems there is a bit of visible light. 
The name for a fluorescent lamp without phosphors is a gas discharge tube.
I love your question. The answer is probably the one cited by the Wikipedia quote on "Primary colors":

 >  Primary colors are not a fundamental property of light but are often related to the physiological response of the eye to light. Fundamentally, light is a continuous spectrum of the wavelengths that can be detected by the human eye, an infinite-dimensional stimulus space.

[Citation](_URL_0_)

The interpretation to make is that primary colors and complementary colors do not exist on their own -- instead they are a trick of the way our eyes see color. Since they don't come out of some mathematical or physical construct, such concepts will only work for light that humans can see.

TL;DR: No, complementary colors only exist for visible light.
Because orbiting objects can interact with other orbiting objects... and change their both orbits into one.

Now imagine lots of tiny asteroids/dust partibles orbiting on different planes, or even different directions. They will eventually crash into each other, and crash, and crash... sometimes forming bigger objects. But mostly those crashes will lead them to form a flat plane where they don't crash anymore.
Try this experiment:  Just hold out a sheet of paper without the glass of water.  Why doesn't the huge column of air above the paper just push the paper to the ground?  It's because the column of air below the paper is supporting the column of air above the paper, so there is no net force on the paper.  Imagine the air as a really tall stack of springs.  The springs near the bottom will be compressed due to the weight of the springs above them.  Now imagine you could insert a sheet of paper between two of the springs near the bottom.  The springs above the paper are pushing down on the paper with the same force as the springs below the paper are pushing up.  Nothing changed by adding the paper into the system of springs.
It's called cold welding. If you bring the two aluminum blocks together so that they touch (assuming no oxygen and no oxides are present), then you'd have a single aluminum block. There is almost no energy involved between the two blocks bonding to each other (unless the bonding is high-velocity, like the two blocks are fired from a pressure cannon at each other). It's just easier to say that manually bringing the two blocks together is the energy put into making them bond to each other.
Well both really. Addiction can be based on a feeling that someone or animal gets from consuming drugs so it's psych0logical that they want and strive for that feeling so they keep taking drugs. But also it can be physical as you in keep taking pain killers and your body gets used to having the dose and you eventually need more or your body can't function without it, you go through withdraw.
It has to do with shifting electrons from the Oxygen, and it has to do with Resonance. Resonance is something Oxygen is really good at doing, and in organic it is one of the most important principles.

Here: _URL_0_ on the wikipedia for Resonance, NO2 is actually an example at the top right. Also Look at this [video](_URL_1_).
Well, assuming the two planets are initially stationary with respect to each other (they don't look like they're in geostationary orbit or anything, and if they were, those planets would have to have days that lasted like 80 minutes), then they'd just pull each other and smash into each other.

Upside Down is a fantasy movie really, there are some pretty major problems with it if you take it too seriously - I don't think it's supposed to be a rigorously scientific scenario.
There are a number of diffrent theories about how stuff should be done. 
One that has been effective is the Kakapo-project i New Zealand. 

Basicly what they did was rid a few Islands of Everything that had not been there when the parrot was around in numbers. 

[Wikipedia](_URL_0_)
The electromagnetic properties pointed out by u/Benevolent_Overlord is correct.  But we do have transparent conductors.  There's this stuff called ITO.  It's not really a metal, strictly speaking, it's a doped oxide.  
Basically it's a crystal made out of oxygen, indium, and it's got some sprinklings of tin in it.  This makes it conduct like a metal, but still lets light through in the visible spectrum with a very high transmittance.  It's gotta be pretty thin though, like a few hundred nanometers.  But that's enough for some cool nanotech applications.  Like window solar panels and stuff.
In humans, it's not a functional thing. It's basically a seam left over from where the two sides of your head came together while you were being made. (If you've ever seen a cleft lip, you've seen an example of how this fusion can fail to happen.) Embryonic development is a weird and complicated process, but his picture gives you the idea:

_URL_0_



In most non-primate mammals, the philtrum helps wick moisture to the nose to keep it moist.
Is your question is asking about what happens to rivers during long-term sea level rise (not short term fluctuations due to tides, etc)? Rivers tend to develop a [graded profile](_URL_0_), which is essentially a minimum energy state required to maintain the profile (simply think of ability of river to erode as a combination of slope and the amount of water moving over that slope, so as you move downstream, amount of water increases so slope can decrease). The graded profile is a "steady-state" feature, and if a change occurs to the system (like a long-term rise in sea level) the river will try to return to a new steady-state condition, i.e. a new graded profile. In the specific case of a sea level rise, as the sea level goes up and floods the lower part of the profile, basically the slope of the river is now too shallow for the amount of water so the river will start to aggrade (start depositing sediment) to reestablish the graded profile.
You are correct: there are no infinitely rigid structures in the universe. That's true all the way down to small objects.
The general rule as it pertains to any discussion of optimization in biology is that you need to ask the question "for what purpose?"

An athlete versus a non-athlete makes a difference.  Within athletes, there are sport-specific requirements -- are you a power lifter or a marathoner, or something in between?  For non-athletes, the question is what benefit you wish to derive.  Lose weight?  Gain strength?  Gain weight?  Improve lifespan?  Fight age-associated muscle-wasting?  Improve hormonal profiles?  So on and so forth.

Not all activity impacts your body the same and not all activity causes the same physiological adaptation, since each activity is a different stress.  In short, there's no silver bullet, except that being active is probably better than being sedentary unless you have a serious medical malady.
Perhaps the most accurate way of saying it is it's always driving towards balance. There's a self-correcting feedback at play driving back towards balance should the system leave it. The amount of IR light (i.e. thermal radiation) the Earth radiates away to space is proportional to the Earth's temperature (to the power of four). If at any given moment IR(out) doesn't balance Sun(in) then the Earth's temperature will rise or fall until it does. At the most fundamental level, that's what the concern over global warming is. We're hampering our IR emission thus Sun(in) is greater than IR(out) and thus the Earth is accumulating energy and this drives the Earth's temperature to rise so that IR(out) can increase until it again matches Sun(in).
